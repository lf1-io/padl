# pylint: disable=too-many-arguments,no-member,not-callable,consider-using-enumerate,fixme,pointless-statement
"""
Module containing classes for transforming possible types of dataframe columns.
Philosophy follows the same philosophy as torchvision; each module is callable
where the parameters of the call are set in the initializer.
"""
import collections
import contextvars
from contextlib import contextmanager
import copy
import os
import functools
import importlib
import inspect
import io
import itertools
import json
import pdb
from random import choice
import re
import shutil
from types import ModuleType
import typing as tp
from uuid import uuid4
from warnings import warn
import traceback
import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import DataLoader

from lf.dataproc.data import SimpleIterator
from lf.typing.types import (Any, Dict, Bool, Number, Float, Integer, Tensor, FloatTensor,
                             LongTensor, Image, Set, Sequence, List, Tuple, Ftype, TypeBuilder,
                             TypeMismatch, Mismatch, Typevar, String, Void, FtypeDict,
                             val, Mvar, Matchable, AnyType)
from lf.typing.checkers import JitError
from lf.utils import evalmodel
from lf.utils import convert, unbatch
from lf.variables import (
    Variable,
    AlephMongoCrossProcessClient,
)
from lf import saving

settings = {
    'strict_types': 'warn',
    'repr_style': None,
    'verbosity': 1,
    'print_actions': False
}

_lf_trace = []


def type_mismatch(msg):  # pragma: no cover
    """
    Raise/ warn/ suppress typing mismatch based on settings variable
    `lf.transforms.core.settings`.

    :param msg: message to print
    """
    if settings['strict_types'] == 'silent':
        return
    if settings['strict_types'] == 'debug':
        warn('Type mismatch: ' + msg)
        print('break point for type-mismatch')
        pdb.set_trace()
    elif settings['strict_types'] == 'strict':
        raise TypeMismatch(msg)
    elif settings['strict_types'] == 'warn':
        warn('Type mismatch: ' + msg)
    else:
        raise TypeError(
            'settings["strict_types"] should be in {"silent", "debug", "strict", "warn"}')


def _loadcls(classname):
    """
    :param classname:
    :return:
    """
    module = '.'.join(classname.split('.')[:-1])
    if module.startswith('torchabc'):  # pragma: no cover
        module = module.replace('torchabc', 'lf')
        warn('"torchabc" has now been moved to "lf" -> replacing in path')
    class_ = classname.split('.')[-1]
    if module:
        module = importlib.import_module(module)
        return getattr(module, class_)
    return eval(class_)


def _load_trans(class_, dict_param: tp.Dict, dict_var: tp.Dict) -> "Transform":
    """
    Load the transform

    :param class_:
    :param dict_param: recursive dictionary storing class parameters and structure
    :param dict_var: dictionary of variables/ torch layers
    """
    try:
        trans_loaded = class_.from_dict(dict_param['kwargs'], vars=dict_var,
                                        handles=dict_param['handles'])
    except json.JSONDecodeError as err:
        warn(str(err))
        trans_loaded = class_.from_dict(dict_param['kwargs'], vars=dict_var, handles={})
    except AttributeError as err:
        warn(str(err))
        if 'from_dict' in str(err):
            warn(f'legacy function used in load or {dict_param}')
            class_ = eval(class_.name)
            trans_loaded = class_.from_dict(dict_param['kwargs'], vars=dict_var,
                                            handles=dict_param['handles'])
        else:
            raise err
    return trans_loaded


def loads(dict_param: tp.Dict, dict_var: tp.Dict) -> "Transform":
    """
    Load transform from dictionaries. These dictionaries are generated by `Transform.to_dict`.

    :param dict_param: recursive dictionary storing class parameters and structure
    :param dict_var: dictionary of variables/ torch layers
    """
    try:
        dict_param = copy.deepcopy(dict_param)
    except TypeError as err:  # pragma: no cover
        if os.environ.get('DO_UNITTEST', 'false') == 'true':
            if "object is not callable" not in str(err):
                raise err
        else:
            raise err

    class_ = _loadcls(dict_param['cls'])
    if 'exception' in dict_param and dict_param['exception']:
        return class_

    if '_type' in dict_param:
        _type = Ftype.from_dict(dict_param.pop('_type'))
        dict_param['kwargs'] = {**dict_param['kwargs'], '_type': _type}
    if 'handles' not in dict_param:
        dict_param['handles'] = {}

    trans_loaded = _load_trans(class_, dict_param, dict_var)
    if '_name' in dict_param:
        trans_loaded._name = dict_param['_name']

    trans_loaded.infer()
    return trans_loaded


def _check_model(input_trans, output):
    """
    Test transform output to desired output
    :param input_trans: transform
    :param output: desired output
    """

    from lf.testing import assert_close
    for in_, out in tqdm(output['samples']):
        with input_trans.set_stage('infer'):
            assert_close(input_trans(in_), out)


def load(
        path_: str,
        compile_transform: bool = True,
        test: bool = False,
        load_samples: bool = True,
        **kwargs,
):
    """
    Load transform from file.

    :param path_: path_ to model (usually `.tabc` filename)
    :param compile_transform: toggle off to `False` if only to load dictionaries
    :param test: toggle on to check sample inputs/ outputs
    :param load_samples: If *True*, load input / output samples if they are stored with the model.
    :param test: If *True*, test transform after loading.
    """

    output = saving.load(path_)

    for val in kwargs:
        output['v'][val] = kwargs[val]

    if compile_transform:
        t = loads(output['d'], output['v'])
        if test:

            if 'samples' not in output or output['samples'] is None:
                warn('No samples for testing found.')
            else:
                print('checking model')
                _check_model(t, output)
        if load_samples:
            t._samples = output.get('output')
        try:
            t._meta_info = output['meta']
        except KeyError:
            pass
        return t

    return output


def move_torch_jit_to_device(variables, to_device):
    """
    Move torch.jit.ScriptModule objects in variables to *to_device*

    :param variables: dictionary of variables
    :param to_device: device to send to. Can be str or dict
    """

    assert isinstance(to_device, str) or isinstance(to_device, dict)

    prev_devices = {}
    for key in variables:
        if isinstance(variables[key], torch.jit.ScriptModule):

            # Determine which device to send to. Priority given to to_device_dict
            if isinstance(to_device, str):
                the_device = to_device
            else:
                the_device = to_device[key]

            try:
                # save devices for afterwards
                prev_devices[key] = next(variables[key].parameters()).device
            except StopIteration:
                pass
            else:
                # Send torch.jit.ScriptModule to the_device
                variables[key].to(the_device)

    return prev_devices


def save(t, path_, legacy=False, **kwargs):
    """
    Save a transform

    :param t: transform
    :param path_: path to which to save
    :param legacy: If *True*, save in legacy format.
    """
    path_ = str(path_)

    if not path_.endswith('.tabc'):
        path_ += '.tabc'

    current_device = t.device
    t.to('cpu')
    my_dict, v = t.to_dict()
    prev_devices = move_torch_jit_to_device(v, 'cpu')

    for key in v.copy():
        if isinstance(v[key], Variable) and v[key].dont_store:
            del v[key]
        elif isinstance(v[key], Variable) and not v[key].dont_store:
            v[key] = v[key].value

    metadata = {**kwargs, '__versions__': t.__versions__}
    if legacy:
        _save_legacy(my_dict, v, path_)
    else:
        saving.save(path_, d=my_dict, v=v, metadata=metadata, samples=t._samples)

    _ = move_torch_jit_to_device(v, prev_devices)
    t.to(current_device)


class Transform:
    """
    Base transform class.

    :param in_type: Input type.
    :param out_type: Output type.
    :param _type: As alternative to in_type, out_type a tuple of both.

    Important methods:

    * `clone` - clone the transform
    * `train` - set transform to train mode
    * `eval` - set transform to eval mode
    * `infer` - set transform to infer mode
    * `to_dict` - get a dict representation of the transform (for saving)
    * `save` - save the transform
    * `state_dict` - get the statedict of layers within the transform
    * `parameters` - get parameters of layers within the transform
    * `to` - change mode
    * `load_state_dict` - load layer parameters

    Important class methods

    * `from_dict` - load a transform from a dict (see to_dict)

    Important properties:

    * `device` - the device the transform is on
    * `mapdevice`
    * `layers` - dict with layers in the transform
    * `variables`
    * `forward` - the "forward" part of the transform
    * `trans` - the preprocessing part of the transform
    * `preprocess`
    * `postprocess` - the postprocess part of the transform
    * `handles`

    """
    requires_args = True
    _no_clone = False
    _forced_stage: tp.Optional[str] = None

    def __init__(self, in_type=None, out_type=None, _type=None, **kwargs):
        # special attributes which can't be set
        not_allowed = {'trans', 'postprocess', 'forward', 'to_dict',
                       'from_dict'}
        if set(kwargs.keys()).intersection(not_allowed):
            raise ValueError(
                f'can\'t include special word from {not_allowed} in arguments'
            )

        for k, v in kwargs.items():
            try:
                self.getattribute_object(k)
            except AttributeError:
                pass
            else:
                warn(
                    f"While creating Transform '{self.__class__.__name__}': "
                    f"Setting existing attribute {k}. "
                    "This might cause problems. "
                    "Please consider choosing a different name. "
                )
            setattr(self, k, v)

        self.type = _type
        if _type is None:
            annos = inspect.getfullargspec(self.do).annotations
            return_ = annos.pop('return', None)
            tuple(annos.values())
            if in_type is None:
                in_type = Any()
                if annos:
                    in_type = annos
            if out_type is None:
                out_type = Any()
                if return_:
                    out_type = return_
            self.type = Ftype.build(in_type, out_type)

        self._stage = 'infer'
        self._mapdevice = {'cpu'}
        self._device = 'cpu'
        self._handles = {}
        self._handle_defs = {}
        self._name = None
        self._preprocess = None
        self._layers = None
        self._traceback = traceback.extract_stack()

        self._teleport = None

        self.type = self.check_types()

        self._arity = len(inspect.signature(self.do).parameters)

        self._samples = None
        self._meta_info = {}

    @property
    def meta_info(self):
        """
        :return: _meta_info
        """
        return self._meta_info

    def add_sample(self, in_sample) -> None:
        """Add a sample input to the transform. This can be saved with the transform and used
        for checking consistency. """
        with self.set_stage('infer'):
            out_ = self(in_sample)
        if self._samples is None:
            self._samples = []
        self._samples.append((in_sample, out_))

    def generate_samples(self, n) -> None:
        """Generate and add random samples using the transform's type."""
        for _ in tqdm(range(n)):
            self.add_sample(self.type.x.sample())

    @property
    def name(self) -> tp.Optional[str]:
        """ The name of the transform. """
        return self._name

    @name.setter
    def name(self, val: str) -> None:
        """ Set the name of the transform. """
        self._name = val

    def set_type(self, in_type: tp.Optional[TypeBuilder] = None,
                 out_type: tp.Optional[TypeBuilder] = None,
                 _type: tp.Optional[tp.Tuple[TypeBuilder, TypeBuilder]] = None) -> None:
        """ Set the type of the transform.

        :param in_type: Type of input.
        :param out_type: Type of output.
        :param _type: Tuple: (type-of-input, type-out-output). If this is set, *in_type* and
            *out_type* should be None.
        """
        if _type is None:
            if in_type is None:
                in_type = Any()
            if out_type is None:
                out_type = Any()
            new_type = Ftype.build(in_type, out_type)
            new_type @ self.type
        else:
            assert in_type is None
            assert out_type is None
            self.type = _type

    def _process_traceback(self, traceback=None) -> str:
        """ Find where the transform was defined (file, lineno, line) given the traceback. """
        a_tb = None
        for a_tb in self._traceback[::-1]:
            if 'lf/transforms' in a_tb[0]:
                continue
            break
        return f'{a_tb.filename} in {a_tb.name}\n----> {a_tb.lineno} {a_tb.line}'

    def _trace_error(self, position: int, arg: tp.Any):
        """ Add some error description to `_lf_trace`. """
        try:
            str_ = self._repr_with_marker(marker=(position, 'error here', 'on'))
            _lf_trace.append((str_, self._process_traceback(), arg, self))
        except Exception:
            warn('Error tracing failed')

    def __getattribute__(self, item: str) -> tp.Any:
        """ Get an attribute, if the attribute is a `Variable`, return its value. """
        res = object.__getattribute__(self, item)
        if isinstance(res, Variable) and not res.custom:
            return res.value
        return res

    def getattribute_object(self, item):
        """ Like getattribute, but not returning variable values, but variable objects. """
        return object.__getattribute__(self, item)

    @property
    def mapdevice(self):
        """
        :return: map device
        """
        return self._mapdevice

    @mapdevice.setter
    def mapdevice(self, v):
        self._mapdevice = v
        # TODO: is this still correct? (we're mutating the children, which we shouldn't)
        warn('mapdevice setter being used :O, tell alex (a@lf1.io)')
        for x in self.__dict__:
            attrib = self.getattribute_object(x)
            if isinstance(attrib, Transform):
                attrib.mapdevice = v
            elif isinstance(attrib, list) and attrib and isinstance(attrib[0], Transform):
                for y in attrib:
                    y.mapdevice = v

    @property
    def is_identity(self) -> bool:
        """ Return *True* iff the transform is the identity transform. """
        return False

    def repr(self):
        """
        string representation
        """
        return '<' + self.__class__.__name__ + '>'

    def clone(self) -> "Transform":
        """ Clone the object while sharing layers. Calls `self._clone` internally plus copies
        names etc.."""
        if self._no_clone:
            return self

        a_clone = self._clone()
        if self._teleport is not None:
            a_clone._teleport = self._teleport.clone()
        a_clone.stage = self.stage
        a_clone.name = self.name
        a_clone.to(self.device)
        a_clone._traceback = self._traceback
        a_clone.type = self.type
        a_clone._handles = self._handles
        a_clone._handle_defs = self._handle_defs
        return a_clone

    def _clone(self) -> "Transform":
        """ Clone the transform. """
        return loads(*self.to_dict())

    @staticmethod
    @contextmanager
    def no_clone():
        """ Context manager for disabling cloning - within the context cloning will not clone, but
        return a reference to the same transform. """
        before = Transform._no_clone
        try:
            Transform._no_clone = True
            yield
        finally:
            Transform._no_clone = before

    @contextmanager
    def set_stage(self, stage: str):
        """
        Set of stage of Transform
        :param stage: stage ('train', 'eval', 'infer')
        :return:
        """
        assert stage in ('train', 'eval', 'infer')

        before = Transform._forced_stage
        layers = self.layers
        training_before = {k: l.training for k, l in layers.items()}
        try:
            for layer in layers.values():
                if stage == 'train':
                    layer.train()
                else:
                    layer.eval()
            Transform._forced_stage = stage
            yield
        finally:
            for k, training in training_before.items():
                layer = layers[k]
                if training:
                    layer.train()
                else:
                    layer.eval()
            Transform._forced_stage = before

    @contextmanager
    def my_stage(self):
        """
        set stage as _stage
        """
        with self.set_stage(self._stage):
            yield

    @property
    def layers(self) -> tp.Dict:
        """
        Get a dict with all layers in the transform (including layers in sub-transforms).
        """
        if self._layers is None:
            layer_dict = {}
            for item in self.__dict__:
                attrib = self.getattribute_object(item)
                if isinstance(attrib, Transform):
                    layer_dict.update(attrib.layers)
                elif type(attrib) in {tuple, list} and attrib and isinstance(attrib[0], Transform):
                    for y in attrib:
                        layer_dict.update(y.layers)
            self._layers = layer_dict
        return self._layers

    @property
    def variables(self) -> tp.Dict[str, Variable]:
        """ Get a dict with all variables in the transform
        (including variables in sub-transforms). """
        v = {}
        for x in self.__dict__:
            attrib = self.getattribute_object(x)
            if isinstance(attrib, Transform):
                v.update(attrib.variables)
            elif isinstance(attrib, Variable):
                v[attrib.name] = attrib
            elif isinstance(attrib, list) and attrib and isinstance(attrib[0], Transform):
                for item in attrib:
                    v.update(item.variables)
        return v

    def replace_subtransform(self, from_: "Transform", to: "Transform") -> None:
        """ Replace any occurence of transform *from_* within this transfrom with the
        transform *to*.

        :param from_: source transform class
        :param to: target transform to replace by
        """

        for x in self.__dict__:
            attrib = self.getattribute_object(x)
            if isinstance(attrib, Transform) and attrib == from_:
                setattr(self, x, to)
            elif isinstance(attrib, list):
                setattr(self, x, [to if y == from_ else y for y in attrib])
            elif isinstance(attrib, tuple):
                setattr(self, x, tuple([to if y == from_ else y for y in attrib]))

    def train(self) -> None:
        """ Set to training mode, calculating backward pass. """
        self._stage = 'train'

    def eval(self) -> None:
        """ Set to evaluation mode: evaluate over multiple data points, but with frozen gradients.
        """
        self._stage = 'eval'

    def infer(self) -> None:
        """ Set to inference mode (single data points). """
        self._stage = 'infer'

    def stage_transform(self, stage: str) -> "Transform":
        """ Return a transform with a given *stage* set."""
        return self

    def train_transform(self) -> "Transform":
        """ Get training transform. """
        t = self.stage_transform('train')
        t.train()
        return t

    def infer_transform(self) -> "Transform":
        """ Get inference transform. """
        t = self.stage_transform('infer')
        t.infer()
        return t

    def eval_transform(self) -> "Transform":
        """ Get evaluation transform. """
        t = self.stage_transform('eval')
        t.eval()
        return t

    @property
    def end_trans(self) -> "Identity":
        """ Get leaf transform/ postprocess transform. """
        return Identity()

    def has_stage_switch(self) -> bool:
        """ Assess whether transform is sensitive to stage train/eval/infer. """
        return False

    def check(self):
        """
        :return: output sample
        """
        self.generate_samples(1)
        print(f'input: {self._samples[0][0]}')
        output = self(self._samples[0][0])
        print(f'output: {output}')
        return output

    def check_types(self):
        """ Check that types match up internally. """
        if self.has_stage_switch():
            with self.no_clone():
                eval_type = self.eval_transform().type
                infer_type = self.infer_transform().type
                train_type = self.train_transform().type
            if infer_type == eval_type == train_type:
                final_type = infer_type
            else:
                final_type = FtypeDict({
                    'infer': infer_type,
                    'eval': eval_type,
                    'train': train_type
                })
        else:
            final_type = self._check_types()
        return final_type

    def _check_types(self):
        """ Check that types match up internally. """
        return self.type

    @property
    def handles(self):
        """ Get keys of the handles dictionary. """
        return self._handles.keys()

    @property
    def stage(self) -> str:
        """
        :return: Stage of Transform
        """
        if self._forced_stage is None:
            return self._stage
        return self._forced_stage

    @stage.setter
    def stage(self, v: str):
        """
        :param v: stage ('train', 'eval', 'infer')
        """
        assert v in ('train', 'eval', 'infer')
        getattr(self, v)()


    def _to_dict_helper_from_list(self, my_val, my_vars):
        _val = []
        for item in my_val:
            y, temp = item.to_dict()
            _val.append(y)
            my_vars.update(temp)
        return _val

    def _to_dict_helper_from_exception(self, my_val):
        module = my_val.__module__
        class_ = my_val.__name__
        if module != 'builtins':
            class_ = module + '.' + class_
        my_val = {'cls': class_, 'exception': True}
        return my_val

    def to_dict(self) -> tp.Tuple[tp.Dict, tp.Dict]:
        """ Convert transform to dictionaries of parameters and variables values. """
        params = inspect.signature(self.__init__).parameters.keys()
        my_dict = {}
        my_vars = {}
        for a_param in params:
            if a_param in ['self', 'in_type', 'out_type', 'kwargs', '_type']:
                continue
            my_val = self.getattribute_object(a_param)
            if isinstance(my_val, (Variable, torch.jit.ScriptModule)):
                my_vars[my_val.name] = my_val
                my_val = '$' + my_val.name
            elif isinstance(my_val, Transform):
                my_val, temp = my_val.to_dict()
                my_vars.update(temp)
            elif isinstance(my_val, list) and my_val and isinstance(my_val[0], Transform):
                my_val = self._to_dict_helper_from_list(my_val, my_vars)
            elif inspect.isclass(my_val) and issubclass(my_val, Exception):
                my_val = self._to_dict_helper_from_exception(my_val)
            my_dict[a_param] = my_val
        self._add_type_to_dict(my_dict)

        res = {
            'cls': self.__class__.__module__ + '.' + self.__class__.__name__,
            'kwargs': my_dict,
            'handles': self._handle_defs,
        }

        if self.name is not None:
            res['_name'] = self.name

        return res, my_vars

    @property
    def __versions__(self):
        versions = self._versions()
        from lf import __version__
        versions['lf'] = __version__
        from torch import __version__
        versions['torch'] = __version__
        return versions

    def _versions(self):
        return {}

    def _add_type_to_dict(self, d: tp.Dict) -> None:
        """ Add type information to transform dict (in `self.to_dict`). """
        params = inspect.signature(self.__init__).parameters.keys()
        if 'in_type' in params and 'out_type' in params:
            t = self.type.to_dict()
            d['_type'] = t

    @staticmethod
    def type_from_dict(d: tp.Dict) -> None:
        """ Extract the type from a transform dict (as built in `to_dict`). """
        if '_type' in d:
            _type = Ftype.from_dict(d.pop('_type'))
            d['_type'] = _type

    @staticmethod
    def copy_from_dict(d: dict):
        """
        :param d: dictionary
        :return: copy of dictionary
        """
        try:
            d = copy.deepcopy(d)
        except TypeError as err:
            if os.environ.get('DO_UNITTEST', 'false') == 'true':
                if not ('object is not callable' in str(err)):
                    raise err
            else:
                raise err
        return d

    @staticmethod
    def from_mongo_client(e):
        """
        get mongo client
        :param e: Exception raised
        :return:
        """
        try:
            with open('config.json') as file1:
                clientdef = json.load(file1)['mongodb']

            warn('**USING DEFAULT CLIENT!!**')
            val = AlephMongoCrossProcessClient(**clientdef, name='mongoclient')
        except (FileNotFoundError, KeyError):
            raise e
        return val

    @classmethod
    def _load_from_dict_helper_str(cls, vals, vars):
        try:
            return vars[vals[1:]]
        except KeyError as err:
            if 'mongoclient' in str(err) and vals == '$mongoclient':
                return cls.from_mongo_client()
            else:
                raise

    @classmethod
    def load_from_dict(cls, d: dict, vars: tp.Dict = None):
        """
        :param d: transform dict
        :param vars: var dict
        :return:
        """
        for k, vals in d.items():
            if isinstance(vals, str) and vals.startswith('$'):
                d[k] = cls._load_from_dict_helper_str(vals, vars)
            elif isinstance(vals, dict) and 'cls' in vals:
                d[k] = loads(vals, vars)
            elif isinstance(vals, list) and vals and isinstance(vals[0], dict) and \
                    'cls' in vals[0]:
                d[k] = [loads(x, vars) for x in vals]
        return d

    @classmethod
    def from_dict(cls,
                  d: dict,
                  vars: tp.Dict = None,
                  handles: tp.Optional[tp.Dict] = None) -> "Transform":
        """ Build a transform from a dict (as returned from `to_dict`). """

        d = cls.copy_from_dict(d)
        cls.type_from_dict(d)

        if not handles:
            handles = {}

        if vars is not None:
            cls.load_from_dict(d, vars)

        try:
            t = cls(**d)
        except TypeError as err:
            if 'got an unexpected keyword argument' in str(err):
                t = cls._alternate_init_(**d)
            else:
                raise err

        handles = cls._parse_handles(handles)
        for k in handles:
            t[k] = handles[k]

        return t

    @staticmethod
    def _parse_handles(handles):
        """
        :param handles:
        """
        for k in handles:
            if not handles[k].startswith('lambda'):
                handles[k] = json.loads(handles[k])
        return handles

    @classmethod
    def _eq_no_types_dict(cls, a: tp.Dict, b: tp.Dict) -> bool:
        """Comparing two dictionaries"""
        for k, v in a.items():
            if k not in b:
                return False
            if k == '_type':
                continue
            if not cls._eq_no_types(v, b[k]):
                return False
        return True

    @classmethod
    def _eq_no_types_list_or_tuple(cls,
                                   a: tp.Union[tp.List, tp.Tuple],
                                   b: tp.Union[tp.List, tp.Tuple]) -> bool:
        """Comparing two lists or tuples"""
        if len(a) != len(b):
            return False
        for item, y in zip(a, b):
            if not cls._eq_no_types(item, y):
                return False
        return True

    @classmethod
    def _eq_no_types(cls,
                     a: tp.Union[tp.List, tp.Dict, tp.Tuple],
                     b: tp.Union[tp.List, tp.Dict, tp.Tuple]) -> bool:
        """ Compare two nested dicts or lists *a* and *b*, ignoring the _type field. """

        if isinstance(a, torch.jit.ScriptModule) and isinstance(b, torch.jit.ScriptModule):
            # TODO Issue 182
            return True

        if type(a) != type(b):
            return False

        if isinstance(a, dict):
            return cls._eq_no_types_dict(a, b)

        if isinstance(a, (list, tuple)):
            return cls._eq_no_types_list_or_tuple(a, b)

        return a == b

    def __eq__(self, other: "Transform") -> bool:
        """ Check if the transform is equal to another transform. This ignores the types. """
        s_dict = self.to_dict()
        o_dict = other.to_dict()
        # ignore type (this can differ in the dict although it's essentially the same)
        return self._eq_no_types(s_dict, o_dict)

    def save(self, path_: str, legacy=False, **kwargs) -> None:
        """ Save transform.

        :param path_: Path to file to save.
        :param kwargs: Key-value pairs to add to meta-data (e.g. documentation etc.).
        :param legacy: If *True*, save in legacy format.
        """
        save(self, path_, legacy=legacy, **{**self.meta_info, **kwargs})

    @property
    def trans(self) -> "Transform":
        """ The preprocessing part (everything that happens before sending to gpu). """
        if 'cpu' in self.mapdevice:
            return self
        return Identity()

    def _forward_part(self) -> "Transform":
        """ The GPU part of the transform. """
        if 'gpu' in self.mapdevice:
            return self
        return Identity()

    @property
    def forward(self) -> "Transform":
        """ The GPU part of the transform """
        f = self._forward_part()
        f.to(self.device)
        return f

    @property
    def postprocess(self) -> "Transform":
        """ The postprocessing part of the transform. """
        if 'bcpu' in self.mapdevice:
            return self
        return Identity()

    @property
    def postprocess_with_fixed_stage(self) -> "Transform":
        """ Get the postprocess transform while fixing it to the current stage (removing all
        branches that belong to a different stage). """
        return self.postprocess

    @property
    def preprocess(self) -> "Transform":
        """ Get the preprocessing part of the transform, that is `self.trans >> self.forward`. """
        # cache
        if self._preprocess is None:
            self._preprocess = self.trans >> self.forward
        self._preprocess._stage = self._stage
        self._preprocess.to(self.device)
        return self._preprocess

    def parameters(self) -> tp.Iterator:
        """ Iterate over parameters. """
        for layer in self.layers.values():
            yield from layer.parameters()

    # DANGER: makes it mutable
    def freeze(self) -> None:
        """ Freeze gradients. """
        for a_param in self.parameters():
            a_param.requires_grad = False

    # DANGER: makes it mutable
    def unfreeze(self) -> None:
        """ Switch on gradients. """
        for a_param in self.parameters():
            a_param.requires_grad = True

    def state_dict(self):
        """ Get dictionary of parameters. """
        state_d = []
        for a_layer in self.layers:
            for y, z in self.layers[a_layer].state_dict().items():
                state_d.append((a_layer + '.' + y, z))
        return collections.OrderedDict(state_d)

    # DANGER: makes it mutable
    def to(self, device: str) -> "Transform":
        """ Set the transform's device to *device*.

        :param device: device on which to map {'cpu', 'cuda'}
        """
        self._device = device
        for item in self.__dict__:
            t = self.getattribute_object(item)
            if isinstance(t, Transform):
                t.to(device)
            elif isinstance(t, list) and t and isinstance(t[0], Transform):
                for a_trans in t:
                    a_trans.to(device)
        return self

    def cuda(self) -> None:
        """ Set device to 'cuda'. """
        self.to('cuda')

    def cpu(self) -> None:
        """ Set device to 'cpu'. """
        self.to('cpu')

    @property
    def device(self):
        """
        :return: device
        """
        return self._device

    # DANGER: makes it mutable
    def load_state_dict(self, other: "Transform", strict: bool = True) -> None:
        """ Load the state dict from another transform *other*.

        The transform *other* must have all parameters *self* has.

        :param other: Transform from which to get parameters.
        :param strict: If False, then ignore missing parameters.
        """
        if isinstance(other, dict):
            for k in other['v']:
                if not isinstance(other['v'][k], torch.nn.Module):
                    continue
                stem = '_'.join(k.split('_')[:-1])  # removes the uuid
                match = next(x for x in self.layers if x.startswith(stem))
                self.layers[match].load_state_dict(
                    other['v'][k].state_dict(),
                    strict=strict
                )
        else:
            for layer in other.layers:
                self.layers[layer].load_state_dict(
                    other.layers[layer].state_dict(),
                    strict=strict
                )

    def __rshift__(self, other: "Transform") -> "Transform":
        """ Compose with *other*.

        `a >> b` is the same as `Compose([a, b], flatten=True)`
        """

        if not isinstance(other, Transform):
            clone = self.clone()
            clone.set_type(None, other)
            return clone
        return Compose([self, other], flatten=True)

    def __rrshift__(self, in_type: TypeBuilder) -> "Transform":
        """ Clone and set the input type to in_type.

        Example:

        >> b = Integer() >> a

        b will be the same as a, except with input type `integer`.
        """
        clone = self.clone()
        clone.set_type(in_type)
        return clone

    def __lshift__(self, other: "Transform") -> tp.Any:
        """ Shortcut for applying a transform.

        `a >> b >> c << 1` is the same as `(a >> b >> c)(1)`
        """
        return self(other)

    def __add__(self, other: "Transform") -> "Rollout":
        """ Rollout with *other*. """
        return Rollout([self, other], flatten=True)

    def __truediv__(self, other: "Transform") -> "Parallel":
        """ Parallel with *other*. """
        return Parallel([self, other], flatten=True)

    def __invert__(self) -> "Map":
        """ Map. """
        return Map(self)

    def __rsub__(self, other):
        return self.__sub__(other)

    def __sub__(self, other: tp.Union[str, tp.Tuple[str, TypeBuilder, TypeBuilder]]) -> "Transform":
        """ Set the name and optionally types.

        >> t - 'hallo'
        >> t.name
        "hallo"

        >> t - ('hallo', Integer(), Float())
        >> t.name
        "hallo"
        """
        if isinstance(other, str):
            self.name = other
        elif isinstance(other, tuple):
            assert len(other) == 3
            self.name = other[0]
            self.set_type(*other[1:])
        return self

    def __rpow__(self, other):
        return self.__pow__(other)

    def __pow__(self, other):
        if isinstance(other, str):
            self.name = other
        elif isinstance(other, tuple):
            assert len(other) == 3
            self.name = other[0]
            self.set_type(*other[1:])
        return self

    @property
    def args(self):
        """
        :return: args
        """
        params = inspect.signature(self.__class__).parameters.keys()
        return {p: self.getattribute_object(p)
                for p in params
                if p not in ['self', 'in_type', 'out_type'] and hasattr(self, p)
                }

    def astr(self, with_type=True, compact=True, with_types=False):
        """
        :param with_type:
        :param compact:
        :param with_types:
        """
        if compact and self.name is not None:
            return f'{type(self).__name__}[{self.name}]'
        a_str = f'{self.__class__.__name__}'
        args = [
            (k, str(v)[:30] + ('...' if len(str(v)) > 30 else ''))
            for k, v in self.args.items()
        ]
        a_str += '(' + ','.join(f'{k}={v}' for k, v in args) + ')'
        if with_type:
            a_str += f'          {self.type}'
        return a_str

    def _add_type_to_str(self, s, format=True):
        """
        :param s: string to add
        :param format: use formatting
        """
        res = '\33[32m ↓  ' + str(self.type.x) + '\33[0m\n'
        if format:
            res += '    ' + '\n    '.join(['\33[1m' + x + '\33[0m' for x in s.split('\n')]) + '\n'
        else:
            res += s
        res += '\33[32m ↓  ' + str(self.type.y) + '\33[0m\n'
        return res

    def fullname(self):
        """"
        Full name for transform
        """
        t = '\33[1m' + type(self).__name__
        if self.name is not None:
            t += f'[{self.name}]'
        t += ':\33[0m\n\n'
        return t

    def pretty_string(self, marker=None):
        """
        Pretty name for transform
        """
        t = self.fullname()

        astr = self.astr(with_type=False, compact=False, with_types=False)
        if marker is not None:
            astr += '\033[31m  <---- ' + marker[1] + '\033[0m'
        a_str = self._add_type_to_str(astr)

        return t + a_str

    def __repr__(self):  # pragma: no cover
        return self._repr_with_marker()

    def _repr_with_marker(self, marker=None):
        if self.has_stage_switch():
            res = f'In mode <{self.stage}>\n\n'
            with self.no_clone():
                rep = res + self.stage_transform(self.stage).pretty_string(marker=marker)
            return rep
        return self.pretty_string(marker=marker)

    @staticmethod
    def _add_parentheses_if_needed(t, string):
        """
        :param t: ListTransform
        :param string: string
        :return:
        """
        if isinstance(t, ListTransform) and t.name is None:
            return '(' + string + ')'
        return string

    def __getitem__(self, item):
        """
        :param item:
        """
        if callable(self._handles[item]):
            return self._handles[item](self)
        return self._handles[item]

    def __setitem__(self, key, item):
        """Set a handle. """
        if isinstance(item, str) and item.startswith('lambda'):
            self._handle_defs[key] = item
            self._handles[key] = eval(item)
        else:
            self._handle_defs[key] = json.dumps(item)
            self._handles[key] = item

    def do(self, args):
        """Do the transform. Inheriting transforms need to implement this. """
        raise NotImplementedError('not implemented')

    def _do(self, args, type_check=False):
        if type_check:
            try:
                self.type.x.jit_check(args)
            except JitError as err:
                raise JitError(
                    f'run-time mismatch on input of \n{self.pretty_string()}'
                    f'\n{str(err)}'
                )

        try:
            if self._arity > 1:
                if isinstance(args, dict):
                    res = self.do(**args)
                else:
                    res = self.do(*args)
            else:
                res = self.do(args)
            if self._teleport is not None:
                self._teleport.var.set(res)
        except Exception as err:
            if not isinstance(self, ListTransform):
                self._trace_error(0, args)
            raise err

        if type_check:
            try:
                self.type.y.jit_check(res)
            except JitError as err:
                raise JitError(
                    f'run-time mismatch on output of \n{self.pretty_string()}'
                    f'\n{str(err)}'
                )

        return res

    def _get_loader(self, iterator, args: tp.Any, flatten: bool = True, batch_size: int = 10,
                    num_workers: int = 4,
                    pin_memory=False, verbose: bool = False, shuffle: bool = False,
                    drop_last=False, horovod=False, batch_sampler=None,
                    loader_kwargs: tp.Optional[dict] = None):
        """
        Get Data Loader
        :param iterator: Iterator
        :param args: Arguments to call with.
        :param flatten: Boolean
        :param batch_size: Batch size
        :param num_workers: Num of workers
        :param pin_memory:
        :param verbose:
        :param shuffle: If *True*, shuffle data.
        :param drop_last:
        :param horovod:
        :param batch_sampler:
        :param loader_kwargs:
        :return:
        """
        if horovod:
            import horovod.torch as hvd
            from torch.utils.data.distributed import DistributedSampler
            sampler = DistributedSampler(
                iterator,
                num_replicas=hvd.size(),
                rank=hvd.rank(),
                shuffle=shuffle,
            )
            loader = DataLoader(
                iterator,
                batch_size=batch_size,
                sampler=sampler,
                num_workers=num_workers,
                pin_memory=True,
                drop_last=drop_last,
                worker_init_fn=lambda _: np.random.seed(),
                **loader_kwargs
            )
        else:
            loader = DataLoader(
                iterator,
                batch_size=batch_size,
                num_workers=num_workers,
                pin_memory=pin_memory,
                shuffle=shuffle,
                drop_last=drop_last,
                worker_init_fn=lambda _: np.random.seed(),
                batch_sampler=batch_sampler,
                **loader_kwargs
            )
        return loader

    @staticmethod
    def _forward_context_do(batch, forward, stage, use_forward):
        """
        :param batch: batch
        :param forward:
        :param stage:
        :param use_forward:
        :return:
        """
        if use_forward:
            if stage != 'train':
                with torch.no_grad():
                    my_o = forward.context_do(batch, stage=stage)
            else:
                my_o = forward.context_do(batch, stage=stage)
        else:
            my_o = batch
        return my_o

    @staticmethod
    def _yield_flatten_data(batch, verbose, pbar):
        """Yield from the flattened data.
        :param p: unbatched data type
        :param verbose: If *true* show a progress bar.
        :param pbar: tqdm progress bar
        """
        for datapoint in batch:
            yield datapoint
            if verbose:
                pbar.update(1)

    def _callyield(self, args: tp.Any, flatten: bool = True, batch_size: int = 10,
                   num_workers: int = 4,
                   pin_memory=False, verbose: bool = False, shuffle: bool = False,
                   drop_last=False, horovod=False, batch_sampler=None,
                   loader_kwargs: tp.Optional[dict] = None):
        """
        :param args: Arguments to call with.
        :param flatten: If true, flatten the output (yield single results instead of batches).
        :param batch_size: Batch size.
        :param num_workers: Number of workers to use for preprocessing.
        :param pin_memory:
        :param verbose:
        :param shuffle: If *True*, shuffle data.
        :param drop_last:
        :param horovod:
        :param batch_sampler:
        :param loader_kwargs: Extra kwargs passed to the loader.
        """

        iterator = SimpleIterator(
            args,
            self.trans.to('cpu').context_do
        )

        if loader_kwargs is None:
            loader_kwargs = {}

        loader = self._get_loader(iterator=iterator,
                                  args=args,
                                  flatten=flatten,
                                  batch_size=batch_size,
                                  num_workers=num_workers,
                                  pin_memory=pin_memory,
                                  verbose=verbose,
                                  shuffle=shuffle,
                                  drop_last=drop_last,
                                  horovod=horovod,
                                  batch_sampler=batch_sampler,
                                  loader_kwargs=loader_kwargs)

        pbar = None
        if verbose:
            if flatten:
                pbar = tqdm(total=len(args))
            else:
                loader = tqdm(loader, total=len(loader))

        forward = self.forward
        post = self.postprocess_with_fixed_stage

        try:
            use_post = not post.is_identity
        except AttributeError as err:
            if 'is_identity' not in str(err):
                raise err
            use_post = False

        use_forward = not forward.is_identity

        # fix the stage, otherwise it may change within the loop
        stage = self.stage

        for batch in loader:
            output = self._forward_context_do(batch, forward, stage, use_forward)

            if use_post:
                output = unbatch(output)
                output = [
                    post.context_do(output[i], stage=stage)
                    for i in range(len(output))
                ]

            if flatten:
                if not use_post:
                    output = unbatch(output)
                yield from self._yield_flatten_data(output, verbose, pbar)
                continue

            yield output

    def __call__(self, args=None, flatten=True, batch_size=10, num_workers=4,
                 pin_memory=False, verbose=False, shuffle=False,
                 drop_last=False, horovod=False, type_check=False,
                 batch_sampler=None, loader_kwargs=None, **kwargs):
        """
        :param args: Arguments to call with
        :param flatten: If true, flatten the output
        :param batch_size: batch size
        :param num_workers: Number of workers to use for preprocessing
        :param pin_memory:
        :param verbose:
        :param shuffle: If true, shuffle data
        :param drop_last:
        :param horovod:
        :param type_check:
        """

        global _lf_trace
        _lf_trace = []

        if isinstance(args, Transform):
            return Compose([args, self], flatten=True)

        if args is None and kwargs:
            return Compose([NamedRollout(**kwargs), self], flatten=True)

        if self.stage == 'infer':
            with torch.no_grad():
                return self.context_do(args, type_check=type_check, stage=self.stage)
        else:
            return self._callyield(
                args,
                flatten=flatten,
                batch_size=batch_size,
                num_workers=num_workers,
                pin_memory=pin_memory,
                verbose=verbose,
                shuffle=shuffle,
                drop_last=drop_last,
                horovod=horovod,
                batch_sampler=batch_sampler,
                loader_kwargs=loader_kwargs
            )

    def context_do(self, args, type_check=False, stage=None):
        """
        :param args: arguments to run
        :param type_check:
        """
        context = contextvars.copy_context()
        if stage is None:
            stage = self.stage
        with self.set_stage(stage):
            return context.run(self._do, args, type_check=type_check)

    @property
    def out(self) -> "TeleportReader":
        """ Create a teleport that teleports the transform's input and return the corresponding
        TeleportReader.
         """
        alias = self.name or ''
        if self._teleport is None:
            self._teleport = TeleportWriter(str(id(self)), alias=alias)
        return TeleportReader(str(id(self)), alias=alias, out_type=self.type.y)


class ListTransform(Transform):
    """
    Abstract class for transforms that operate on lists.

    :param trans_list: List of child transforms.
    :param in_type: Input type.
    :param out_type: Output type.
    :param _type: Type tuple (assumes input and output type are *None*)
    """
    _op_str = ' '

    def __init__(self, trans_list: tp.List["Transform"], in_type: tp.Optional[TypeBuilder] = None,
                 out_type: tp.Optional[TypeBuilder] = None,
                 _type: tp.Optional[tp.Tuple[TypeBuilder, TypeBuilder]] = None, **kwargs):

        self.kwargs = kwargs

        for i in range(len(trans_list)):
            assert isinstance(trans_list[i], Transform)

        super().__init__(in_type=in_type, out_type=out_type, _type=_type, trans_list=trans_list,
                         **kwargs)

        try:
            self._mapdevice = set.union(*[t.mapdevice for t in trans_list])
        except (AttributeError, TypeError):
            self._mapdevice = None

        self._mapdevice_list = [t.mapdevice for t in trans_list]

        self._trans = None
        self._forward = None

    def __getitem__(self, item):
        """
        Get item
        :param item:
        :return:
        """
        if isinstance(item, int):
            t = self.trans_list[item]
        elif isinstance(item, slice):
            t = type(self)(self.trans_list[item])
        elif isinstance(item, str) and item in self._handles:
            if callable(self._handles[item]):
                return self._handles[item](self)
            return self._handles[item]
        else:
            raise TypeError('unknown type for get item')

        t.stage = self.stage
        return t

    def _versions(self):
        versions = {}
        for t in self.trans_list:
            versions.update(t._versions())
        return versions

    def _clone(self) -> "ListTransform":
        kwargs = self.kwargs
        if 'flatten' in kwargs:
            kwargs['flatten'] = False
        r = type(self)(self.trans_list, **kwargs)
        try:
            r.flatten = self.flatten
        except AttributeError:
            pass
        return r

    def stage_transform(self, stage) -> "ListTransform":
        kwargs = self.kwargs
        if 'flatten' in kwargs:
            kwargs['flatten'] = False
        my_o = type(self)([x.stage_transform(stage) for x in self.trans_list],
                          **kwargs)
        my_o.name = self.name
        return my_o

    @property
    def end_trans(self):
        return type(self)([x.end_trans for x in self.trans_list], **self.kwargs)

    def has_stage_switch(self) -> bool:
        return any([x.has_stage_switch() for x in self.trans_list])

    @property
    def trans(self) -> "Transform":
        if self._trans is None:
            self._trans = type(self)([x.trans for x in self.trans_list], **self.kwargs)

        self._trans.stage = self.stage
        return self._trans

    @property
    def postprocess(self):
        tl_ = [x.postprocess for x in self.trans_list]
        if all([isinstance(t, Identity) for t in tl_]):
            return None
        return Parallel(tl_)

    def _forward_part(self):
        if self._forward is None:
            try:
                self._forward = type(self)(
                    [x.forward for x in self.trans_list],
                    **self.kwargs
                )
            except AttributeError:
                return Identity()
        return self._forward

    @classmethod
    def flatten_list(cls, list_):
        """
        :param cls:
        :param list_:
        """
        list_flat = []

        for t in list_:
            if isinstance(t, cls) and t.name is None:
                list_flat += t.trans_list
            else:
                list_flat.append(t)

        return list_flat

    @staticmethod
    def _trans_list_str(trans_list, types=False, mark=None, linestart=' ',
                        names=None):
        """
        :param trans_list:
        :param types:
        :param mark:
        :param linestart:
        :param names:
        """
        if names is None:
            names = list(range(len(trans_list)))
            nlen = 0
        else:
            nlen = max(*[len(n) for n in names])

        ncols = shutil.get_terminal_size().columns
        if types:
            tlen_ = max(*[len(str(x.type)) for x in trans_list] + [-1]) + 5
        else:
            tlen_ = 0

        def cutstr(s):
            """
            :param s:
            """
            if len(s) > ncols - tlen_ - 8 - nlen - 20:
                s = s[:ncols - tlen_ - 8 - nlen - 20] + '...    '
            s = f'{s:<{max(ncols - tlen_ - nlen - 20, 0)}}'
            return s

        return '\n'.join([f' {names[i]:<{nlen}}:  {linestart if i else " "} '
                          + cutstr(Transform._add_parentheses_if_needed(x,
                                                                        x.astr(False,
                                                                               compact=True))
                                   )
                          + (f'{str(x.type):<{tlen_}}' if types else '')
                          + (' <---' if mark == i else '')
                          for i, x in enumerate(trans_list)])

    def astr(self, with_type=True, compact=False, with_types=True):
        """
        :param with_type:
        :param compact:
        :param with_types:
        """
        if compact and self.name is not None:
            return Transform.astr(self, compact=True)
        if compact:
            a_str = f' {self._op_str} '.join([
                Transform._add_parentheses_if_needed(x, x.astr(False, compact=True))
                for x in self.trans_list])
            if with_type:
                a_str += f' - {self.type}'
        else:
            if with_type:
                a_str = ' ↓  ' + str(self.type.x) + '\n'
            else:
                a_str = ''
            a_str += '\33[1m' + self._trans_list_str(self.trans_list, with_types,
                                                     linestart=self._op_str) + '\33[0m'
            if with_type:
                a_str += '\n ↓  ' + str(self.type.y)
        return a_str

    def pretty_string(self, marker=None):
        return self.astr(with_type=True, compact=False, with_types=False)


class Layer(Transform):
    """ Transform wrapping a pytorch layer.

    :param layer: `torch.jit.ScriptModule` or `torch.nn.Module`
    :param layer_name: name of layer
    :param script: name of method to use (default: `forward`)
    :param _type: type tuple
    :param in_type: input type
    :param out_type: output type
    """

    def __init__(self, layer=None, layer_name='', script='forward', _type=None,
                 in_type=Any(), out_type=Any(), uid=None):

        if uid is None:
            uid = str(uuid4())

        super().__init__(in_type=in_type, out_type=out_type,
                         script=script, layer=layer, layer_name=layer_name,
                         _type=_type, uid=uid)

        self.script_fn = getattr(self.layer, self.script)

    def do(self, args):
        """
        :param args:
        """
        if type(args) in {tuple, list}:
            return self.script_fn(*args)
        if type(args) == torch.Tensor:
            return self.script_fn(args)
        raise TypeError('only support tensors or tuples/lists thereof')

    @property
    def layers(self):
        return {self.layer_name + f'_{self.uid}': self.layer}

    def train(self):
        self.layer.train()
        self._stage = 'train'

    def eval(self):
        self.layer.eval()
        self._stage = 'eval'

    def infer(self):
        self.layer.eval()
        self._stage = 'infer'

    def to_dict(self):
        """
        Convert layer to dictionaries
        :return:
        """

        layer_name = self.layer_name + f'_{self.uid}'
        my_dict = {
            'layer': '$' + layer_name,
            'script': self.script,
            'layer_name': self.layer_name,
            'uid': self.uid,
        }

        v = {layer_name: self.layer}

        self._add_type_to_dict(my_dict)

        res = {
            'cls': self.__class__.__module__ + '.' + self.__class__.__name__,
            'kwargs': my_dict,
            'handles': self._handle_defs
        }

        if self._name is not None:
            res['_name'] = self.name
        return res, v

    @classmethod
    def from_dict(cls, d, vars=None, handles=None):
        """
        Build from a dict
        :param d: dict
        :param vars: var dict
        :param handles:
        :return:
        """
        cls.type_from_dict(d)
        if not handles:
            handles = {}
        if 'uid' in d:
            layer_name = d['layer_name'] + f'_{d["uid"]}'
            uid = d['uid']
        else:
            layer_name = d['layer_name']
            uid = None

        t = cls(layer=vars[layer_name], layer_name=d['layer_name'],
                script=d['script'], _type=d.get('_type'), uid=uid)

        handles = cls._parse_handles(handles)
        for k in handles:
            t[k] = handles[k]

        return t

    def repr(self):
        str_ = super().repr()[:-1] + ': "$' + self.layer_name + '/' + \
               self.script + '"'
        str_ += '\n'
        str_ += '\n'.join(['    ' + x
                           for x in self.layer.__repr__().split('\n')])
        str_ += '>'
        return str_

    def to(self, device):
        """
        Move to device
        :param device: device
        """
        self._device = device
        self.layer.to(device)
        return self

    def astr(self, with_type=True, compact=True, with_types=False):
        return self.layer_name

    def traced(self, example=None):
        """Return a traced version of the layer.

        :param example: Example input needed for tracing. If *None*, will try to infer from
            input type.
        """
        if example is None:
            example = self.type.x.sample()
        return TracedLayer(layer=self.layer, example=example, in_type=self.type.x,
                           out_type=self.type.y, script=self.script, layer_name=self.layer_name)


class TracedLayer(Transform):
    """ Wrapper around a jit-traceable pytorch module.

    :param layer: `torch.nn.Module`
    :param example: example on which to perform tracing
    :param in_type: in-type
    :param out_type: out-type
    :param layer_name: name of layer
    :param script: name of method
    :param _type: tuple of `(in_type, out_type)`
    """

    def __init__(self, layer, example=None,
                 in_type=Any(), out_type=Any(),
                 layer_name='', script='forward', _type=None, uid=None):

        if uid is None:
            uid = str(uuid4())

        super().__init__(in_type=in_type, out_type=out_type, script=script, layer=layer,
                         layer_name=layer_name, _type=_type, example=example, uid=uid)

        self.script_fn = getattr(self.layer, self.script)

    def do(self, args):
        """
        :param args: Args to call with
        :return:
        """
        if type(args) in {tuple, list}:
            return self.script_fn(*args)
        if type(args) == torch.Tensor:
            return self.script_fn(args)
        raise TypeError('only support tensors or recursively tuples thereof')

    @property
    def layers(self):
        return {self.layer_name + f'_{self.uid}': self.layer}

    def train(self):
        self.layer.train()
        self._stage = 'train'

    def eval(self):
        self.layer.eval()
        self._stage = 'eval'

    def infer(self):
        self.layer.eval()
        self._stage = 'infer'

    def _clone(self):
        t = type(self)(
            layer=self.layer,
            example=self.example,
            layer_name=self.layer_name,
            script=self.script,
            _type=self.type
        )
        return t

    def to_dict(self):
        """
        Convert to dictionaries
        """
        my_dict = {}
        layer_name = self.layer_name + f'_{self.uid}'
        my_dict['layer'] = '$' + layer_name
        my_dict['script'] = self.script
        my_dict['layer_name'] = self.layer_name
        my_dict['uid'] = self.uid

        if not isinstance(self.layer, torch.jit.TopLevelTracedModule) and self.example is not None:
            with evalmodel(self.layer):
                if not isinstance(self.example, tuple):
                    example = self.example.to(self.device)
                else:
                    example = tuple([ex.to(self.device) for ex in self.example])
                traced_layer = torch.jit.trace(self.layer, example)
        else:
            traced_layer = self.layer

        v = {layer_name: traced_layer}

        self._add_type_to_dict(my_dict)

        res = {
            'cls': self.__class__.__module__ + '.' + self.__class__.__name__,
            'kwargs': my_dict,
        }

        if self._name is not None:
            res['_name'] = self.name

        return res, v

    @classmethod
    def from_dict(cls, d, vars=None, handles=None):
        """
        Build from dictionary
        :param d: dict
        :param vars: var dict
        :param handles:
        :return:
        """
        if '_type' in d:
            _type = Ftype.from_dict(d.pop('_type'))
            d = {**d, '_type': _type}
        else:
            _type = None
        if 'uid' in d:
            layer_name = d['layer_name'] + f'_{d["uid"]}'
            uid = d['uid']
        else:
            layer_name = d['layer_name']
            uid = None
        t = cls(layer=vars[layer_name], layer_name=d['layer_name'],
                script=d['script'], _type=_type, uid=uid)

        if handles is not None:
            for k in handles:
                t[k] = handles[k]

        return t

    from_dict.__doc__ = Transform.from_dict.__doc__

    def repr(self):
        str_ = super().repr()[:-1] + ': "$' + self.layer_name + '/' + self.script + '"'
        str_ += '\n'
        str_ += '\n'.join(['    ' + x for x in self.layer.__repr__().split('\n')])
        str_ += '>'
        return str_

    def to(self, device):
        """
        Move to device
        :param device: device
        """
        self._device = device
        self.layer.to(device)
        return self


class Compose(ListTransform):
    """Compose transformations.

    :param trans_list: List of transforms to compose.
    :param flatten: If *True* flatten transforms -
        `Compose([Compose([a,b]), c]) becomes Compose([a, b, c])``
    """

    def __init__(self, trans_list, flatten=False):
        assert settings['strict_types'] in {'silent', 'warn', 'debug', 'strict'}

        if flatten:
            trans_list = self.flatten_list(trans_list)

        self._type_chain = []

        super().__init__(trans_list, flatten=flatten)

        set_gpu = False
        set_bcpu = False
        for i, a_trans in enumerate(self.trans_list):
            if 'gpu' in self.trans_list[i].mapdevice:
                set_gpu = True
                continue
            if 'bcpu' in self.trans_list[i].mapdevice:
                set_bcpu = True
                continue
            if set_gpu and 'bcpu' not in a_trans.mapdevice:
                self._mapdevice_list[i] = {'gpu'}
            if set_bcpu:
                self._mapdevice_list[i] = {'bcpu'}

        self._op_str = '>'
        self._forward = None
        self._trans = None
        self._postprocess = None
        self._marker = None

    def __len__(self):
        with self.set_stage(self.stage):
            return len(self.trans_list[0])

    @property
    def requires_args(self):
        """
        :return: requires_args
        """
        return self.trans_list[0].requires_args

    def _check_types(self):
        trans_list = self.trans_list
        from_type = trans_list[0].type.copy({})
        last_trans = trans_list[0]
        self._type_chain = [from_type.x]
        for i, a_trans in enumerate(trans_list[1:]):
            to_type = a_trans.type.copy({})
            if isinstance(last_trans, IfInStage):
                warn("Condition based on stage not at the end "
                     "of the Transform-composition")
            try:
                from_type.y @ to_type.x
            except Mismatch:
                type_mismatch(
                    '\n\n' + self.pretty_string((i, f'{from_type.y} !@ {to_type.x}', 'after')) +
                    f'\n\n{last_trans.astr()}'
                    f'\n\n{a_trans.astr(compact=False)}'
                    '\n\n' + self._process_traceback() + '\n'
                )

            last_trans = a_trans
            self._type_chain.append(to_type.x)
            from_type = Ftype(from_type.x, to_type.y)
        self._type_chain.append(from_type.y)
        a_type = from_type
        Typevar.remove_single_names(self._type_chain)
        return a_type

    @property
    def trans(self):
        if self._trans is None:
            t = [
                t.trans for t, d in zip(self.trans_list, self._mapdevice_list) if 'cpu' in d
            ]

            if len(t) == 1:
                self._trans = t[0].trans
            elif t:
                self._trans = Compose(t)
            else:
                self._trans = Identity()

        self._trans.stage = self.stage
        return self._trans

    @property
    def postprocess(self):
        if self._postprocess is None:
            t = [
                t for t, d in zip(self.trans_list, self._mapdevice_list) if 'bcpu' in d
            ]

            if len(t) == 1:
                self._postprocess = t[0].postprocess
            elif t:
                self._postprocess = Compose(t)
            else:
                self._postprocess = Identity()

        try:
            self._postprocess.stage = self.stage
        except AttributeError:
            pass
        return self._postprocess

    def _list_of_forward_parts(self):
        """Accumulate all forward parts of the transforms"""
        ts_ = []
        for a_trans, dev in zip(self.trans_list, self._mapdevice_list):
            if 'gpu' in dev:
                if len(dev) == 1:
                    ts_.append(a_trans)
                else:
                    ts_.append(a_trans.forward)
        return ts_

    def _forward_part(self):
        """
        Forward part of transforms
        :return:
        """
        if self._forward is None:
            ts_ = self._list_of_forward_parts()

            if len(ts_) == 1:
                self._forward = ts_[0]
            elif ts_:
                self._forward = Compose(ts_)
            else:
                self._forward = Identity()

        try:
            self._forward.stage = self.stage
        except AttributeError:
            pass
        return self._forward

    def do(self, arg):
        """
        :param arg: Arg to call with
        :return:
        """
        _in_arg = arg
        for i, trans_ in enumerate(self.trans_list):
            self._marker = i, '', 'on'
            try:
                arg = trans_._do(arg)
            except Exception as err:
                self._trace_error(i, _in_arg)
                raise err

        self._marker = None

        return arg

    def repr(self):
        """
        String representation
        """
        start = 'lf.transforms.Compose([\n'
        lines = []
        for x in self.trans_list:
            lines.extend(['    ' + y for y in x.repr().split('\n')])
        body = '\n'.join(lines)
        end = '\n])'
        return start + body + end

    @property
    def end_trans(self):
        return [x.end_trans for x in self.trans_list][-1]

    @staticmethod
    def _update_string(res_, i, marker, marker_n, marker_text):
        if marker is not None and marker_n == i:
            res_ += '\033[31m  <---- ' + marker_text + '\033[0m'
        return res_

    def _pretty_string(self, type_chain, marker, marker_n, marker_text, marker_pos):
        """Pretty string for transform"""
        res = ''
        for i, (a_trans, type_) in enumerate(
                itertools.zip_longest(self.trans_list, type_chain[1:], fillvalue='?')):
            res += f'{i:>2}: ' + '\033[1m' + a_trans.astr(compact=True, with_type=False,
                                                          with_types=False) + '\033[0m'
            if marker_pos == 'on':
                res = self._update_string(res, i, marker, marker_n, marker_text)
            res += '\n'
            if i != len(self.trans_list) - 1:
                if str(type_) == 'void':
                    res += ''
                else:
                    res += ' ↓  ' + str(type_)
            if marker_pos != 'on':
                res = self._update_string(res, i, marker, marker_n, marker_text)
            if i != len(self.trans_list) - 1:
                res += '\n'
        return res

    def pretty_string(self, marker=None):
        """
        Pretty name for transform

        :param marker: marker
        """

        if marker is None and self._marker is not None:
            marker = self._marker

        marker_n = marker_text = marker_pos = None
        if marker is not None:
            marker_n, marker_text, marker_pos = marker

        name_of_trans = self.fullname()

        if self._type_chain:
            type_chain = self._type_chain
        else:
            type_chain = ['?'] * (len(self.trans_list) + 1)

        res = self._pretty_string(type_chain, marker, marker_n, marker_text, marker_pos)
        return name_of_trans + self._add_type_to_str(res, False)


class NamedParallel(Transform): # consider merging with Parallel
    """ Apply transforms in parallel to tuples of inputs.
    Same as *Parallel* except that transforms get a name.

    :param return_dict: toggle to `True` to return a dictionary
    :param **kwargs: key-values for transforms in parallel
    """
    _op_str = '/'

    def __init__(self, return_dict=False, **kwargs):
        trans_list = []
        keys = []
        for x in kwargs:
            trans_list.append(kwargs[x])
            keys.append(x)

        for x in keys:
            setattr(self, x, kwargs[x])

        super().__init__(return_dict=return_dict, trans_list=trans_list, keys=keys, kwargs=kwargs)
        self.kwargs = kwargs
        self._mapdevice = set.union(*[t.mapdevice for t in trans_list])

    def _versions(self):
        versions = {}
        for t in self.trans_list:
            versions.update(t._versions())
        return versions

    def __len__(self):
        with self.set_stage(self.stage):
            lens = [len(x) for x in self.trans_list]
            assert len(set(lens)) == 1
            return lens[0]

    def _check_types(self):
        """
        Check types
        """
        trans_list = self.trans_list
        types = [t.type.copy() for t in trans_list]
        in_type = tuple(t.x for t in types)
        out_type = tuple(t.y for t in types)

        return Ftype.build(in_type, out_type)

    def stage_transform(self, stage):
        """
        :param stage:
        :return:
        """
        my_o = type(self)(**{k: x.stage_transform(stage) for k, x in self.kwargs.items()})
        my_o.name = self.name
        return my_o

    def has_stage_switch(self):
        return any(x.has_stage_switch() for x in self.trans_list)

    @property
    def trans(self):
        t = NamedParallel(**{k: v.trans for k, v in self.kwargs.items()})
        t.stage = self.stage
        return t

    def _forward_part(self):
        t = NamedParallel(**{k: v.forward for k, v in self.kwargs.items()})
        t.stage = self.stage
        return t

    @property
    def postprocess(self):
        tl_ = {
            k: (x.postprocess if x.postprocess is not None else Identity())
            for k, x in self.kwargs.items()
        }
        if all([isinstance(t, Identity) for t in tl_.values()]):
            return None

        t = NamedParallel(**tl_)
        t.stage = self.stage
        return t

    def to_dict(self):
        """
        Convert to dict
        """
        my_dict = {}
        v = {}
        for x in self.kwargs:
            if isinstance(self.kwargs[x], Transform):
                my_dict[x], temp = self.kwargs[x].to_dict()
                v.update(temp)
            elif isinstance(self.kwargs[x], list) and self.kwargs[x] and \
                    isinstance(self.kwargs[x][0], Transform):
                val = []
                for y in self.kwargs[x]:
                    z, w_dict = y.to_dict()
                    val.append(z)
                    v.update(w_dict)
            else:
                my_dict[x] = self.kwargs[x]

        res = {'cls': self.__module__ + '.' + self.__class__.__name__, 'kwargs': my_dict}

        if self.name is not None:
            res['_name'] = self.name

        return res, v

    def do(self, args):
        """
        :param args: Args to call with
        :return:
        """
        out = []
        for i, trans in enumerate(self.trans_list):
            try:
                out.append(trans._do(args[i]))
            except Exception as err:
                self._trace_error(i, args)
                raise err
        return out

    def repr(self):
        """
        String representation
        """
        start = 'lf.transforms.NamedParallel(\n'
        lines = []
        for _, k in enumerate(self.keys):
            new_lines = ['    ' + k + ':']
            new_lines.extend([
                '        ' + x
                for x in self.kwargs[k].__repr__().split('\n')
            ])
            lines.extend(new_lines)
        body = '\n'.join(lines)
        end = '\n)'
        return start + body + end

    def __getitem__(self, item):
        if isinstance(item, int):
            return self.trans_list[item]
        return super().__getitem__(item)

    def astr(self, with_type=True, compact=False, with_types=True):
        if compact:
            a_str = f' {self._op_str} '.join([
                Transform._add_parentheses_if_needed(x, x.astr(False, compact=True))
                for x in self.trans_list])
            if with_type:
                a_str += f' - {self.type}'
        else:
            a_str = ListTransform._trans_list_str(
                self.trans_list, with_types,
                linestart=self._op_str,
                names=[f'{i}: {k}' for i, k in enumerate(self.kwargs)]
            )
            if with_type:
                a_str += f'\n\n{self.type}'
        return a_str

    @property
    def end_trans(self):
        return NamedParallel(**{k: v.end_trans
                                for k, v in self.kwargs.items()})

    def pretty_string(self, marker=None):
        """
        Pretty name for transform
        :return:
        """
        t = '\33[1mParallel'
        if self.name is not None:
            t += f'[{self.name}]'
        t += ':\33[0m\n\n'
        a_str = ''
        for (i, n), trans in zip(enumerate(self.kwargs), self.trans_list):
            a_str += ' ↴  ' + str(trans.type.x) + '\n'
            a_str += f'{i:>2}: ({n}) ' + '\033[1m' + trans.astr(compact=True, with_type=False,
                                                                with_types=False) + '\033[0m'
            a_str += '\n'
            a_str += ' ↳  ' + str(trans.type.y)
            if marker is not None and marker[0] == i:
                a_str += '\033[31m  <---- ' + marker[1] + '\033[0m'
            a_str += '\n'
        a_str = self._add_type_to_str(a_str, False)
        return t + a_str


class Parallel(ListTransform): # consider renaming
    """ Apply transforms in parallel to a tuple of inputs.

    Parallel([f1, f2, ...])((x1, x2, ..)) := (f1(x1), f2(x2), ...)
    """

    def __init__(self, trans_list, flatten=False):
        """
        :param trans_list:
        :param flatten:
        """
        if flatten:
            trans_list = self.flatten_list(trans_list)
        super().__init__(trans_list=trans_list, flatten=flatten)
        self._op_str = '/'

    def __len__(self):
        with self.set_stage(self.stage):
            lens = [len(x) for x in self.trans_list]
            assert len(set(lens)) == 1
            return lens[0]

    def _check_types(self):
        """
        Check types
        """
        trans_list = self.trans_list
        types = [t.type.copy() for t in trans_list]
        in_type = tuple(t.x for t in types)
        out_type = tuple(t.y for t in types)
        return Ftype.build(in_type, out_type)

    def do(self, args):
        """
        :param args: Args to call with
        :return:
        """
        out = []
        i = 0
        if sum([t.requires_args for t in self.trans_list]) == 1:
            args = (args,)
        for trans in self.trans_list:
            try:
                if trans.requires_args:
                    out.append(trans._do(args[i]))
                    i += 1
                else:
                    out.append(trans._do(None))
            except Exception as err:
                self._trace_error(i, args)
                raise err
        return tuple(out)

    def repr(self):
        """
        String representation
        """
        start = 'lf.transforms.Parallel(\n'
        lines = []
        for trans in self.trans_list:
            lines.extend(['    ' + y for y in trans.repr().split('\n')])
        body = '\n'.join(lines)
        end = '\n)'
        return start + body + end

    def pretty_string(self, marker=None):
        """
        Pretty name for transform
        :return:
        """
        t = '\33[1mParallel'
        if self.name is not None:
            t += f'[{self.name}]'
        t += ':\33[0m\n\n'
        a_str = ''
        for i, trans in enumerate(self.trans_list):
            a_str += ' ↴  ' + str(trans.type.x) + '\n'
            a_str += f'{i:>2}: ' + '\033[1m' + trans.astr(compact=True, with_type=False,
                                                          with_types=False) + '\033[0m'
            a_str += '\n'
            a_str += ' ↳  ' + str(trans.type.y)
            if marker is not None and marker[0] == i:
                a_str += '\033[31m  <---- ' + marker[1] + '\033[0m'
            a_str += '\n'
        a_str = self._add_type_to_str(a_str, False)
        return t + a_str


class Rollout(ListTransform): # consider renaming
    """Apply several transforms to the same set of arguments and return a
    list of results.

    Rollout([t1, t2, ...])(x) := [t1(x), t2(x), ...]

    :param trans_list: Transforms that should be rolled out.
    """

    def __init__(self, trans_list, flatten=False):
        """
        :param trans_list:
        :param flatten:
        """
        if flatten:
            trans_list = self.flatten_list(trans_list)

        super().__init__(trans_list=trans_list, flatten=flatten)

        self._op_str = '+'
        self._forward = None

    def __len__(self):
        with self.set_stage(self.stage):
            lens = [len(x) for x in self.trans_list]
            assert len(set(lens)) == 1
            return lens[0]

    def _check_types(self):
        """
        Check types
        """
        trans_list = self.trans_list
        type_copies = [t.type.copy({}) for t in trans_list]

        mappings = {}
        try:
            Typevar.match_all([t.x for t in type_copies])
        except Mismatch:
            type_mismatch('In "Rollout": Types don\'t match')
            in_type = Any()
        else:
            in_type = trans_list[0].type.x.copy(mappings)
        out_type = tuple(t.type.y.copy(mappings) for t in trans_list)
        return Ftype.build(in_type, out_type)

    def do(self, x):
        """
        :param x: args to call with
        :return:
        """
        out = []
        for i, t in enumerate(self.trans_list):
            try:
                out.append(t._do(x))
            except Exception as err:
                self._trace_error(i, x)
                raise err
        out = tuple(out)
        return out

    def repr(self):
        """
        String representation
        """
        start = 'lf.transforms.Rollout([\n'
        lines = []
        for x in self.trans_list:
            lines.extend(['    ' + y for y in x.repr().split('\n')])
        body = '\n'.join(lines)
        end = '\n])'
        return start + body + end

    def _forward_part(self):
        """
        Forward part
        """
        if self._forward is None:
            if len(list(self._mapdevice)) >= 2 and 'gpu' in self._mapdevice:
                self._forward = Parallel([x.forward for x in self.trans_list])
            else:
                self._forward = Rollout([x.forward for x in self.trans_list])
        return self._forward

    @property
    def postprocess(self):
        tl_ = [x.postprocess for x in self.trans_list]
        if all([isinstance(t, Identity) for t in tl_]):
            return Identity()
        if len(list(self._mapdevice)) >= 2 and 'bcpu' in self._mapdevice:
            return Parallel(tl_)
        return Rollout(tl_)

    def pretty_string(self, marker=None):
        """
        Pretty name for Rollout
        """
        t = '\33[1mRollout'
        if self.name is not None:
            t += f'[{self.name}]'
        t += ':\33[0m\n\n'
        a_str = ''
        for i, trans in enumerate(self.trans_list):
            a_str += f'{i:>2}: ' + '\033[1m' + trans.astr(compact=True, with_type=False,
                                                          with_types=False) + '\033[0m'
            a_str += '\n'
            a_str += ' ↳  ' + str(trans.type.y)
            if marker is not None and marker[0] == i:
                a_str += '\033[31m  <---- ' + marker[1] + '\033[0m'
            a_str += '\n'
        a_str = self._add_type_to_str(a_str, False)
        return t + a_str


class NamedRollout(Rollout): # ? discuss merging with Rollout
    """
    Apply several transforms to the same set of arguments and return a
    list of results.
    :param flatten:
    :param trans_list: Transforms that should be rolled out.
    """

    def __init__(self, flatten=False, **kwargs):

        self.keys, trans_list = zip(*kwargs.items())
        super().__init__(trans_list=trans_list, flatten=flatten)

    def __len__(self):
        with self.set_stage(self.stage):
            lens = [len(x) for x in self.trans_list]
            assert len(set(lens)) == 1
            return lens[0]

    def _clone(self) -> "NamedRollout":
        """
        Clone NamedRollout
        """
        kwargs = self.kwargs
        if 'flatten' in kwargs:
            kwargs['flatten'] = False
        for k, t in zip(self.keys, self.trans_list):
            kwargs[k] = t
        res = type(self)(**kwargs)
        try:
            res.flatten = self.flatten
        except AttributeError:
            pass
        return res

    def to_dict(self) -> tp.Tuple[tp.Dict, tp.Dict]:
        """
        Convert to dictionary
        """
        my_dict = {}
        v = {}
        for k, t in zip(self.keys, self.trans_list):
            my_dict[k], temp = t.to_dict()
            v.update(temp)
        my_dict['flatten'] = self.flatten
        res = {'cls': self.__module__ + '.' + self.__class__.__name__, 'kwargs': my_dict}
        if self.name is not None:
            res['_name'] = self.name
        return res, v

    def do(self, args):
        """
        :param args: Args to call with
        :return:
        """
        out = {}
        for i, trans in enumerate(self.trans_list):
            try:
                out[self.keys[i]] = trans._do(args)
            except Exception as err:
                self._trace_error(i, args)
                raise err
        return out


class Map(Transform):
    """Apply one transforms to each element of a list.

    >>> Map(t)([x1, x2, x3]) == [t(x1), t(x2), t(x3)]
    True

    """

    def __init__(self, transform):
        """
        :param transform: The transform that should be mapped.
        """
        super().__init__(transform=transform)
        self._mapdevice = transform.mapdevice

    def _check_types(self):
        mappings = {}
        in_type = Sequence(len='?a', vtype=self.transform.type.x.copy(mappings))
        out_type = Sequence(len='?a', vtype=self.transform.type.y.copy(mappings))
        return Ftype.build(in_type, out_type)

    def do(self, arglist):
        """
        :param arglist: Args list to call transforms with
        :return:
        """
        return [self.transform._do(arg) for arg in arglist]

    def repr(self):
        """
        String representation
        """
        start = 'lf.transforms.Map(\n'
        lines = ['    ' + y for y in self.transform.repr().split('\n')]
        body = '\n'.join(lines)
        end = '\n)'
        return start + body + end

    def astr(self, with_type=True, compact=True, with_types=False):
        trs = self.transform.astr(False, compact=compact)
        a_str = '~ ' + Transform._add_parentheses_if_needed(self.transform, trs)
        if with_type:
            a_str += f' - {self.type}'
        return a_str

    def pretty_string(self, marker=None):
        """
        Pretty name
        """
        a_str = self.astr(with_type=False, with_types=False, compact=True)
        return self._add_type_to_str(a_str)


def flatten(x): #? how to determine type
    """ Flatten a tuple.

    Example:

    >>> flatten(((1,2), 3, ((4,), 5)))
    (1, 2, 3, 4, 5)
    """
    if not isinstance(x, tuple):
        return (x,)
    return sum([flatten(y) for y in x], ())


class TeleportWriter(Transform):
    """
    Class to make it easier to join far away points of a transform.
    This is the input part
    """

    context_vars = {}

    def __init__(self, id, alias=None):
        """
        :param name: name of variable
        """
        self.var = contextvars.ContextVar(id)
        self.context_vars[id] = self.var
        super().__init__(id=id, alias=alias, in_type=Any('?a'), out_type=Any('?a'))

    def do(self, args):
        """
        :param args: args to set
        :return:
        """
        self.var.set(args)
        return args

    def astr(self, with_type=False, compact=False, with_types=False):
        return f'⇝ {self.alias}'


class TeleportReader(Transform):
    """
    Output part of teleport.
    """

    requires_args = False

    def __init__(self, id, alias=None, out_type=None):
        """
        :param name: name of variable
        :param type_: type_ of variable
        """
        super().__init__(id=id, alias=alias, in_type=Void(), out_type=out_type)
        self.var = TeleportWriter.context_vars[id]

    def do(self, *args):
        """
        :param args:
        :return:
        """
        try:
            res = self.var.get()
            return res
        except LookupError:
            raise LookupError('Trying to read from empty teleport.')

    def __getitem__(self, item):
        return self >> x[item]

    def astr(self, with_type=False, compact=False, with_types=False):
        return f'{self.alias}⇝'


class Choose(Transform): # do we need this I've never used it
    """Choose one of two transforms based on the value of *condition* at
    execution time.

    ??a duplicate: remove, use If below
    """

    def __init__(self, trans_a=None, trans_b=None, condition=None):
        """
        :param trans_a:
        :param trans_b:
        :param condition:
        """
        warn("Deprecated: Use lf.transforms.If instead")
        super().__init__(trans_a=trans_a, trans_b=trans_b,
                         condition=condition)

    def do(self, args):
        """
        :param args: Args to call transforms with
        :return:
        """
        if self.condition(args):
            return self.trans_a._do(args)
        return self.trans_b._do(args)

    def _check_types(self):
        trans_a, trans_b = self.trans_a, self.trans_b
        # make sure the types of both transforms match
        try:
            _type = trans_a.type.copy() @ trans_b.type.copy()
        except Mismatch:
            type_mismatch("In Choose: Types don't match:\n\n"
                          f'{trans_a.type} !@ {trans_b.type}')
            _type = Ftype.build(Any(), Any())
        return _type

    def _versions(self):
        versions = self.trans_a._versions().copy()
        versions.update(self.trans_b._versions())
        return versions

    @property
    def trans(self):
        trans = Choose(trans_a=self.trans_a.trans, trans_b=self.trans_b.trans,
                       condition=self.condition)
        trans.stage = self.stage
        return trans

    def stage_transform(self, stage):
        """
        Set strage for transforms
        :param stage: stage
        :return:
        """
        my_o = type(self)(self.trans_a.stage_transform(stage),
                          self.trans_b.stage_transform(stage), **self.kwargs)
        my_o.name = self.name
        return my_o

    def has_stage_switch(self):
        return self.trans_a.has_stage_switch() or self.trans_b.has_stage_switch()


class If(Transform):
    """
    Perform *t* if *condition* (a transform) holds at execution time, else perform *else_*.
    :param t: transform for the if part
    :param condition: test of the condition (outputs `bool`)
    :param else_: transform for the else part
    """

    def __init__(self, t, condition, else_=None):
        if else_ is None:
            else_ = Identity()
        super().__init__(t=t, condition=condition, else_=else_)

    def _versions(self):
        versions = self.t._versions().copy()
        if self.else_ is not None:
            versions.update(self.else_._versions())
        return versions

    def _clone(self):
        """
        Clone this
        """
        return If(self.t, self.condition, self.else_)

    def _check_types(self):
        return self.t.type.copy()

    def do(self, *args):
        """
        :param args: Args to call transforms with
        """
        if self.condition._do(*args):
            return self.t._do(*args)
        return self.else_._do(*args)

    def repr(self):
        """
        String representation
        """
        str_ = 'If [{}]:'.format(self.condition)
        if_lines = '    ' + '    '.join(self.transform.repr().split('\n'))
        else_lines = '    ' + '    '.join(self.else_.repr().split('\n'))
        str_ += '\n' + if_lines + '\nElse:\n' + else_lines
        return str_

    @property
    def trans(self):
        trans = type(self)(t=self.t.trans, condition=self.condition.trans, else_=self.else_.trans)
        trans.stage = self.stage
        return trans

    def stage_transform(self, stage):
        """
        Set stage of transform
        :param stage: stage
        :return:
        """
        my_o = type(self)(self.t.stage_transform(stage),
                          self.condition.stage_transform(stage),
                          self.else_.stage_transform(stage))
        my_o.name = self.name
        return my_o

    def has_stage_switch(self):
        return self.t.has_stage_switch() or self.condition.has_stage_switch()


class IfInStage(Transform):
    """Perform *t* if self.stage is *target_stage*, else perform *else_*.
    """

    def __init__(self, t, target_stage, else_=None):
        """
        :param t: transform for the if part
        :param target_stage: stage {'train', 'eval', 'infer'}
        :param else_: transform for the else_ part
        """
        if else_ is None:
            else_ = Identity()
        super().__init__(
            t=t,
            target_stage=target_stage,
            else_=else_,
        )
        self._mapdevice = set.union(*[t.mapdevice for t in [self.t, self.else_]])

    def _versions(self):
        versions = self.t._versions().copy()
        if self.else_ is not None:
            versions.update(self.else_._versions())
        return versions

    def _clone(self):
        return type(self)(self.t, self.target_stage, self.else_)

    def stage_transform(self, stage):
        if self.target_stage == stage:
            return self.t.stage_transform(stage)
        return self.else_.stage_transform(stage)

    def has_stage_switch(self):
        return True

    def infer_transform(self):
        return self.stage_transform('infer')

    def train_transform(self):
        return self.stage_transform('train')

    def eval_transform(self):
        return self.stage_transform('eval')

    def do(self, *args):
        if self.stage == self.target_stage:
            return self.t._do(*args)
        return self.else_._do(*args)

    def repr(self):
        lines = ['If{}{}:'.format(self.target_stage[0].upper(), self.target_stage[1:])]
        lines.extend(['    ' + x for x in self.t.repr().split('\n')])
        lines.append('Else:')
        lines.extend(['    ' + x for x in self.else_.repr().split('\n')])
        return '\n'.join(lines)

    @property
    def trans(self):
        trans = type(self)(t=self.t.trans, target_stage=self.target_stage,
                           else_=self.else_.trans)
        trans.stage = self.stage
        return trans

    def _forward_part(self):
        trans = type(self)(t=self.t.forward, target_stage=self.target_stage,
                           else_=self.else_.forward)
        trans.stage = self.stage
        return trans

    @property
    def postprocess(self):
        trans = type(self)(t=self.t.postprocess, target_stage=self.target_stage,
                           else_=self.else_.postprocess)
        trans.stage = self.stage
        return trans

    @property
    def postprocess_with_fixed_stage(self) -> "Transform":
        """ Get the postprocess transform while fixing it to the current stage (removing all
        branches that belong to a different stage). """
        if self.stage == self.target_stage:
            return self.t.postprocess_with_fixed_stage
        return self.else_.postprocess_with_fixed_stage

    def astr(self, with_type=True, compact=True, with_types=False):
        return f'if {self.target_stage}: ({self.t.astr(False, True)}) else: ({self.else_.astr(False, True)})'

    @property
    def end_trans(self):
        if self.target_stage == 'infer':
            return self.t
        return Identity()


def IfInfer(t, else_=None):
    """
    :param t: transform for infer phase
    :param else_: transform otherwise
    """
    return IfInStage(t, 'infer', else_=else_)


def IfTrain(t, else_=None):
    """
    :param t: transform for train phase
    :param else_: transform otherwise
    """
    return IfInStage(t, 'train', else_=else_)


def IfEval(t, else_=None):
    """
    :param t: transform for eval phase
    :param else_: transform otherwise
    """
    return IfInStage(t, 'eval', else_=else_)


class Identity(Transform):
    """Do nothing."""

    def __init__(self):
        super().__init__(in_type=Any('?a'), out_type=Any('?a'))

    @property
    def is_identity(self):
        return True

    def do(self, *args):
        # remove, make consistent
        if len(args) == 1:
            return args[0]
        return args


class Try(Transform):
    """ Perform *transform*, if this fails with any exception from
    *exceptions*, perform *catch_transform*.
    """

    def __init__(self, transform, catch_transform, exceptions):
        """
        :param transform: transform to try
        :param catch_transform: transform to fall back on
        :param exceptions: catch conditions
        """
        if type(exceptions) not in (tuple, list):
            exceptions = (exceptions,)
        exception_strings = tuple([e if isinstance(e, str)
                                   else e.__name__ for e in exceptions])
        super().__init__(transform=transform,
                         catch_transform=catch_transform,
                         exceptions=exception_strings)
        exceptions = []
        for es_ in exception_strings:
            ex = eval(es_)
            assert issubclass(ex, Exception)
            exceptions.append(ex)
        self._exceptions = tuple(exceptions)

    def _versions(self):
        versions = self.transform._versions().copy()
        versions.update(self.catch_transform._versions())
        return versions

    def __len__(self):
        with self.set_stage(self.stage):
            return len(self.transform)

    def _clone(self):
        return Try(self.transform, self.catch_transform, self.exceptions)

    @classmethod
    def _alternate_init_(cls, t, s, exceptions):
        warn('using lecacy signature Try(t, s, exceptions)')
        return cls(t, s, exceptions)

    def _check_types(self):
        return self.transform.type.copy()

    def do(self, *args):
        try:
            return self.transform._do(*args)
        except self._exceptions as err:
            return self.catch_transform._do(*args)

    def repr(self):
        str_ = 'Try:\n'
        str_ += '    ' + '\n    '.join(self.transform.repr().split('\n')) + '\n'
        str_ += 'Except ['
        if isinstance(self.exceptions, tuple):
            str_ += '(' + ','.join(self.exceptions) + ')]:\n'
        else:
            str_ += self.exceptions + ']:\n'
        str_ += '    ' + '\n    '.join(self.catch_transform.repr().split('\n')) + '\n'
        return str_

    def stage_transform(self, stage):
        my_o = type(self)(self.transform.stage_transform(stage),
                          self.catch_transform.stage_transform(stage), self.exceptions)
        my_o.name = self.name
        return my_o

    def has_stage_switch(self):
        return self.transform.has_stage_switch() or \
               self.catch_transform.has_stage_switch()

    def astr(self, with_type=True, compact=True, with_types=False):
        if compact and self.name is not None:
            return self.name
        return (
            f'try: [{self.transform.astr(with_type=False, compact=True, with_types=False)}] '
            f'except ({", ".join(self.exceptions)}): '
            f'[{self.catch_transform.astr(with_types=False, compact=True, with_type=False)}]'
        )

    def pretty_string(self, marker=None):
        a_str = 'try:\n'
        a_str += f'        {self.transform.astr(with_type=False, compact=True, with_types=False)}\n'
        a_str += f'    except ({", ".join(self.exceptions)}):\n'
        a_str += f'        {self.catch_transform.astr(with_type=False, compact=True, with_types=False)}'
        return self._add_type_to_str(a_str)


class Unbatchify(Transform):
    """ Remove batch dimension (inverse of Batchify). """

    def __init__(self, dim=0):
        """
        :param dim: batching dimension
        """
        super().__init__(dim=dim)

    def do(self, args):
        if self.stage != 'infer':
            return args
        if isinstance(args, tuple):
            return tuple([self(x) for x in args])
        if isinstance(args, torch.Tensor):
            return args.squeeze(self.dim)
        raise TypeError('only tensors and tuples of tensors recursively supported...')


class Batchify(Transform):
    """Add a batch dimension at dimension *dim*. During inference, this unsqueezes
    tensors and, recursively, tuples thereof.

    :param dim: batching dimension
    """

    def __init__(self, dim=0):
        super().__init__(dim=dim)

    def _to_match_with(self, other, for_x: bool):
        """Get a type variable to match the input- or output- type variable with.

        (see _check_types).

        :param for_x: If *True*, this is for the input variable, else for the output
            variable.
        """
        if for_x:
            my_shape = [f'...{self.dim}a', '...c']
            other_shape = [f'...{self.dim}a', '?b', '...c']
        else:
            my_shape = [f'...{self.dim}a', '?b', '...c']
            other_shape = [f'...{self.dim}a', '...c']

        other = val(other)
        # if the other type is Any, don't do anything
        if isinstance(val(other.type), AnyType):
            return other

        # if the other type is a tuple typevar, recusively match with all
        if other.is_tuple_typevar:
            return Typevar.build(
                tuple(self._to_match_with(o, for_x) for o in val(other.type)))

        # match the shape
        uuid = str(uuid4())
        shape = Matchable.convert(my_shape, uuid)
        if 'shape' in other:
            shape @ other['shape']

        # if this is a sequence of tensors
        if val(other.type) is not None and val(other.type) <= Sequence:
            return Typevar.build(
                val(other.type)(
                    vtype=Tensor(shape=other_shape)))

        # if this is a tensor
        return Typevar.build(Tensor(shape=other_shape), uuid)

    def _construct_ftype(self, x, y):
        """ Type constructor (see _check_types).

        :param x: Input type.
        :param y: Output type.
        """

        def out_type_from_in_type(other):
            y.match_callback = None
            y @ self._to_match_with(other, True)

        x.match_callback = out_type_from_in_type

        def in_type_from_out_type(other, top_level=True):
            x.match_callback = None
            x @ self._to_match_with(other, False)

        y.match_callback = in_type_from_out_type

    def _check_types(self):
        """Type check for batchify:

        Batchify adds a batch dimension. Therefore, the output type of batchify is the same as
        its input with one extra dimension for its shape attribute.

        E.g.:
            Tensor(shape=[100, 200]) -> Batchify() -> Tensor(shape=[1, 100, 200])

        As input and output type variables can be free, we need to use match callbacks.
        """
        # only do this in strict mode, otherwise will cause problems
        if settings['strict_types'] != 'strict':
            return Ftype.build(Any(), Any())

        in_type = Typevar(Mvar())
        out_type = Typevar(Mvar())

        return Ftype(in_type, out_type, self._construct_ftype)

    def do(self, args):
        if self.stage != 'infer':
            return args
        if type(args) in {tuple, list}:
            return tuple([self(x) for x in args])
        if isinstance(args, torch.Tensor):
            return args.unsqueeze(self.dim)
        if type(args) in [float, int]:
            return torch.tensor([args])
        raise TypeError('only tensors and tuples of tensors recursively supported...')


class CPU(Transform):
    """
    Mark the beginning of the postprocess part of the transform on
    the cpu
    """

    def __init__(self, unbatchify=True):
        """
        :param unbatchify:
        """
        super().__init__(unbatchify=unbatchify)
        self._mapdevice = {'bcpu'}

    def do(self, args):
        args = convert(args, 'cpu')
        if self.unbatchify:
            unb = Unbatchify()
            unb.stage = self.stage
            args = unb._do(args)
        return args


class GPU(Transform):
    """
    Mark the beginning of the gpu-part of the transform.
    """

    def __init__(self, batchify=False):
        """
        :param batchify:
        """
        super().__init__(batchify=batchify)
        self._mapdevice = {'gpu'}

    def do(self, args):
        if self.batchify:
            my_b = Batchify()
            my_b.stage = self.stage
            args = my_b._do(args)
        return convert(args, self.device)


def strip_decorator(source):
    """
    :param source:
    :return:
    """
    source = source.split('\n')
    start = next(i for i, x in enumerate(source) if x.strip().startswith('def')
                 or x.strip().startswith('class'))
    return [x for x in source[start:] if x]


class Function(Transform):
    """
    Convert a regular function to a Transform. The function must not make use
    of any external variables except of modules specified in *imports*.
    For lambda functions use either `Lambda` or `Dill`.

    :param f: function
    :param in_type: in-type
    :param out_type: out-type
    :param _type: `(in_type, out_type)`
    :param imports: modules to import
    """

    def __init__(self, f, source=None, in_type=Any(), out_type=Any(),
                 imports=()):
        if source is None:
            source = strip_decorator(inspect.getsource(f))
            if source[0].strip().startswith('@'):
                source = source[1:]
            if source[0][0] != ' ':
                padding = 0
            else:
                padding = [i for i, x in enumerate(source[0]) if x != ' '][0]

            source = '\n'.join([x[padding:] for x in source])

        super().__init__(f=f, imports=imports, source=source, in_type=in_type, out_type=out_type)

        self._arity = len(inspect.signature(self.f).parameters)

    def _clone(self):
        type_ = self.type.copy()
        return type(self)(f=self.f,
                          source=self.source,
                          imports=self.imports,
                          in_type=type_.x,
                          out_type=type_.y)

    def repr(self):
        return 'Function:\n' + self.source

    def pretty_string(self, marker=None):
        return self.fullname() + self._add_type_to_str(self.source)

    def to_dict(self):
        my_dict = {
            'defn': self.source,
            'imports': self.imports,
            'name': self.f.__name__
        }

        self._add_type_to_dict(my_dict)
        res = {
            'cls': 'Function',
            'kwargs': my_dict,
            'handles': self._handle_defs
        }
        if self.name is not None:
            res['_name'] = self.name

        return res, {}

    def _versions(self):
        versions = {}
        for k in self.imports:
            if 'as' in k:
                k = k.split(' as ')[0].strip()
            package = k.split('.')[0]
            try:
                versions[package] = importlib.import_module(package).__version__
            except AttributeError as err:
                if '__version__' in str(err):
                    warn(str(err))
                else:
                    raise err
        return versions

    @staticmethod
    def _source_with_imports(source, imports):
        if not imports:
            return source

        defn = ''
        lines = source.split('\n')
        for i, line in enumerate(lines):
            a_str = re.split(r'\)\s*:', line)
            if len(a_str) > 1:
                assert len(a_str) == 2
                defn += a_str[0] + '):' + '\n'
                if a_str[1].strip():
                    defn += ';'.join(['import ' + x for x in imports])
                else:
                    leading_spaces = len(lines[i + 1]) - len(lines[i + 1].lstrip())
                    ind = ' ' * leading_spaces
                    defn += ind + f'\n{ind}'.join(['import ' + x for x in imports])
                defn += a_str[1] + '\n'
                break
            defn += line
        return defn + '\n'.join(lines[i + 1:])

    @classmethod
    def from_dict(cls, d, vars=None, handles=None):
        """
        Build from dict
        :param d: dict
        :param vars: var dict
        :param handles:
        :return:
        """
        temp_module = ModuleType('_temp')
        exec(cls._source_with_imports(d['defn'], d['imports']), temp_module.__dict__)

        _type = Ftype.from_dict(d.pop('_type'))

        f = getattr(temp_module, d['name'])

        obj = cls(f, d['defn'], _type.x, _type.y, imports=d['imports'])

        handles = cls._parse_handles(handles)

        for k in handles:
            obj[k] = handles[k]

        return obj

    from_dict.__doc__ = Transform.from_dict.__doc__

    def do(self, *args, **kwargs):
        return self.f(*args, **kwargs)

    def astr(self, with_type=True, compact=True, with_types=False):
        a_str = f'Function[{self.f.__name__}]'
        if not compact:
            a_str += '(' + ','.join(f'{k}={v}' for k, v in self.args.items()) + ')'
        if with_type:
            a_str += f'          {self.type}'
        return a_str

    @property
    def __name__(self):
        return self.f.__name__


class Class(Transform):
    """ Wraps a Transform-subclass defined inline such that it can be saved.

    :param c: The class to wrap (must inherit from Transform).
    :param imports: Collection of imports needed.
    :param source: Source of the class (leave empty).
    """

    def __init__(self, c, imports=(), source='', in_type=Any(), out_type=Any()):
        """
        :param c:
        :param imports:
        :param source:
        """

        assert isinstance(c, Transform)

        if not source:
            source = inspect.getsource(c.__class__)
            source = strip_decorator(source)
            if source[0][0] != ' ':
                padding = 0
            else:
                padding = [i for i, x in enumerate(source[0]) if x != ' '][0]

            source = '\n'.join([x[padding:] for x in source])

        super().__init__(c=c,
                         imports=imports,
                         source=source,
                         in_type=c.type.x,
                         out_type=c.type.y)

    def __len__(self):
        with self.set_stage(self.stage):
            return len(self.c)

    def repr(self):
        return 'lf.transforms.Class:\n' + self.source

    def _clone(self):
        return type(self)(c=self.c, imports=self.imports, source=self.source)

    @staticmethod
    def _source_with_imports(source, imports):
        defn = 'from lf.transforms import Transform\nfrom lf import transforms as lf\n'
        if not imports:
            return defn + source

        return defn + '\n'.join(['import ' + x for x in imports]) + '\n' + source

    def to_dict(self):
        my_d, v = self.c.to_dict()

        res = {
            'cls': 'Class',
            'kwargs': {
                'defn': self.source,
                'imports': self.imports,
                'c': {
                    'cls': '_temp.' + self.c.__class__.__name__,
                    'kwargs': my_d['kwargs'],
                }
            },
        }
        if self.name is not None:
            res['_name'] = self.name
        return res, v

    def _versions(self):
        versions = {}
        for k in self.imports:
            package = k.split('.')[0]
            try:
                versions[package] = importlib.import_module(package).__version__
            except AttributeError as err:
                if '__version__' in str(err):
                    warn(str(err))
                else:
                    raise err
        return versions

    @classmethod
    def from_dict(cls, d, vars=None, handles=None):
        if handles is None:
            handles = {}
        a_hash = ''.join([str(choice(range(10))) for _ in range(8)])
        temp_module = ModuleType(f'_temp_{a_hash}')
        exec(cls._source_with_imports(d['defn'], d['imports']), temp_module.__dict__)
        c = getattr(temp_module, d['c']['cls'].split('.')[-1])
        if '_type' in d['c']['kwargs']:
            _type = Ftype.from_dict(d['c']['kwargs'].pop('_type'))
            in_type = _type.x
            out_type = _type.y
        else:
            in_type = Any()
            out_type = Any()
        instance = c(**d['c']['kwargs'])
        t = cls(instance,
                imports=d['imports'],
                source='\n'.join(d['defn'].split('\n')),
                in_type=in_type,
                out_type=out_type)
        handles = cls._parse_handles(handles)
        for k in handles:
            t[k] = handles[k]
        return t

    from_dict.__doc__ = Transform.from_dict.__doc__

    def do(self, *args):
        return self.c.do(*args)


def wrap_class(imports, in_type=Any(), out_type=Any()):
    """Decorator for wrapping a class such that it becomes a "Class"-Transform. """

    imports_ = []
    for x in imports:
        if isinstance(x, str):
            imports_.append(x)
        else:
            imports_.append(x.__name__)

    def decorator(cls):
        """
        :param cls: class
        :return:
        """

        def wrapper(*args, **kwargs):
            """
            :param args:
            :param kwargs:
            :return:
            """
            c = cls(*args, **kwargs)
            cls_ = Class(c, imports=imports_)
            cls_.set_type(in_type, out_type)
            return cls_

        return wrapper

    return decorator


def wrap_layer(layer_class: torch.nn.Module, in_type=Any(), out_type=Any()):
    """ Wrap a torch Module.

    :param layer_class: Module / layer class.
    :param in_type: Input type.
    :param out_type: Output type.
    """

    @functools.wraps(layer_class)
    def creator(*args, script='forward', **kwargs):
        """
        :param args:
        :param script:
        :param kwargs:
        :return:
        """
        layer = layer_class(*args, **kwargs)
        return Layer(
            layer,
            layer_name=f'{layer}',
            in_type=in_type,
            out_type=out_type,
            script=script,
        )

    return creator


def trans(in_type=Any(), out_type=Any(), imports=()):
    """
    Decorator that wraps functions and Transform instances in Function.

    :param in_type: Input type
    :param out_type: Output type
    :param imports: Collection of imports needed.
    """
    import_strs = []
    for imp in imports:
        if not isinstance(imp, str):
            import_strs.append(imp.__name__)
        else:
            import_strs.append(imp)

    def decorator(f):
        """
        :param f:
        :return:
        """
        if inspect.isclass(f):
            if issubclass(f, torch.nn.Module):
                return wrap_layer(f, in_type, out_type)
            if issubclass(f, Transform):
                return wrap_class(imports, in_type, out_type)(f)
        return Function(f, in_type=in_type, out_type=out_type, imports=import_strs)

    return decorator


class Debug(Transform):
    """ Add a breakpoint to a transform. """

    def __init__(self):
        super().__init__(in_type=Any('?a'), out_type=Any('?a'))

    def to_dict(self):
        raise ValueError('Cant convert transform with breakpoint...')

    def _clone(self):
        return type(self)()

    def do(self, *args):
        pdb.set_trace()
        if len(args) == 1:
            return args[0]
        return args


class ForceStage(Transform): # do we need this?
    """Always execute a transform with a given stage."""

    def __init__(self, t, stage2set='train'):
        """
        :param t: transform
        :param stage2set: stage into which transform should be locked
        """
        _type = t.type.copy()
        if isinstance(_type, dict):
            super().__init__(t=t,
                             stage2set=stage2set,
                             in_type=_type[stage2set].x,
                             out_type=_type[stage2set].y)
        else:
            super().__init__(t=t, stage2set=stage2set,
                             in_type=_type.x, out_type=_type.y)

    def _clone(self):
        return ForceStage(self.t, self.stage2set)

    def do(self, *args):
        with self.set_stage(self.stage2set):
            out = self.t._do(*args)
        return out

    def repr(self):
        start = 'lf.transforms.ForceStage:{}(\n'.format(self.stage2set)
        lines = ['    ' + y for y in self.t.repr().split('\n')]
        body = '\n'.join(lines)
        end = '\n)'
        return start + body + end


class Cast(Transform):
    """Change the output type to *type_*,
    (this does nothing, apart from changing the type signature of the
     transform.)
    """

    def __init__(self, out_type=Any(), in_type=Any(), _type=None):
        """
        :param out_type: out-type
        :param in_type: in-type
        :param _type: `(out_type, in_type)`
        """
        super().__init__(in_type=in_type, out_type=out_type, _type=_type)

    def do(self, x):
        return x


def _format_slice(s):
    """
    :param s:
    :return:
    """
    out = []
    has_end = s.step is not None
    if has_end:
        out = [str(s.start) if s.start else '',
               str(s.stop) if s.stop else '',
               str(s.step) if s.step else '']
        return ':'.join(out)
    out = [str(s.start) if s.start else '',
           str(s.stop) if s.stop else '']
    return ':'.join(out)


class _X:
    """
    Transform factory for capturing attributes/ get-items
    """

    @staticmethod
    def check_args(d):
        """Modify args"""
        for i, x in enumerate(d['args']):
            if isinstance(x, dict) and 'LF:slice' in x:
                if isinstance(x['LF:slice'], str):
                    d['args'][i] = eval(x['LF:slice'])
                else:
                    d['args'][i] = tuple([eval(y) for y in x['LF:slice']])
        return d

    @staticmethod
    def check_kwargs(d):
        """Modify kwargs"""
        for k in d['kwargs']:
            if isinstance(d['kwargs'][k], dict) and 'LF:slice' in d['kwargs'][k]:
                if isinstance(d['kwargs'][k]['LF:slice'], str):
                    d['kwargs'][k] = eval(d['kwargs'][k]['LF:slice'])
                else:
                    d['kwargs'][k] = tuple([eval([x for x in d['kwargs'][k]])])
        return d

    def from_dict(self, d, vars=None, handles=None):
        """
        :param d: dict
        :param vars: var dict
        :param handles:
        :return:
        """
        self.check_args(d)
        self.check_kwargs(d)

        if 'attr' in d:
            return getattr(self, d['attr'])(*d['args'], **d['kwargs'])
        return self(d['fname'])

    def __getitem__(self, item):
        return self.__getattr__('__getitem__')(item)

    def __getattr__(self, attr):
        class T(Transform):
            """
            :param args:
            :param kwargs:
            """

            def __init__(self, *args, **kwargs):
                self.__args = args
                self.__kwargs = kwargs
                super().__init__()

            def do(self, input_):
                return getattr(input_, attr)(*self.__args, **self.__kwargs)

            def astr(self, with_type=True, compact=True, with_types=False):
                if attr == '__getitem__':
                    arg = self.__args[0]
                    if isinstance(arg, slice):
                        return f'x[{_format_slice(arg)}]'
                    if isinstance(arg, tuple):
                        to_format = []
                        for part in arg:
                            if isinstance(part, int):
                                to_format.append(str(part))
                            else:
                                to_format.append(_format_slice(part))
                        return f'x[{", ".join(to_format)}]'
                    return f'x[{arg}]'
                return f'x.{attr}({", ".join([str(x) for x in self.__args])})'

            def to_dict(self):
                args = list(self.__args[:])
                kwargs = self.__kwargs.copy()
                for i, x in enumerate(args):
                    if isinstance(x, slice):
                        args[i] = {'LF:slice': str(x)}
                    if isinstance(x, tuple) and \
                            sum([isinstance(y, slice) for y in x]):
                        args[i] = {'LF:slice': [str(y) for y in x]}

                for k in kwargs:
                    if isinstance(kwargs[k], slice):
                        kwargs[k] = {'LF:slice': str(kwargs[k])}
                    if isinstance(kwargs[k], tuple) and \
                            isinstance(kwargs[k][0], slice):
                        kwargs[k] = {'LF:slice': [str(x) for x in kwargs[k]]}

                return {
                           'cls': 'x',
                           'kwargs': {'kwargs': kwargs, 'args': args, 'attr': attr},
                       }, {}

        return T

    def __call__(self, fname):
        class T(Transform):
            """
            """

            def __init__(self):
                super().__init__()

            def do(self, input_):
                return __builtins__[fname](input_)

            def astr(self, with_type=True, compact=True, with_types=False):
                return f'{fname}'

            def to_dict(self):
                return {
                           'cls': 'x',
                           'kwargs': {'fname': fname},
                       }, {}

        return T()


x = _X()


class Value(Transform):
    """
    :param value: constant value to return
    """
    requires_args = False

    def __init__(self, value: tp.Any):
        super().__init__(value=value)

    def do(self, *args):
        return self.value


@trans(imports=('torch',))
def to_tensor(x):
    """
    :param x: item to convert
    """
    return torch.tensor(x)


def use_teleport(name: str):
    """
    :param name: name of teleport
    """
    return TeleportWriter(name, alias=name), TeleportReader(name, alias=name)


def I(*args, **kwargs): # consider renaming multiple of these...
    """
    Get Identity
    :param args:
    :param kwargs:
    :return:
    """
    return Identity()(*args, **kwargs)
