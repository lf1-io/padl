{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may also need to install torchvision and matplotlib\n",
    "# !pip install matplotlib\n",
    "# !pip install torchvision==0.11.0\n",
    "\n",
    "# These might be useful if there are errors regarding ipywidgets while downloading torchvision.datasets\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fcbb10",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing `padl` and most importantly `transform` decorator used to change any `callable` to `padl.Transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ebda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd560eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import padl\n",
    "from padl import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e86d14",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "MNIST dataset available through torchvision is used in this notebook. The dataset can be separately downloaded from MNIST website or can be loaded as given below. \n",
    "\n",
    "More details on torchvision's MNIST dataset can be found here: https://pytorch.org/vision/stable/datasets.html#mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b627c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_dataset = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "mnist_test_dataset = torchvision.datasets.MNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5e057",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 1. Plot few images to check the data\n",
    "\n",
    "`plot_image` is a normal standard function that takes in an image tensor and uses `matplotlib.pyplot` to plot the image. With `@transform` decorator, we can easily convert it to `padl.transform`. This allows us to use `padl` functional api and build data pipeline easily and quickly. \n",
    "\n",
    "Quick recap to `padl` operators:\n",
    "- `>>`: Compose operator: $(f_1 >> f_2)(x) \\rightarrow f_2(f_1(x))$\n",
    "- `+`: Rollout operator: $(f_1 + f_2) (x) \\rightarrow (f_1(x), f_2(x))$\n",
    "- `/`: Parallel operator: $(f_1 / f_2)((x_1,x_2)) \\rightarrow (f_1(x_1), f_2(x_2))$\n",
    "- `-`: Name operator: Names a transform so that its output can be accesed by given name or the transform itself can be accessed by its name from the pipeline:  \n",
    "    - $((f_1 - \\text{'zulu'})+f_2)(x) \\rightarrow \\text{Namedtuple}(\\text{'zulu'}:f_1(x), \\text{'out_1'}:f_2(x))$\n",
    "    - $((f_1 - \\text{'zulu'})+f_2)[\\text{'zulu'}] = f_1$\n",
    "    \n",
    "Note that outputs of `Rollout` and `Parallel` are tuples.\n",
    "\n",
    "For more details on operators and building compound transforms, refer to the documentation: https://lf1-io.github.io/padl/gettingstarted.html#defining-compound-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def plot_image(img_tensor):\n",
    "    fig= plt.figure(figsize=(2,2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img_tensor, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75656124",
   "metadata": {},
   "source": [
    "### 1.1 Building a simple ploting pipeline using `padl` operators\n",
    "\n",
    "Description of inbuilt transforms used.\n",
    "\n",
    "- `padl.this` is a self reflexive trasform that allows for a quick mutation of input. \n",
    "\n",
    "        Example: padl.this[0]([1,2,3]) = 1\n",
    "\n",
    "- `padl.Identity()` is a simple transform that does exactly as it sounds, passes the input on as it is. \n",
    "\n",
    "        Example: padl.Identity()([1,2,3]) = [1,2,3]\n",
    "\n",
    "\n",
    "\n",
    "Description of `transform` pipeline defined below.\n",
    "- `convert_plot`: Takes in a `PIL.Image` that is converted to numpy array, and then is plotted using `plot_image` transform defined above. \n",
    "- `plot_datapoint`: \n",
    "    - It is a `parrallel` that passes first part of input to convert_plot and passes second part of input as it is with `padl.Identity`\n",
    "    - transforms are also named by `-`, so the output is a named tuple, with elements named as the transform name.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def img_to_array(img):\n",
    "    return np.asarray(img)\n",
    "\n",
    "convert_plot = (\n",
    "    img_to_array\n",
    "    >> plot_image\n",
    ")\n",
    "\n",
    "plot_datapoint = (convert_plot - 'image')/ (padl.Identity() - 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594cc416",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = plot_datapoint(mnist_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ac086",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting some of the train datapoints\n",
    "\n",
    "for _ in range(5):\n",
    "    output = plot_datapoint(mnist_train_dataset[np.random.randint(len(mnist_train_dataset))])\n",
    "    print(f'Label : {output.label}')\n",
    "    # showing image\n",
    "    plt.show()\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06535ca",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 2. Model\n",
    "We will build a simple `Unet` to classify `MNIST` handwritings. In the cell below, a simple pytorch net is defined with just one added decorator `@transform`. This is enough to wrap the pytorch model into `padl.Transform` and use it with other transform to build a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6434c1",
   "metadata": {},
   "source": [
    "### 2.1 Simple Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "        \n",
    "@transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv 1\n",
    "        # size : input: 28x28x1 -> output : 26 x 26 x 32\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 2\n",
    "        # size : input: 26x26x32 -> output : 24 x 24 x 32\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 3\n",
    "        # size : input: 24x24x32 -> output : 12 x 12 x 32\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 4\n",
    "        # size : input : 12 x 12 x 32 -> output : 8 x 8 x 64\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Conv 5\n",
    "        # size : input: 8x8x64 -> output : 4 x 4 x 64 -> Linearize = 1024\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "        \n",
    "        # FC 1 \n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        \n",
    "        # FC 2\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2265f",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing\n",
    "\n",
    "The `preprocess` pipeline below, again splits the datapoint to two different pipelines for `PIL.Image` and `int` label. First part of parralel pipeline, converts the image into torch tensor of type float, and second part passes the label as it is. The label later will be used for the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5170e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def convert_to_tensor(img):\n",
    "    arr = np.asarray(img)\n",
    "    return torch.tensor(arr).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "preprocess = (\n",
    "    convert_to_tensor / convert_to_tensor\n",
    "    >> padl.this.reshape(-1, 28, 28) / padl.Identity()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec776202",
   "metadata": {},
   "source": [
    "### 2.3 Instantiating the network and loss function\n",
    "\n",
    "Initialising instances of `SimpleNet` and `loss` function. Loss function here is a wrapped `torch` negative log likelihood loss which is again wrapped easily with same `transform` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a699bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()\n",
    "loss_func = transform(F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616132d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25508e",
   "metadata": {},
   "source": [
    "### 2.4  Building the training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691e6e7",
   "metadata": {},
   "source": [
    "`train_model` is now composed (`>>`) with the transforms already defined.\n",
    "- preprocess: preprocessing transform defined above\n",
    "- Batchify: Batchify is a inbuilt `transform` that marks end of preprocess (dataloading) and that adds batch dimension to the inputs. Batchify also moves the input tensors to device specified for the model\n",
    "- simplenet: Instance of SimpleNet\n",
    "- padl.this: A self reflexive trasform that allows for a quick mutation of input.\n",
    "\n",
    "`train_model` is then sent to the intended device. It is by default in `cpu`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device to be used: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = (\n",
    "    preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.this.type(torch.long)\n",
    ")\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d982878",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 3. Training and validating the `train_model`\n",
    "\n",
    "Training is not much different than the normal torch training steps, except dataloading and training is made even simplier by `train_apply`. It is one of the three inbuilt methods along with `infer_apply` and `eval_apply` that handles the stage context of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95e62b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 2\n",
    "num_workers = 0\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(mnist_train_dataset, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b68ed",
   "metadata": {},
   "source": [
    "### 3.1 Accuracy of the model\n",
    "\n",
    "We can quickly build a `validation_model` by adding a further step to `train_model` to get the number associated with the maximum confidence predicted by the model. \n",
    "\n",
    "\n",
    "Note: As we don't have a separate validation dataset with labels, we will have to use the same train data to `validate` the model in this example.\n",
    "\n",
    "\n",
    "First, lets look at the format of the prediction by `infer_apply`ing one of datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b39dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.infer_apply(mnist_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8105d54",
   "metadata": {},
   "source": [
    "`train_model` predicts a tensor of confidence associated for the 10 numbers, and the index associated with the maximum of these confidence is the prediction by the model. Thus, we can add another transform to the same `train_model` to get that index associated with maximum of the confidence. \n",
    "\n",
    "Note that the new `validation_model` is a new instance of Transform but it contains same objects as `train_model` with added two new `transform`s. All the transform objects that are already in `train_model` is in `device` as assigned above, but the main `validation_model` object itself will have by default `cpu` device assigned. Thus, to move it to (or assign it with) correct device, we have to again call `pd_to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_model = (\n",
    "    train_model\n",
    "    >> padl.transform(lambda x: x.max(1).indices) / padl.Identity()\n",
    ")\n",
    "\n",
    "# We need to send the validation_model to device again\n",
    "validation_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27736457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy over the test dataset\n",
    "\n",
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(mnist_test_dataset, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ len(mnist_test_dataset)\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f570",
   "metadata": {},
   "source": [
    "Not a bad accuracy of `~0.95` for a quick train model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fe164",
   "metadata": {},
   "source": [
    "### 3.2 Infer few images from test data\n",
    "\n",
    "Although we do not have labels for images in test data, we can still infer and verify the predictions ourselves. For that, we can again use model object `simplenet` that we have trained by using `train_model` and now stack it with other `transform`s to build an infer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_preprocess =(\n",
    "    padl.this[0]\n",
    "    >> convert_to_tensor\n",
    ")\n",
    "infer_model = (\n",
    "    infer_preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> padl.this.unsqueeze(1) \n",
    "    >> simplenet\n",
    "    >> padl.transform(lambda x: x.max(1).indices)\n",
    ")\n",
    "\n",
    "infer_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7d6ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    data_point = mnist_test_dataset[np.random.randint(len(mnist_test_dataset))]\n",
    "    output = plot_datapoint(data_point)\n",
    "    plt.show()\n",
    "    print(f'Prediction: {infer_model.infer_apply(data_point).item()}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab222191",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 4. Using further image augmentation on training\n",
    "\n",
    "We can easily use some of the `torchvision.transforms` for image augmentation and add it to our preproccessing of image to help with training. Lets add a couple of augmentations to our training: `GaussianBlur` and `RandomRotation`\n",
    "We need to wrap the call to these `torchvision.transforms` with our `padl.transform` before instantiating them, and that is all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ba923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_blur = transform(T.GaussianBlur)(kernel_size=(3,3), sigma=0.1)\n",
    "\n",
    "rotate_img = transform(T.RandomRotation)(degrees=(-15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a0a2f",
   "metadata": {},
   "source": [
    "Now we can again use `padl`'s functional api to build an `image_augmentation` pipeline.\n",
    "\n",
    "Note: `torchvision.transforms` expect images with channels but our images are just in grayscale. So, we need to unsqueeze our image tensor here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_augmentation = (\n",
    "    padl.this.unsqueeze(0)\n",
    "    >> rotate_img\n",
    "    >> gaussian_blur\n",
    "    >> padl.this[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0dc1b",
   "metadata": {},
   "source": [
    "####  Sample of image augmentation\n",
    "\n",
    "Lets try the augmentation on one of image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209547d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = mnist_train_dataset[2]\n",
    "\n",
    "# plot datapoint \n",
    "plot_datapoint(datapoint)\n",
    "\n",
    "# convert datapoint's image to tensor\n",
    "img_tensor = convert_to_tensor(datapoint[0])\n",
    "\n",
    "# tensor shape\n",
    "print('tensor shape:', img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4d5f5",
   "metadata": {},
   "source": [
    "Augmented images: Running image augmentation on same image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    out_aug = image_augmentation(img_tensor)\n",
    "    plot_image(out_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6005c",
   "metadata": {},
   "source": [
    "### 4.2 We can add the `image_augmentation` pipeline easily to the `preprocess` and rebuild `train_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_with_augmentation = (\n",
    "    convert_to_tensor / convert_to_tensor\n",
    "    >> padl.this.reshape(-1, 28, 28) / padl.Identity()\n",
    "    >> image_augmentation / padl.Identity()\n",
    ")\n",
    "\n",
    "train_model = (\n",
    "    preprocess_with_augmentation\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.this.type(torch.long)\n",
    ")\n",
    "\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd544eca",
   "metadata": {},
   "source": [
    "### 4.3 Retraining the model with `image_augmentation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffaae3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 3\n",
    "num_workers = 0\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(mnist_train_dataset, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3aeb9",
   "metadata": {},
   "source": [
    "### 4.4 Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(mnist_test_dataset, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ len(mnist_test_dataset)\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenen_env",
   "language": "python",
   "name": "tenen_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
