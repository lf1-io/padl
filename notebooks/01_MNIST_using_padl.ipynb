{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b4084e",
   "metadata": {},
   "source": [
    "# Install `PADL`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f0a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/lf1-io/padl.git\n",
      "  Cloning https://github.com/lf1-io/padl.git to /private/var/folders/_j/9htwwr6x7hsdj_mb_1ws54cc0000gn/T/pip-req-build-3r23ewf0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lf1-io/padl.git /private/var/folders/_j/9htwwr6x7hsdj_mb_1ws54cc0000gn/T/pip-req-build-3r23ewf0\n",
      "  Resolved https://github.com/lf1-io/padl.git to commit ef3f3a5d1c28ea1956528c26d3dbeb18d9149bf2\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/jasonkhadka/Aleph-One-GmbH/padl/.venv/lib/python3.8/site-packages (from padl==0.2.1) (1.10.2)\n",
      "Requirement already satisfied: numpy in /Users/jasonkhadka/Aleph-One-GmbH/padl/.venv/lib/python3.8/site-packages (from padl==0.2.1) (1.22.2)\n",
      "Requirement already satisfied: astunparse in /Users/jasonkhadka/Aleph-One-GmbH/padl/.venv/lib/python3.8/site-packages (from padl==0.2.1) (1.6.3)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /Users/jasonkhadka/Aleph-One-GmbH/padl/.venv/lib/python3.8/site-packages (from astunparse->padl==0.2.1) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jasonkhadka/Aleph-One-GmbH/padl/.venv/lib/python3.8/site-packages (from astunparse->padl==0.2.1) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/jasonkhadka/Aleph-One-GmbH/padl/.venv/lib/python3.8/site-packages (from torch->padl==0.2.1) (4.0.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install git+https://github.com/lf1-io/padl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b96b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These might be useful if there are errors regarding ipywidgets while downloading torchvision.datasets\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d843846",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fcbb10",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing `padl` and most importantly `transform` decorator used to change any `callable` to `padl.Transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd560eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import padl\n",
    "from padl import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e86d14",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "MNIST dataset available through torchvision is used in this notebook. The dataset can be separately downloaded from MNIST website or can be loaded as given below. \n",
    "\n",
    "More details on torchvision's MNIST dataset can be found here: https://pytorch.org/vision/stable/datasets.html#mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be9bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "96.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_train_dataset = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "mnist_test_dataset = torchvision.datasets.MNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5e057",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 1. Plot few images to check the data\n",
    "\n",
    "`plot_image` is a normal standard function that takes in an image tensor and uses `matplotlib.pyplot` to plot the image. With `@transform` decorator, we can easily convert it to `padl.transform`. This allows us to use `padl` functional api and build data pipeline easily and quickly. \n",
    "\n",
    "Quick recap to `padl` operators:\n",
    "- `>>`: Compose operator: $(f_1 >> f_2)(x) \\rightarrow f_2(f_1(x))$\n",
    "- `+`: Rollout operator: $(f_1 + f_2) (x) \\rightarrow (f_1(x), f_2(x))$\n",
    "- `/`: Parallel operator: $(f_1 / f_2)((x_1,x_2)) \\rightarrow (f_1(x_1), f_2(x_2))$\n",
    "- `-`: Name operator: Names a transform so that its output can be accesed by given name or the transform itself can be accessed by its name from the pipeline:  \n",
    "    - $((f_1 - \\text{'zulu'})+f_2)(x) \\rightarrow \\text{Namedtuple}(\\text{'zulu'}:f_1(x), \\text{'out_1'}:f_2(x))$\n",
    "    - $((f_1 - \\text{'zulu'})+f_2)[\\text{'zulu'}] = f_1$\n",
    "    \n",
    "Note that outputs of `Rollout` and `Parallel` are tuples.\n",
    "\n",
    "For more details on operators and building Pipelines, refer to the documentation: https://lf1-io.github.io/padl/usage/transform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a60c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def plot_image(img_tensor):\n",
    "    fig= plt.figure(figsize=(2,2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img_tensor, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75656124",
   "metadata": {},
   "source": [
    "### 1.1 Building a simple plotting pipeline using `padl` operators\n",
    "\n",
    "Description of inbuilt transforms used.\n",
    "\n",
    "- `padl.same` is a self reflexive transform that allows for a quick mutation of input. \n",
    "\n",
    "        Example: padl.same[0]([1,2,3]) = 1\n",
    "\n",
    "- `padl.Identity()` is a simple transform that does exactly as it sounds, passes the input on as it is. \n",
    "\n",
    "        Example: padl.Identity()([1,2,3]) = [1,2,3]\n",
    "\n",
    "\n",
    "\n",
    "Description of `transform` pipeline defined below.\n",
    "- `convert_plot`: Takes in a `PIL.Image` that is converted to numpy array, and then is plotted using `plot_image` transform defined above. \n",
    "- `plot_datapoint`: \n",
    "    - It is a `parrallel` that passes first part of input to convert_plot and passes second part of input as it is with `padl.Identity`\n",
    "    - transforms are also named by `-`, so the output is a named tuple, with elements named as the transform name.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012b6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def img_to_array(img):\n",
    "    return np.asarray(img)\n",
    "\n",
    "convert_plot = (\n",
    "    img_to_array\n",
    "    >> plot_image\n",
    ")\n",
    "\n",
    "plot_datapoint = (convert_plot - 'image')/ (padl.Identity() - 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594cc416",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEdElEQVR4nO2dSyhtURzG17lkQB6ZUEpiQEgmKClJkmLgMVFGZEQZmZgZkPIYiIGRMpGhx4SB10ApeUyUOZl5v8O5s935Vvfug/Owz/m+32h9rfbZq36t87eOvfby+f1+Izj489sDENFDsomQbCIkmwjJJkKyiUh06/T5fFqXxRh+v9/3vz7NbCIkmwjJJkKyiZBsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjJJkKyiZBsIiSbCNfHkmKZhIQEyOnp6V++tr+/H3JycjLkwsJCyH19fZAnJiYgd3Z2Qn55eYE8NjbmtIeHh788zu+imU2EZBMh2UR4tmbn5uZCTkpKglxdXQ25pqYGckZGBuT29vawje38/Bzy9PQ05NbWVsj39/eQT05OIO/s7IRtbG5oZhMh2URINhE+tzcvRHP7T3l5OeTNzU3I31knh5vPz0/I3d3dkB8eHlyvv7y8hHx9fQ357OwshNEh2v4jjDGSTYVkE+GZmp2ZmQl5f38fcn5+ftjuZX/2zc0N5Lq6Oshvb2+Qf/Pvh2CoZgtjjGRTIdlEeOa38aurK8iDg4OQm5ubIR8dHUG2f5+2OT4+dtoNDQ3Q9/j4CLmkpATywMCA62fHCprZREg2EZJNhGfW2cFIS0uDbP+PeG5uDnJPTw/krq4up724uBjm0XkHrbOFMUayqZBsIjyzzg7G3d2da//t7a1rf29vr9NeWlqCPvv/1fGKZjYRkk2EZBMRM+vsYKSkpEBeXV2FXFtb67Sbmpqgb2NjI3IDizJaZwtjjGRTIdlExE3NtikoKIB8eHjotO1nzra2tiAfHBxAnp2dhezlUw5Vs4UxRrKpiNuvcZvAbbTz8/PQl5qa6nrt0NAQ5IWFBcj29p7fRF/jwhgj2VRINhE0NTuQ0tJSyFNTU5Dr6+tdr7cfgRoZGYF8cXERwuhCQzVbGGMkmwrJJoKyZtvYr9FqaWmBbK/LfT4si/YrQeztRdFENVsYYySbCskmQjX7C7y+vkJOTMQnsN/f3yE3NjZC3t7ejsi4/oVqtjDGSDYVkk1EzGz/CSdlZWWQOzo6IFdUVEC2a7TN6ekp5N3d3RBGFzk0s4mQbCIkm4i4rdn2cUyBxze1tbVBX3Z29rc+++PjA7L9DJpXtwBrZhMh2URINhExW7PtOmsfgWgfsZiXl/fje9nbgexnzlZWVn782dFEM5sIySZCsonwbM3OysqCXFxcDHlmZgZyUVHRj+9lHyMxPj4OeXl5GbJX19HB0MwmQrKJkGwifq1m20c72fun7OMaQz3qaW9vz2lPTk5C3/r6OuTn5+eQ7uVVNLOJkGwiJJuIiNbsqqoqp20f3VRZWQk5JycnpHs9PT1Bto9+Gh0dddr20U4saGYTIdlERPRrPPB1VIHtr2A/nru2tgbZ3nJjL6fstxgKzWwqJJsIySZCW3bjDG3ZFcYYyaZCsomQbCIkmwjJJkKyiZBsIiSbCMkmQrKJcP1tXMQXmtlESDYRkk2EZBMh2URINhF/AVStNDfc/xylAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = plot_datapoint(mnist_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9488170b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76f0ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEpklEQVR4nO2dzSttXxjH17koL50MvIYJJeWMEJlQytSEMjAgQykhBiaUl5CZRIkRE8WIQgoZSDLwF4hk4H1E5OX8ZifPk7v2Pb+7zz7b/X4/o/Ntr8Nz+7Q8Z627zt6BcDhsCAa/4l0A8Q7KBoKygaBsICgbCMoGItF2MRAIcF32wwiHw4HfXePMBoKygaBsICgbCMoGgrKBoGwgKBsIygaCsoGgbCAoGwjKBoKygaBsICgbCMoGgrKBsB5LiifBYFDk9fV1kdva2kS+uLiIeU3/l6ysLJF3d3dFDoVCkdcjIyPi2tDQkGt1cGYDQdlAUDYQvu3ZqampItfU1IhcWVkpsp96dm5ursgbGxsil5aWiry6uhp5PTk5GbO6OLOBoGwgKBsI3/ZsJ+bn50XWa9eHhwcvyxHU1dWJXFZWZh0/NzcXef38/ByTmozhzIaCsoGgbCB+bM9OT08XOTExfv+UpKQkkbu6uqzjT05ORD44OHC9pu/gzAaCsoGgbCB827P1/rKfqaioELmqqso6Xu8RfHx8uF7Td3BmA0HZQFA2EL7t2c3NzdbrOzs7Ij8+PsayHCtOtb68vIisz9N5BWc2EJQNBGUD4due7bTXvbW1JfLb21ssyxEEAvKOkQUFBdbxR0dHIt/c3Lhe05/AmQ0EZQNB2UD4pmfX1taK3NPTYx1/fX0t8vT0tMgpKSm/fe/p6anIKysrIt/d3Vl/d3Z2tshNTU3W8YeHh9brXsGZDQRlA0HZQMStZ+vvLI+NjYmckJBgff/y8rJrtRQXF4vc3d1tHd/Y2Gi9fnV1JbL+TKApKiqKvD47O7OO/Rs4s4GgbCACtgevxvLpP0tLSyK3tLS4+vPv7+9F/rr8aWhoENf0saDLy0uRdcvo6OgQOSMjQ+SnpyeRx8fHRdbLwq+16W3gaOHTf4gxhrKhoGwgPOvZuq8dHx+LXFhYaH3/6+uryPv7+yLrW0rZtijb29tFHh4eFjk/P99ai/4vTqcHzuvrCwsLIo+OjkZe688L0cKeTYwxlA0FZQPhWc+empoSube31zpeHzPq7OwUeXFx0Z3CjDFpaWkit7a2iqxvV6XHa/RXcre3t0UeHByMtsQ/hj2bGGMoGwrKBsKznv35+Smy09p0b29P5Pr6erdKcUSvs/VR4Ly8PJFvb29Frq6uFvn8/Ny94hxgzybGGMqGgrKB8M1R4tnZWZGd1uGxpLy8XGTdozV9fX0ie9mjo4EzGwjKBoKygfCsZzudMdvc3BTZy6/gJicni9zf3x/V+9/f390sJ2ZwZgNB2UBQNhBxOzfuJwYGBkT+eibsO/RtuEpKSkTWZ9a9hHvjxBhD2VBQNhC+2RuPJzk5OVGN1+fO49mjo4EzGwjKBoKygYBcZ2dmZoqsz7vpRx5rgsGgyLF8pGK0cJ1NjDGUDQVlAwG5zl5bWxM5FAqJrD/HzMzMiKy/K/5T4MwGgrKBgPwz7nTnf32ri4mJCZG9esKe23BmA0HZQFA2EJDbpf8y3C4lxhjKhoKygaBsICgbCMoGgrKBoGwgKBsIygaCsoGw7o2TfwvObCAoGwjKBoKygaBsICgbiP8AGSozZYZqKJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEMElEQVR4nO2dPSh9YRzHnyPcMCCyMphkwWaRlJdSimyyKCl5WZSJKCksNiuLgSTF4qVsBoPJIAOTJBtG97/dfH9xjuuel3v/3+9nOt/Oce5zffz87rnnuc/10um0ExwUJT0AER+STYRkEyHZREg2EZJNRLHfTs/zdF1WYKTTae+nfapsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjJJkKyiZBsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjfqcRJUlZWBrmrqwvywsIC5La2Nsj7+/uQZ2dnM9tPT08hjPDvpFIpyDMzM5nt7e1t2Pf8/Bza46qyiZBsIiSbiLzp2Z2dnZCXlpYgt7e3Q/Y8/JSLXUGip6cHckVFRa5DDI3V1VXI09PTmW07bvtaJRdU2URINhGSTURiPbu5uRny2toa5JaWFt+fPzw8hLy1tQX59fUV8v39fZYjDA/7emRkZATy+fl5ZntycjKycaiyiZBsIiSbiMR69tzcHOSgHj0/Pw95Y2Mj9DGFRU1NDeS9vT3IVVVVkC8uLjLbUb62UGUTIdlESDYRsfXsuro6yIODg77H397eQrbX0fnM1NQUZNujHx8fIe/s7EQ9JOecKpsKySZCsomIrWcvLi5CLi8vh3x3dwfZ3sd9e3uLZmARYJ/r5+cn5JubG8hxzYlTZRMh2URINhGx9eyJiQnIds7Yw8MD5JeXl6iHFBp9fX2QbY/++PiAvL6+HvmYvkOVTYRkEyHZROTNvPFCxvZsy+npKeSrq6soh/MjqmwiJJsI/Rv/A/39/ZDHx8d9jz84OIhyOL9GlU2EZBMh2UTE1rOvr68h22UxOjo6IJ+dnUEeGBiA/P7+HuLosqO6uhpycTH+Gu3bo3aKVVKosomQbCIkmwjP3mqEnSF+y65d5uLo6AiyXUajtLTU93xFRfh3am8r2qWx7PTdrwQt2dHQ0AB5aGgoq7Ftbm5C/rpMV9joW3aFc06yqZBsImLr2UGMjY1BtstNNDU1QS4pKYHs9zyCCOrZ2WKnBtspWcfHxzmd3w/1bOGck2wqJJuIvOnZQdj3zu11eG1tLeTR0dFfn9u+l20/HryysgK5tbXV93yNjY2Q7TTpKFHPFs45yaZCsokomDlol5eXWR2/u7sb2mPbqcJBPTuflrv+iiqbCMkmQrKJKJjr7CTp7u6GfHJy4nt8ZWUl5Djny+k6WzjnJJsKySaiYK6z8wl7/9uS5Jx2P1TZREg2EZJNhHr2H8h1jlpSqLKJkGwiJJsI9ewIGB4ehmy/6ikpVNlESDYR+jceAfX19UkP4VtU2URINhGSTYSmJf2CVCoF2S4J0tvbC3l5eRmypiWJ2JFsIiSbCPXs/wz1bOGck2wqJJsIySZCsomQbCIkmwjf62zxf6HKJkKyiZBsIiSbCMkmQrKJ+AeamvyMH4+RmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADAElEQVR4nO3dv0ojURSA8dx10cpCLAQ7QQQFQQRb0cLaxmcQLHwIqzyIlfgIIv5pVHwFxVIQtBAVG5ntgifg6LCTmXi/71flkAQvfBwvJrCbiqLoiOFP2wdQc4wNYmwQY4MYG8TYIH/Lnkwp+XfZL1MURfrqOTcbxNggxgYxNoixQYwNYmwQY4MYG8TYIMYGMTaIsUGMDWJskNLvsykWFhbCfHZ2Fub7+/swr6+vh/nx8XEwB6uZmw1ibBB/jXc6nZ2dnTBPTk6Wzt1uN8zb29uDOVjN3GwQY4MYGwR5Z4+OjoZ5dXW10vuPjo7qPE5j3GwQY4MYG6S1O3tkZCTMY2NjYX57exvYz97Y2Ajz4uJipfdfXV3VeZzGuNkgxgYxNkhrd3b/5839XxseHBw0eZxSFxcXYX56emrpJP/HzQYxNoixQVq7sx8eHsLc5B29ublZ6fU3Nzdhfnl5qfM4jXGzQYwNYmwQ5PfZVT8Lz4WbDWJsEGODYO7spaWl3uO5ublK7z08PKz5NO1ws0GMDWJsEMydvbu723s8MTFR+trz8/Mwn5ycDORMTXOzQYwNYmwQzJ09Ozv749deXl6G+fX1te7jtMLNBjE2CObXeBX7+/ttH2Eg3GwQY4MYGyTbO3t5eTnMn7/ipHKzQYwNYmyQbO/slZWVMI+Pj7d0kuHhZoMYG8TYINne2VtbWz9+7fHxcZhvb2/rPs5QcLNBjA1ibJBs7+wqnp+fw/z+/t7SSQbLzQYxNoixQbK5s6empsI8PT3d0kmGl5sNYmwQY4Nkc2fPzMyEeX5+Pswppd7joigaOdOwcbNBjA1ibJBs7uzvUO/pz9xsEGODGBskmzv7+vo6zKenp2FeW1v78r39/5VTrtxsEGODGBskmzv74+OjdL67u+s93tvbC8/l+m+o9HOzQYwNkso+Rkwp+RnjL1MURfrqOTcbxNggxgYxNoixQYwNYmwQY4MYG8TYIMYGMTaIsUGMDWJskNLvs5UXNxvE2CDGBjE2iLFBjA3yDwbIdgWjrKqqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAElklEQVR4nO2dyyt0YRzHf6M3uWRI2Ayl7ChiY2xskKJYKLd/QFhLkawsrCRlYe0ShSjZKEVZWCCXjbKQy4KN5LJi3t3J79f7Dmduzsz3+1k9384wjz6e+c15zvOc4wuFQkIwSPvtDpDEQdlAUDYQlA0EZQNB2UD8CXfQ5/PxvCzJCIVCvv8d48gGgrKBoGwgKBsIygaCsoGgbCAoGwjKBoKygaBsICgbCMoGgrKBoGwgKBsIygaCsoEIuyyJ/JuSkhKV/X6/q59/fHxU+eHhIeo+/QSObCAoGwjKBgKmZgeDQadta66lra1N5UAgoHJ5ebnKBQUFKvt8ejWv3Sl7fX2t8sHBgdOemppSx46OjsL21Q0c2UBQNhCUDUTS1uycnByVu7u7VR4dHVW5qKjIaaenp6tj39VYt7y8vKj89vamclZWlsqNjY1O++bmRh1jzSYRQdlAUDYQvnD1yctbdu258NraWsS/y9bsk5MTlZ+enlS+u7tTeXNzU+Xj42OVr66uIu6bW7hll4gIZUNB2UAk7Xl2Zmamq9ff39877Y2NDXVsf39f5a2tLZVfX19d9s6bcGQDQdlAUDYQnq3ZtbW1Kk9OTqpcX1+vsp0vWF1dVbmzszOGvUtOOLKBoGwgKBsIz9TsvLw8le25sF3nZWu0zefn57HrXIrAkQ0EZQNB2UB4pmb39fWpbGu0ZW9vT+WxsTGV7TVlwpENBWUDQdlAeKZmZ2RkuHq9nRufn59X2e6Zmp6ejqxjKQRHNhCUDYRnlhJ3dHSovLy8HPb1323Z+fj4UPny8tJp26nUiYkJlZN5qpVLiYmIUDYUlA2EZ2r2d/T09KhcVlamcldXl8qFhYVh81fS0vT//Ofnp8qDg4MqLy4uqvz8/Pzf351oWLOJiFA2FJQNRNLUbLeUlpaq3Nra6rTb29vVsYaGBpW/u83Gzs6OyiMjIyrH8tYYbmHNJiJC2VBQNhApW7PDYbf7trS0qGxrcGVlpcp2Xv79/V1l+x3g8PAwon5GAms2ERHKhoKygYCs2W6x232Hh4dVrqqqUtneWmthYUFlu2QqlrBmExGhbCgoG4iE1ez8/HyV7bmuvUWkl8nOzlb567y7iMjS0pLK9nq3nZu3W5migTWbiAhlQ0HZQCSsZp+enqo8Pj6u8vr6eqze6teZmZlReWBgQOXd3V2Vvz42IlpYs4mIUDYUlA1Ewmq23XtlHyNs123Z227c3t7Gqitxp6mpSeXt7W2Vvz6OUUSkt7fXaUf7d7JmExGhbCgoG4iE3WZjdnZW5f7+fpWbm5tVPjs7U3loaEhle43YrgOLJ7m5uSrX1NSobB9DZamrq1O5urraacfzuwlHNhCUDQRlA5Gw82y/36/yysqKym7nh+06L/sIxIuLC5UrKip+dOwnBINBlQOBgMp2XbmdU5ibm1PZXieIBp5nExGhbCh+bSmx/VgvLi6O11v9OnZZUjxPr/gxTkSEsqGgbCC4/SfFYM0mIkLZUFA2EJQNBGUDQdlAUDYQlA0EZQNB2UBQNhBh58ZJasGRDQRlA0HZQFA2EJQNBGUD8RcdGlt740WlqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADbklEQVR4nO3dv0tqYRzH8edchVYHw5YQWgwJamyKpoZyC2pzjAI3/wbHCNqEoBaJhpoaaglam4OCIiKEQFqaQii8wwW538O9p/Qc9Xg+79fkFzvwwJvHB80fXqfTcdDwa9QLwPAQWwixhRBbCLGFEFtIOuhOz/N4XjZmOp2O97/72NlCiC2E2EKILYTYQogthNhCiC2E2EKILYTYQogthNhCiC2E2EIC/5+NP4rFopkPDg7MvLW1Zebb29uBr6kf7GwhxBbCw/g/TE9Pm/n09NTMhULBzJubm2a+u7sz89fXV4Sr6x87WwixhRBbiBf0KU7VtxJfXV2ZeXl5uafrZ2ZmzPz8/BxyRT/HW4nhnCO2FGIL4Xl2BOr1uplfX19HtJJg7GwhxBZCbCGc2c65hYUFM+fz+Z6ubzabZm6322GXNBDsbCHEFkJsIZKvjc/OzprZ/1r41NRU4PVPT09mXlxcNPPb21uI1YXDa+NwzhFbCrGFSD7PrtVqZv7ujPY7PDw08yjP6F6ws4UQWwixhcic2aVSqXt7dXW1p2tfXl7MfHJyEsmaho2dLYTYQogtRObMrlQq3dsTExOBf/v+/m7mjY0NMz8+Pka3sCFiZwshthBiC0nsme1/L/fKysqPr72+vjbzzc1NJGsaNXa2EGILSczDeCqVMvN3T6/+5v8ajL29vUjWFDfsbCHEFkJsIYk5s/1fhVEul3987dHRkZn9T72Sgp0thNhCiC1kbM/snZ0dM/u/GThIq9Uy8+7ubiRrijt2thBiCyG2kLE5s3O5nJn9X/s8Pz8feP3Dw0P39vr6urnv/v4+5OrGAztbCLGFEFtIbM/syclJMx8fH5t5aWkp8HrPs982cXFx0b0d11/nGTR2thBiCyG2kNie2dvb22bu9acbzs7OzFytVsMuaeyxs4UQWwixhcT2zA7r/PzczJ+fnyNaSXyws4UQWwixhcTmzM5ms2ZeW1sb0UqSi50thNhCiC0kNmd2Om2Xkslkerp+f3/fzI1GI+ySEoedLYTYQogtJLY/9TQ3N2fmy8tLM/t/nsn/Wa+Pj4/BLCzm+KknOOeILSW2D+PoDw/jcM4RWwqxhRBbCLGFEFsIsYUQWwixhRBbCLGFEFsIsYUQWwixhQT+PxvJws4WQmwhxBZCbCHEFkJsIb8BR7Wlrm5+wwcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Plotting some of the train datapoints\n",
    "\n",
    "for _ in range(5):\n",
    "    output = plot_datapoint(mnist_train_dataset[np.random.randint(len(mnist_train_dataset))])\n",
    "    print(f'Label : {output.label}')\n",
    "    # showing image\n",
    "    plt.show()\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06535ca",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 2. Model\n",
    "We will build a simple neural net to classify `MNIST` handwritings. In the cell below, a simple pytorch net is defined with just one added decorator `@transform`. This is enough to wrap the pytorch model into `padl.Transform` and use it with other transform to build a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6434c1",
   "metadata": {},
   "source": [
    "### 2.1 Simple net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ad5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "        \n",
    "@transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv 1\n",
    "        # size : input: 28x28x1 -> output : 26 x 26 x 32\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 2\n",
    "        # size : input: 26x26x32 -> output : 24 x 24 x 32\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 3\n",
    "        # size : input: 24x24x32 -> output : 12 x 12 x 32\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 4\n",
    "        # size : input : 12 x 12 x 32 -> output : 8 x 8 x 64\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Conv 5\n",
    "        # size : input: 8x8x64 -> output : 4 x 4 x 64 -> Linearize = 1024\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "        \n",
    "        # FC 1 \n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        \n",
    "        # FC 2\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2265f",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing\n",
    "\n",
    "The `preprocess` pipeline below, again splits the datapoint to two different pipelines for `PIL.Image` and `int` label. First part of parralel pipeline, converts the image into torch tensor of type float, and second part passes the label as it is. The label later will be used for the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ab1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def convert_to_tensor(img):\n",
    "    arr = np.asarray(img)\n",
    "    return torch.tensor(arr).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "preprocess = (\n",
    "    convert_to_tensor / convert_to_tensor\n",
    "    >> padl.same.reshape(-1, 28, 28) / padl.Identity()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec776202",
   "metadata": {},
   "source": [
    "### 2.3 Instantiating the network and loss function\n",
    "\n",
    "Initialising instances of `SimpleNet` and `loss` function. Loss function here is a wrapped `torch` negative log likelihood loss which is again wrapped easily with same `transform` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a699bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()\n",
    "loss_func = transform(F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bce20bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mSimpleNet()\u001b[0m - \"simplenet\":\n",
       "\n",
       "   SimpleNet(\n",
       "     (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "     (batchnorm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "     (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "     (batchnorm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (batchnorm4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv5): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "     (batchnorm5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv5_drop): Dropout2d(p=0.5, inplace=False)\n",
       "     (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "     (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "   )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "616132d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mnll_loss\u001b[0m - \"loss_func\":\n",
       "\n",
       "   def nll_loss(\n",
       "       input: Tensor,\n",
       "       target: Tensor,\n",
       "       weight: Optional[Tensor] = None,\n",
       "       size_average: Optional[bool] = None,\n",
       "       ignore_index: int = -100,\n",
       "       reduce: Optional[bool] = None,\n",
       "       reduction: str = \"mean\",\n",
       "   ) -> Tensor:\n",
       "       r\"\"\"The negative log likelihood loss.\n",
       "\n",
       "       See :class:`~torch.nn.NLLLoss` for details.\n",
       "\n",
       "       Args:\n",
       "           input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
       "               in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n",
       "               in the case of K-dimensional loss. `input` is expected to be log-probabilities.\n",
       "           target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
       "               or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
       "               K-dimensional loss.\n",
       "           weight (Tensor, optional): a manual rescaling weight given to each\n",
       "               class. If given, has to be a Tensor of size `C`\n",
       "           size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "               the losses are averaged over each loss element in the batch. Note that for\n",
       "               some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
       "               is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "               when reduce is ``False``. Default: ``True``\n",
       "           ignore_index (int, optional): Specifies a target value that is ignored\n",
       "               and does not contribute to the input gradient. When :attr:`size_average` is\n",
       "               ``True``, the loss is averaged over non-ignored targets. Default: -100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25508e",
   "metadata": {},
   "source": [
    "### 2.4  Building the training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691e6e7",
   "metadata": {},
   "source": [
    "`train_model` is now composed (`>>`) with the transforms already defined.\n",
    "- preprocess: preprocessing transform defined above\n",
    "- Batchify: Batchify is a inbuilt `transform` that marks end of preprocess (dataloading) and that adds batch dimension to the inputs. Batchify also moves the input tensors to device specified for the model\n",
    "- simplenet: Instance of SimpleNet\n",
    "- padl.same: A self reflexive transform that allows for a quick mutation of input.\n",
    "\n",
    "`train_model` is then sent to the intended device. It is by default in `cpu`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8134bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to be used:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device to be used: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c73d5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"train_model\":\n",
       "\n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ img                 ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor   \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                     │\n",
       "      ▼ args                ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28) \u001b[32m/\u001b[0m Identity()       \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)    \n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ x                   ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mSimpleNet()         \u001b[32m/\u001b[0m type(torch.int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model = (\n",
    "    preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.same.type(torch.long)\n",
    ")\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d982878",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 3. Training and validating the `train_model`\n",
    "\n",
    "Training is not much different than the normal torch training steps, except dataloading and training is made even simplier by `train_apply`. It is one of the three inbuilt methods along with `infer_apply` and `eval_apply` that handles the mode context of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f95e62b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0; Step: 0; loss: 2.32843279838562\n",
      "Epoch:0; Step: 10; loss: 2.0219168663024902\n",
      "Epoch:0; Step: 20; loss: 1.6625076532363892\n",
      "Epoch:0; Step: 30; loss: 1.3506853580474854\n",
      "Epoch:0; Step: 40; loss: 1.0103685855865479\n",
      "Epoch:0; Step: 50; loss: 0.9072323441505432\n",
      "Epoch:0; Step: 60; loss: 0.6765532493591309\n",
      "Epoch:0; Step: 70; loss: 0.5504281520843506\n",
      "Epoch:0; Step: 80; loss: 0.5161393880844116\n",
      "Epoch:0; Step: 90; loss: 0.48721829056739807\n",
      "Epoch:0; Step: 100; loss: 0.4040915369987488\n",
      "Epoch:0; Step: 110; loss: 0.4484071135520935\n",
      "Epoch:0; Step: 120; loss: 0.4975182116031647\n",
      "Epoch:0; Step: 130; loss: 0.3997626006603241\n",
      "Epoch:0; Step: 140; loss: 0.36582285165786743\n",
      "Epoch:0; Step: 150; loss: 0.36289504170417786\n",
      "Epoch:0; Step: 160; loss: 0.3392089605331421\n",
      "Epoch:0; Step: 170; loss: 0.25499850511550903\n",
      "Epoch:0; Step: 180; loss: 0.3611798882484436\n",
      "Epoch:0; Step: 190; loss: 0.24412992596626282\n",
      "Epoch:0; Step: 200; loss: 0.29443666338920593\n",
      "Epoch:0; Step: 210; loss: 0.31384363770484924\n",
      "Epoch:0; Step: 220; loss: 0.262349933385849\n",
      "Epoch:0; Step: 230; loss: 0.11465992778539658\n",
      "Epoch:1; Step: 0; loss: 0.27609071135520935\n",
      "Epoch:1; Step: 10; loss: 0.268146812915802\n",
      "Epoch:1; Step: 20; loss: 0.3030300438404083\n",
      "Epoch:1; Step: 30; loss: 0.3005148470401764\n",
      "Epoch:1; Step: 40; loss: 0.2602699100971222\n",
      "Epoch:1; Step: 50; loss: 0.32787078619003296\n",
      "Epoch:1; Step: 60; loss: 0.2652759850025177\n",
      "Epoch:1; Step: 70; loss: 0.22171984612941742\n",
      "Epoch:1; Step: 80; loss: 0.25606244802474976\n",
      "Epoch:1; Step: 90; loss: 0.23839057981967926\n",
      "Epoch:1; Step: 100; loss: 0.24537107348442078\n",
      "Epoch:1; Step: 110; loss: 0.29235419631004333\n",
      "Epoch:1; Step: 120; loss: 0.3353230059146881\n",
      "Epoch:1; Step: 130; loss: 0.28084319829940796\n",
      "Epoch:1; Step: 140; loss: 0.25138482451438904\n",
      "Epoch:1; Step: 150; loss: 0.27523210644721985\n",
      "Epoch:1; Step: 160; loss: 0.2614591419696808\n",
      "Epoch:1; Step: 170; loss: 0.1818043738603592\n",
      "Epoch:1; Step: 180; loss: 0.32764333486557007\n",
      "Epoch:1; Step: 190; loss: 0.1667155772447586\n",
      "Epoch:1; Step: 200; loss: 0.2421381026506424\n",
      "Epoch:1; Step: 210; loss: 0.2603617310523987\n",
      "Epoch:1; Step: 220; loss: 0.25019964575767517\n",
      "Epoch:1; Step: 230; loss: 0.09175363183021545\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 2\n",
    "num_workers = 0\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(mnist_train_dataset, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b68ed",
   "metadata": {},
   "source": [
    "### 3.1 Accuracy of the model\n",
    "\n",
    "We can quickly build a `validation_model` by adding a further step to `train_model` to get the number associated with the maximum confidence predicted by the model. \n",
    "\n",
    "\n",
    "Note: As we don't have a separate validation dataset with labels, we will have to use the same train data to `validate` the model in this example.\n",
    "\n",
    "\n",
    "First, lets look at the format of the prediction by `infer_apply`ing one of datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35b39dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"train_model\":\n",
       "\n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ img                 ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor   \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                     │\n",
       "      ▼ args                ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28) \u001b[32m/\u001b[0m Identity()       \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)    \n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ x                   ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mSimpleNet()         \u001b[32m/\u001b[0m type(torch.int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "556e1b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "namedtuple(simplenet=tensor([[-5.2692, -6.7916, -5.2562, -0.9152, -8.7154, -0.5542, -7.0435, -6.5707,\n",
       "         -4.5927, -6.9072]]), out_1=tensor([5]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.infer_apply(mnist_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8105d54",
   "metadata": {},
   "source": [
    "`train_model` predicts a tensor of confidence associated for the 10 numbers, and the index associated with the maximum of these confidence is the prediction by the model. Thus, we can add another transform to the same `train_model` to get that index associated with maximum of the confidence. \n",
    "\n",
    "Note that the new `validation_model` is a new instance of Transform but it contains same objects as `train_model` with added two new `transform`s. All the transform objects that are already in `train_model` is in `device` as assigned above, but the main `validation_model` object itself will have by default `cpu` device assigned. Thus, to move it to (or assign it with) correct device, we have to again call `pd_to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27a6b4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"validation_model\":\n",
       "\n",
       "   \u001b[32m   │└───────────────────────────┐\n",
       "      │                            │\n",
       "      ▼ img                        ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor          \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                            │\n",
       "      ▼ args                       ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28)        \u001b[32m/\u001b[0m Identity()       \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)           \n",
       "   \u001b[32m   │└───────────────────────────┐\n",
       "      │                            │\n",
       "      ▼ x                          ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mSimpleNet()                \u001b[32m/\u001b[0m type(torch.int64)\n",
       "   \u001b[32m   │                            │\n",
       "      ▼ x                          ▼ args\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mlambda x: x.max(1).indices \u001b[32m/\u001b[0m Identity()       "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_model = (\n",
    "    train_model\n",
    "    >> padl.transform(lambda x: x.max(1).indices) / padl.Identity()\n",
    ")\n",
    "\n",
    "# We need to send the validation_model to device again\n",
    "validation_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27736457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 0.9519\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy over the test dataset\n",
    "\n",
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(mnist_test_dataset, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ len(mnist_test_dataset)\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f570",
   "metadata": {},
   "source": [
    "Not a bad accuracy of `~0.95` for a quick train model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fe164",
   "metadata": {},
   "source": [
    "### 3.2 Infer few images from test data\n",
    "\n",
    "Although we do not have labels for images in test data, we can still infer and verify the predictions ourselves. For that, we can again use model object `simplenet` that we have trained by using `train_model` and now stack it with other `transform`s to build an infer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "278c177b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"infer_model\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m0: \u001b[0m__getitem__(0)            \n",
       "   \u001b[32m   │\n",
       "      ▼ img\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mconvert_to_tensor         \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)           \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0munsqueeze(1)              \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mSimpleNet()               \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m5: \u001b[0mlambda x: x.max(1).indices"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_preprocess =(\n",
    "    padl.same[0]\n",
    "    >> convert_to_tensor\n",
    ")\n",
    "infer_model = (\n",
    "    infer_preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> padl.same.unsqueeze(1) \n",
    "    >> simplenet\n",
    "    >> padl.transform(lambda x: x.max(1).indices)\n",
    ")\n",
    "\n",
    "infer_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dca7d6ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADyUlEQVR4nO2dTyu0URiHz/N6YySUJBYsJVGKhYWVBTs+zHwHW1laWimRJYqywEJSyoas2LAQdv7VvKtXc9/xMOaZmTPzu67V/HpmzjnT1e3OPOfMJIVCIYAGf2q9AKgeyBYC2UIgWwhkC4FsIf6mXUyShP/L6oxCoZB8dY3KFgLZQiBbCGQLgWwhkC0EsoVAthDIFgLZQiBbCGQLgWwhkC0EsoVAthDIFgLZQqRuS2pU+vr6TD49PTW5t7fX5KGhIZMvLi4yXc/8/PzH44ODA3Pt/v4+s3mobCGQLQSyhYi2Z+dyOZOfn58zG3tqasrknp4ek/3J1paWlszm/ozX19cv584SKlsIZAuBbCGi6dm+L66urpq8v79v8tLS0q/nGh8fL+n5vqdnzdbWVkXH/w+VLQSyhUC2ENH0bN9H5+bmTB4cHDS5nJ5dKjc3N1Wbq5JQ2UIgWwhkCxFNz768vEy9PjAwYPLIyIjJ5+fnma+p0aCyhUC2EMgWIpqe/f7+bvLDw4PJXV1dJnd0dPx6riRJUvPV1ZXJt7e3v54rJqhsIZAtBLKFiKZne/xeLJ9HR0dNPjo6+vHY/rV+7Lu7O5Ofnp5+PHbMUNlCIFsIZAsRTc9+fHw02Z958ve3JyYmTF5eXv7xXP61KlDZQiBbCGQLEU3PbmtrM3l4eDjT8dvb2z8eNzU1pT63tbXVZH8+299bn56eTh3v8PDQ5J2dHZOLz3pVEipbCGQLkaQdEa3mr/90d3eb7D+y9Pjr29vbJvf395tcfAt1ZmYmdWx/y7PcY7R+vHw+b/Li4mJZ4xfDr/9ACAHZUiBbiGh6tv9aDX/LcmxsLPX1WfZZP5b/1+j4+Dj19X5btH8vZ2dnJp+cnJS6xC+hZ0MIAdlSIFuIaHq2Z3Jy0uS1tTWTOzs7TX55eTHZv6/ircfNzc2pc6+srJi8sLBgst9qHBP0bAghIFsKZAsRbc/+Dn+b8fr6OvX5m5ubH4/9FifP7Oysybu7uyWurnbQsyGEgGwpkC1ENNuSSuW7Hl0O9dSjS4HKFgLZQiBbiLrt2Vny3ddyNQpUthDIFgLZQsj07OJ9ZX6PmT8e3KhQ2UIgWwhkCyHTs4vv21fyJxBjhsoWAtlCIFsImZ6dxt7eXq2XUBWobCGQLQSyhZDs2f489MbGRo1WUl2obCGQLYTkn/H19XWT397earSS6kJlC4FsIZAtRN0e2YXP4cguhBCQLQWyhUC2EMgWAtlCIFsIZAuBbCGQLQSyhUj9bBwaCypbCGQLgWwhkC0EsoVAthD/AKM65ryAwj8PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 4\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADzklEQVR4nO2d2VHcQBRFn1zOgiWNYUmDJQ4gDpY0WOJgSQPIY/xjU+prTz9U0vRofM/5oktCCE71PG4vUrdcLgM8+LHpG4B2INsIZBuBbCOQbQSyjfhZO9h1Hblsy1gul92qY/RsI5BtBLKNQLYRyDYC2UYg2whkG4FsI5BtBLKNQLYRyDYC2UYg2whkG4FsI5BtRHVZ0jaxt7dXtE9OTor2xcXFynOVx8fHov309FS0F4tF9Xzl9fW1erwV9GwjkG0Eso3oars457yU+OzsrGjf399Xz395efn6+vDwcC33tIqPj4+ifXx8XD0+BpYSQ0Qg2wpkG7E1Ofv6+rpoX15eFm3NuldXV9Xvb4nmer0X/f9jXdCzjUC2Ecg2YrY5++HhoWifnp4W7Zubm6KtNTr7/j5a//VczeXn5+dFu5/hI/6uybWfrdcfO45OzoaIQLYVyDZiNjm7P98c8Xed07qoNVqzaq1Oag3WOrm7u1s9f2dnp2jr2LbeS/YUyf69rnPum55tBLKNQLYRs8nZWV3b398v2lonn5+fi7bW2SFZNpsr13H4bGw7u7c+XbcyJn8LcjZEBLKtQLYRG8vZ2fyyjn1rjT44OCjaWgc1l0+ZX3Udecbd3V3Rbr0G7g/0bCOQbQSyjdhYzh6bq9/f34u2rvPSOWed366R5ewJsvDKY9k8/TeuTc4GZFuBbCOa5Wydr87I9j9le6y1po8h2389JZ+fn2u7Nj3bCGQb0exjPBsiHLptVc/Xj/Vs6dEQhsafMeh9397eTnZterYRyDYC2UY0Gy7NhkeHLvXJhjR1ivPo6Ci7xWbU/hZjhnl/X5vhUkC2Fcg2Yjbbf4bmbK1l+vhK3f7TX8Y0l8dL/ouhNXoI9GwjkG0Eso2YTc3OpiwzNHdre8pHWWwr9GwjkG0Eso2YTc3OHh81FM2rul2oJdk4f6tlT/RsI5BtBLKNmM18tqKPmJxyLda60TGDbFlzf6vT2FdIMJ8NEYFsK5BtRLOcrVkyy9W6dVUfbTHl65GmRufWFV1n1up3oWcbgWwjkG1Es5w9NHsqLV9xOJTsNVQtxwzI2RARyLYC2UZs7NFYY2u4ojlea/jb29vX17oPLHvtw2KxKNo6RqC/y9B9a1NCzYaIQLYVyDZiNq+N0DViWhc1q24SrfGamzc5907NhohAthWz+RjPyN4un1GbUh0S2yLWu612LHyMQ0Qg2wpkG7E1NRu+BzUbIgLZViDbCGQbgWwjkG0Eso1AthHINgLZRiDbiOrYOPxf0LONQLYRyDYC2UYg2whkG/ELeVh155tRIkMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADXUlEQVR4nO3csUsjQRjG4W+PQxHSCRZCShs7O1tJKzEIErsIVnaW2gRCyD+gRSCQWpCQRrQS0lpY2pg2hSCkCmilueYQvsHsXe42mt3391R5yQpzvMyNM+5uNB6PDRp+fPcA8HUoWwhlC6FsIZQthLKF/Iz7Mooi9mUpMx6Po0nfMbOFULYQyhZC2UIoWwhlC6FsIZQthLKFULYQyhZC2UIoWwhlC6FsIZQthLKFULaQ2NuS8G+2t7ddvrq6cnlvb8/lTqcz8zGZMbOlULYQyhbCmp2Azc1Nly8uLlx+f3//yuFMxMwWQtlCKFsIa3YCCoWCy0tLSy4/Pz+73Ov1Zj6mzzCzhVC2EMoWEsW9LYlHdj/XbDZdrlQqLi8sLLicz+ddfnp6ms3AjEd28RtlC6FsIeyz/8L9/b3LGxsbLr+8vLi8s7Pj8izX6Gkws4VQthDKFsKabWbLy8su7+/vuxyu0aPRyOWzszOXr6+vExxdcpjZQihbCGULkTwbD9fo09NTl4+Pj12Ooij2+/Pz88TG9r84G4eZUbYUyhYiuWa3222Xw79Hh25ublwul8suv76+JjOwBLBmw8woWwplC5E5G2+1Wh+fDw4OYq8Nz7aLxeIshvTlmNlCKFtIZrdeq6urLg8Gg4/P4b+52+26HG7F5mlr9SdsvWBmlC2FsoVkZusVPmITd2tQuEYfHh66nKY1ehrMbCGULYSyhWRmzT46OnJ5fX194rWXl5cuh7cGZxUzWwhlC6FsIak9Gz85OXG50WjEXv/4+PjxOW49TzvOxmFmlC2FsoWkZp+9uLjocvgKybjfPczMSqVS0kNKHWa2EMoWQtlCUrNmh4/Fbm1txV5fr9dd7vf7iY8pbZjZQihbCGULSc3Z+Nvbm8vhuMN7znZ3d2N/Pqs4G4eZUbYUyhYyt/vsarXqcvh6quFw6HKtVnNZZY2eBjNbCGULoWwhc7PPDl8x+fDw4PLKyorL4eupOp3ObAaWMuyzYWaULYWyhczNPnttbc3lXC73TSPJLma2EMoWMjf/jd/d3bl8e3vrclbeMvidmNlCKFsIZQuZm+NSJIPjUpgZZUuhbCGULYSyhVC2EMoWQtlCKFsIZQuhbCGxZ+PIFma2EMoWQtlCKFsIZQuhbCG/AD8ozpg3oWMfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEkklEQVR4nO2cTSi0XRzGz7yekoWPKPkqWRALUmKBhUjyEUUsbIRIoVhIbCysLSysLVhILERJkgUpG8pHSTY+IgslC1+leRdv3blOz9zzzmNmzOO6fqtzdWbGmX4d/zn3ue/j8Xq9RnDwz3cPQIQPySZCsomQbCIkmwjJJuKXW6fH49G67C/D6/V6fPVpZhMh2URINhGSTYRkEyHZREg2EZJNhGQTIdlESDYRkk2EZBMh2URINhGSTYRkEyHZREg2EZJNhGQTIdlEuN5KLP4jNTUV8sPDA+T39/dwDueP0cwmQrKJkGwiaGp2cnKy005MTIS+zs5OyHV1dZDt0yna2togl5eXQ15aWoL8+PgY0FhDhWY2EZJNhGQT8WNqdkJCAuSpqSnI1dXVTjs9PR36PB58ytXfCVKNjY2Qh4aGIJ+dnUF+fX2FPDw8DLm7u9vna4OJZjYRkk2EZBPhcatPkXzMRlxcHOSJiQnILS0tkJOSkpz24uIi9HV1dUE+OTmBvLe3B7m1tRXy29sb5IODA8hpaWmQMzIyIOfn5zvt5+dn6Ht6ejKBoGM2hDFGsqmQbCL+2po9MzMDua+vD7K9x7ywsOC07WvhDQ0NkPf39yG/vLxAttfp9rX04+NjyFFRUZDn5+chb21tOe3JyUnoOz09NYGgmi2MMZJNhWQTEbE1u7S0FPLIyAjkpqYmyPb3WFlZgdzc3BzE0X2NgYEByNPT00776uoK+rKysgL6bNVsYYyRbCokm4hv28/u7e2FfHR0BNleb1ZUVEC+u7uDvLOz4/r+SMLterf2s0VQkGwiJJuIsK2z7XurBwcHIVdWVkKOjY2FvLGxAdmu+ff3918dYsgoKCiAPDs7C7mwsNDne+3r6v7QOlsYYySbirAtvUZHRyHX19e7vt7eNjw8PIQcyf+2y8rKIK+trUGOj4/3+d6xsbGQjMkYzWwqJJsIySYibDU7Ozsbsr3k6+/vh7y5uQn54+MjNAP7H0RHR0O2l4n275GYmBjI9m3P9ne/vb112uvr6388Tn9oZhMh2URINhFhq9k5OTmQ7bqVl5cHOdg1OjMz02dfTU0N5NraWsgpKSmQS0pKXP+Wv0eA7VuVe3p6nHagtw4HgmY2EZJNhGQTEbYtzrm5Ocjt7e2Q7cd1bm5uIC8vL0P+fDSFMf6Pxri8vHTa9u8HezvV32fZ2Edf2WO18/b2NuRgHoepLU5hjJFsKiSbiLDVbHuPd3V1FbLbHu/vCPQ4q6981ud6b4wx19fXkO09aPtYjnCimi2MMZJNhWQT8W2P7Nq31xYVFUEuLi6G/Pn4qN9xfn4O2X6k1z52+jN2zbbXvVVVVZB3d3ddx/KdqGYLY4xkUyHZRETsMRtfpaOjA3Jubq7Ttq+Nj4+PQ7b30i8uLoI8utChmi2MMZJNhWQT8WNrNiuq2cIYI9lUSDYRkk2EZBMh2URINhGSTYRkEyHZREg2EZJNhGQTIdlESDYRkk2EZBMh2URINhGSTYRkEyHZREg2EZJNhGQTIdlEuD7+I34WmtlESDYRkk2EZBMh2URINhH/AptwSuV8zDl3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEPUlEQVR4nO2dTSi0URiGz6D8lJ9Q2LCgCCFRklAKZWXLTlkKe7GyUTaSslA2yl5RklixEJKfrZUiC+VnI+bbTXOf7/uOMfOeMeO+r9XcvWf06PLM4533LxQOh43gIOOnCxDJQ7KJkGwiJJsIySZCsonIcm0MhULaL0szwuFw6H/b1NlESDYRkk2EZBMh2URINhGSTYRkEyHZREg2EZJNhGQTIdlESDYRkk2E83i2CIbGxkbIFxcXkNfX1yOvx8bGvNWhziZCsonQx7gHcnJyIC8tLUG2r8IpLi72XpMx6mwqJJsIySZCM9sD9kwuLS11ri8sLPRZTgR1NhGSTYRkE5E2M9veF83KwtIfHh6SWY6Tnp4eyA0NDc71KysrPsuJoM4mQrKJkGwiUnZmT0xMQJ6dnYV8c3MDubu723tNsTIzM+Pcfnp6CnlnZ8dnORHU2URINhGSTcSPzezq6mrIvb29kO0Zbe9n5+XleakrHqqqqiA3NTU51+/v70N+eXkJvKZ/oc4mQrKJkGwikjazMzMzIQ8ODkK2z9Oy+fj4gLy1tRVMYXFg/y7T09OQ8/PzIX9+fkLe3t72U9gXqLOJkGwiJJuIpM3sqakpyAsLC871b29vkMfHxyFvbm4GUlc8tLe3Q7a/x7ex/x85PDwMvKZYUGcTIdlESDYRXmd2RUVF5PXIyIhz7dPTE+SBgQHIJycngdWVKMPDw87t19fXkBcXF32WEzPqbCIkmwjJJsLrzO7o6Ii8bmlpca5dXV2FnEoz2j52XlJS4lx/e3sL+e7uLuiS4kKdTYRkE+H1Yzz6MpiMDPy7ury8hLy2tuazlISwT4nq6uqCHArhA3eKiop8lxQX6mwiJJsIySbC68yO3t2yT82pr6+HbM/w3d3dQGs5OzuLvG5tbYVtXz0wvq6uDnJNTY3z/an6AHp1NhGSTYRkExFyzZdEn7IbfYhzY2MDttm3okgm9n5x0DN2cnIS8vLycqA/34WesiuMMZJNhWQT4XVmR5OdnQ25ubkZcl9f37d+nn1qcW5ubszvfX5+hvz4+Ai5ra0Nsn25j83x8THk/v5+yK+vrzHXliia2cIYI9lUSDYRSZvZqUxlZSXko6MjyOXl5ZDt22J0dnZCvrq6CrC676GZLYwxkk2FZBORsrezTCb2JbcFBQXO9XNzc5B/ckZ/B3U2EZJNhGQTQTmz7e+uh4aGINuX+xwcHEC2b0eZLqiziZBsIiSbCMqZbR9br62tda7f29uDbD/ZPl1QZxMh2URINhGUM/v9/R3y/f095LKyMsjn5+e+S0oK6mwiJJsIySZC56CZv88THx0dhTw/Pw/ZPs88ldA5aMIYI9lU6GP8l6GPcWGMkWwqJJsIySZCsomQbCIkmwjJJkKyiZBsIiSbCMkmQrKJkGwiJJsI5/Fs8btQZxMh2URINhGSTYRkEyHZRPwBIF3oFbpIbJkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 4\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    data_point = mnist_test_dataset[np.random.randint(len(mnist_test_dataset))]\n",
    "    output = plot_datapoint(data_point)\n",
    "    plt.show()\n",
    "    print(f'Prediction: {infer_model.infer_apply(data_point).item()}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab222191",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 4. Using further image augmentation on training\n",
    "\n",
    "We can easily use some of the `torchvision.transforms` for image augmentation and add it to our preproccessing of image to help with training. Lets add a couple of augmentations to our training: `GaussianBlur` and `RandomRotation`\n",
    "We need to wrap the call to these `torchvision.transforms` with our `padl.transform` before instantiating them, and that is all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "543ba923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f645938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_blur = transform(T.GaussianBlur)(kernel_size=(3,3), sigma=0.1)\n",
    "\n",
    "rotate_img = transform(T.RandomRotation)(degrees=(-15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a0a2f",
   "metadata": {},
   "source": [
    "Now we can again use `padl`'s functional api to build an `image_augmentation` pipeline.\n",
    "\n",
    "Note: `torchvision.transforms` expect images with channels but our images are just in grayscale. So, we need to unsqueeze our image tensor here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f39aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_augmentation = (\n",
    "    padl.same.unsqueeze(0)\n",
    "    >> rotate_img\n",
    "    >> gaussian_blur\n",
    "    >> padl.same[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0dc1b",
   "metadata": {},
   "source": [
    "####  Sample of image augmentation\n",
    "\n",
    "Lets try the augmentation on one of image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "209547d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADrklEQVR4nO3dvyt9cRzH8XMkZSEkKSUDNiYDA4oku4lsKP+GSZlNBgOj2AwmE8LodylisClFSl3byfvz5Xy59/M5516v52M6r07O+eTVx8fh3HPiQqEQQUNV3gNAdihbCGULoWwhlC2EsoVUp+2M45jrsgpTKBTi7/Yxs4VQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQtpDU25JQnJGREZM3NjZMHhoaMvny8jL4mKKImS2FsoVQtpCga/bg4GCy3dTUZPZtbW2FPHWu+vr6TD46OsppJBYzWwhlC6FsIUHX7OHh4WS7s7PT7PtLa3ZVlZ0zHR0dJre3t5scx99+QicoZrYQyhZC2UKCrtkzMzPJ9v7+fshT5aq1tdXk2dlZk9fX102+uLgIPqavMLOFULYQyhYSdM12rz//qtXV1dT919fXGY0knUYbiKKIsqVQthCva3ZPT4/JLS0tPg9fturr61P37+7uZjSSdMxsIZQthLKFeF2zJyYmTK6trfV5+LLh/i7i/v/a9fDwEHI4P8bMFkLZQihbiNc1u7u7+9t9p6enPk+Vq+XlZZPdNfzq6srk5+fn4GP6CWa2EMoWQtlCMvt8drl83ukrdXV1Jo+Pj5s8PT1t8tjYWOrxFhcXTX56eip+cB4xs4VQtpDMfow3NjaW9PW9vb0mux+hGR0dNbmtrc3kmpqaZHtqasrsc2+fen19Nfnw8NDkt7c3k6ur7bfx5OQkKkfMbCGULYSyhcRpL0v/7Rv7VlZWTJ6fn0+23cuPu7u73xz6n1ue3DX7/f3d5JeXF5PPzs6SbXcNPj4+Nnlvb8/kx8dHk+/v701uaGgw+fPvB1njjX2IooiypVC2EK/X2QsLCybf3t4m2wMDAyUd213jt7e3TT4/Pzf54OCgpPN9Njc3Z3Jzc7PJNzc33s4VEjNbCGULoWwhQf82vrS0FPLwmXEfKe3a3NzMaCSlYWYLoWwhlC2E10Z4UCmP5mRmC6FsIZQthLKFULYQyhZC2UK4zi6Ce/9bV1eXyT7/l+4TM1sIZQuhbCGs2UVw77WvlEdtV8Yo4QVlC6FsIazZHvT395u8traWz0D+g5kthLKFULYQ1uwi5PVK5FIxs4VQthB+jP/Azs6OyZOTkzmNpDTMbCGULYSyhXh9NBbyx6OxEEURZUuhbCGULYSyhVC2EMoWQtlCKFsIZQuhbCGpfxvH38LMFkLZQihbCGULoWwhlC3kAw+zohFeAXUJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapoint = mnist_train_dataset[2]\n",
    "\n",
    "# plot datapoint \n",
    "plot_datapoint(datapoint)\n",
    "\n",
    "# convert datapoint's image to tensor\n",
    "img_tensor = convert_to_tensor(datapoint[0])\n",
    "\n",
    "# tensor shape\n",
    "print('tensor shape:', img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4d5f5",
   "metadata": {},
   "source": [
    "Augmented images: Running image augmentation on same image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05fa5c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAD1klEQVR4nO3dvyt9cRzH8XMkZaGUDJQsbMwMKCWZsBhEmZSZklX5GwySgVHKYsCghMJIfpQBi00pMh3T9+b9yT3uj88593zv6/mY7qsP937y6uPTOefec8MoigJoqKn0BJAeyhZC2UIoWwhlC6FsIbVxg2EYclz2n4miKMw3xsoWQtlCKFsIZQuhbCGULYSyhVC2EMoWQtlCKFsIZQuhbCGULYSyhcRez8bvhoaGYse3t7dNHhgYMPnu7s77nArByhZC2UL4N16Co6Mjk5eWlky+uLhIczoFY2ULoWwhlC2EPbsENTV2jXR0dJjc3t5uchjmfXdvqljZQihbCGULCePuvFDux3/Gx8dzj3d3d8t5qkxpbW01+fn52eStrS2TZ2ZmEp/TP3z8B0EQULYUyhbi9Ti7v7/f5ImJidzjatqz19fXY8cfHh5SmklxWNlCKFsIZQvxumcPDg76fLrMamxsjB0/ODhIaSbFYWULoWwhlC3E657tngM+Ozvz+fQV09LSYrJ7/dp1enpqsnt92z2XnhZWthDKFkLZQrzu2e57s6rF6+uryYeHhyZPTU2ZfH9/b/L7+3syEytSdbaDX1G2EMoW4nXPHhsbMznuOHt6etrnSydqb2/P5OHh4difX1lZMfnt7c33lErCyhZC2UIoW4jXPXt0dNTk+vr63OPr62ufL+VVQ0ODySMjIya7n9X6+voyubbW/hmvrq48zs4fVrYQyhbi9d94V1dX3rGkbz2xsLBgcltbm8l1dXW5x+7pTfc07+fnp8nux31eXl5M/rldBUEQ3N7eFjDj9LGyhVC2EMoWktptNpqamsr6/Z6eHpPdw6HV1VWTPz4+TL65uck93tjYMGOXl5cmHx8fmzw7O2tyc3OzyY+Pj/mmnSmsbCGULYSyhaS2Z6+trZm8vLxc1O93d3eb7O7ZfX19Jp+fnxf1/HH+ugvxzs6Ot9dKEitbCGULoWwhXvds95zyz9tunZyclPXcT09PseM+92jX5ORk7Pji4qLJ7i1FkpxbMVjZQihbCGUL8bpnz8/Px2ZUFitbCGULoWwhfG2EB+55+s7OTpM5zkbqKFsIZQthz/bA/eqNrN5uJJuzQiIoWwhlC2HPTkBvb6/Jm5ublZmIg5UthLKFULYQ9mwPsvKVyX9hZQuhbCGULYQ9uwTu+8j39/dNdj//7Zqbm/M+p0KwsoVQtpDQvTxnBsMw/yAyKYqivMeBrGwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQihbSOz1bFQXVrYQyhZC2UIoWwhlC6FsId8HeLN6u6pAJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAD4klEQVR4nO2dzys1URyHz0hKirCQEtlYiSxZoPxI9lZIkr2FkL9B2VKK7ChhIStrxEL5kV/ZsLGzYnnfzdt973euO++NOXMvn+dZzacz5px6fO9x7swZQSqVcqBBSaEHAMmBbCGQLQSyhUC2EMgWojSqMQgC1mU/jFQqFeRqo7KFQLYQyBYC2UIgWwhkC4FsIZAtBLKFQLYQyBYC2UIgWwhkC4FsISLvZ/uku7vb5NraWpN3d3eTHI4EVLYQyBaiYB/jvb29Jl9eXhZmIEJQ2UIgWwhkCxFE7eKM81Hix8dHk4+Pj00eHx+PqytpeJQYnHPIlgLZQiS2zi4p0f29WlhYMHlmZibnuT09PSbf3d3FNg5dA4IgWwhkC+F1zm5ra0sf19XV+eyqqAj/fdLc3Gzy2dmZybOzs+njOOforHF5uzIUHcgWAtlCeJ2zh4eH08fl5eU+uyoq6uvrTZ6enjb56urK5CDI+XV2rFDZQiBbCGQLEeucvb6+nve519fXcXZdVKytrUW2X1xcmHx7e+txNP+gsoVAthDIFsLrOntiYiJ9PDAwYNqOjo58dl1QqqqqItsfHh4SGomFyhYC2UIgW4jEnkGrqalJqivvhO/Nh9fN4fampiaTn5+fvYzrf1DZQiBbCGQLEeuc/fHxYXLmPrKVlRXTtri4aHJHR0fktdvb200O3wPu7+83eWlpKXqwGVRWVpo8NDRk8vb2duS1w3P0/f193n0nCZUtBLKF8Lpld35+Pn3c1dUVeW5jY2Nke+Zjyc5lf4zPzc2Z3NDQYHJZWVn6eHR01LSFH/2dmpoyeWxszOTBwcGc13Yue/vxwcGByW9vb84XbNkF5xyypUC2EIm9ZiPM/v7+t35+b2/P5NXVVZPf399Nvrm5SR+fnp6atvPzc5OXl5dNDp8ffjtjRUWFya2trSYn9diRc8zZ8BdkC4FsIQo2Zxcz4TX66+uryS8vLyZXV1ebHF53JwlzNjjnkC0FsoUo2Cuoi5nMrcbOZd+efXp6MvmnvEKEyhYC2UIgWwjm7E/o6+uLbN/Z2UloJPFCZQuBbCGQLQRz9hf4qf9NkMoWAtlCIFsIZAuBbCGQLQSyhWCdnQfhfWUtLS0mn5ycJDmcL0NlC4FsIZAtBHO2c25ra8vkkZERkycnJ03e3Nz0PiYfUNlCIFsIZAvBnJ0HnZ2dJm9sbBRmIN+EyhYC2ULwMf4JSf0HvaShsoVAthDIFoI52zl3eHhocvjr0t8ClS0EsoVAthC8GuuXwauxwDmHbCmQLQSyhUC2EMgWAtlCRK6z4XdBZQuBbCGQLQSyhUC2EMgW4g9uZL711b+bmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAD30lEQVR4nO2dvS8lURiHz8hGIhKCQogWjRAlBRIfhZ4GUYjGn6DVSpRIJDQKhaBSqZFIKBBfkQgKLQmN5G6ze/e+B7N2M3Pu8Huean45k5mTPN55TWbumSiXyznQoKTYE4BwIFsIZAuBbCGQLQSyhfgRNxhFEfdlX4xcLhd9NEZlC4FsIZAtBLKFQLYQyBYC2UIgWwhkC4FsIZAtBLKFQLYQyBYC2ULEPs9Ok66uLpNrampM3tjYCDkdCahsIZAtRNEu4z09PSY3NjaazGU8eahsIZAtBLKFCNazr66uTN7d3TV5bGws1FRkobKFQLYQyBYiWM8uKeHv6je9vb0mr66u5re7u7vN2Pn5eWLnxYAQyBYC2UKk2rNbW1vz27W1tWmeKlP4/5/Mz8+bXF9fb3Jhn06yR7+ZV2pHhsyBbCGQLUSqPXtwcDC/XVZWluapMkVdXZ3Jk5OTJh8fH5scRR+ujJEoVLYQyBYC2UKk2rObm5s/HDs5OUnz1EVlaWkpdvzo6Mjks7OzFGfzBypbCGQLgWwhEu3Zy8vLJo+Pj+e3+/v7zdjOzk6Sp84UlZWVseOXl5eBZmKhsoVAthDIFiLYO2jV1dWhTpU6/rN5/77ZH+/s7Ex7Sp+CyhYC2UIgW4hEe/bLy4vJhV8DXFhYMGPT09Mmt7e3xx67ra3NZP8ZcF9fn8mzs7Pxky2goqLC5MfHx9j9/WP7Pfri4sLk+/t7k29vbz89tyShsoVAthCJXsanpqZMvrm5yW//7fbj8PAwdrzwtWTn3l7GX19fTW5oaDC5tLQ0vz0yMmLG/Fd/JyYmTB4dHTV5YGAgdq4zMzMmPz09xe4fCipbCGQLgWwhoriPpYf8Yt/W1tY/7b+5uWny4uKiyc/Pzyafnp7mt/f3983YwcGByXNzcyb7+/urM5aXl5vc0tJicqjXjpzji33wC2QLgWwhMtOzs4R/j/7w8GDy3d2dyVVVVSYX3tOHhp4NzjlkS4FsIYq2BHWWKfypsXNvH89eX1+b/FWWEKGyhUC2EMgWgp79Dv5ykz7r6+uBZpIsVLYQyBYC2ULQs/+Dr/oZKipbCGQLgWwhkC0EsoVAthDIFoL7bOfc2tqaycPDwyYXLvHlnHNNTU0m7+3tpTOxhKGyhUC2EMgWgp79Dv679P7vt1dWVgLOJjmobCGQLQSyhaBnf4KOjg6T6dmQeZAtBJfxdwj1Bb3QUNlCIFsIZAtBz3bObW9vmzw0NFSkmaQLlS0EsoVAthAsjfXNYGkscM4hWwpkC4FsIZAtBLKFQLYQsffZ8L2gsoVAthDIFgLZQiBbCGQL8RPr9LqLi+dD0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADzElEQVR4nO3dvS9kURzG8XtFJBoSCgWJKNChUVAgoRANCpWXRuJfkGi1/gCFiIJCIUqFSuMlXqLxnmhUOonESyKZrXayv7M7h+HcO3fn+X6q++Rk7xz75OzZ687ciXO5XAQNFaWeANJD2UIoWwhlC6FsIZQtpNI3GMcx12X/mVwuFxcaY2ULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQrz3s/E1g4ODJm9sbJjc399v8s3NTeJz+hdWthDKFsI/4wF0d3ebfHx8XKKZ+LGyhVC2EMoWwp79DRUVdo20tLSY3NzcbHIcF3x3b6pY2UIoWwhlC4l9T14o9uM/fX19JtfX1+ePt7e3i5xadjU2Npr88PBg8vr6uskzMzOJz+k3Pv6DKIooWwplCwl6nT0wMGBya2tr/ric9uyVlRXv+N3dXUozKQ4rWwhlC6FsIUH3bPd68uDgIOTpM6O2ttY7vru7m9JMisPKFkLZQihbSNA9273PW67Gx8dNPj8/N3l/f99k9/62+7v0tGi0gyiKKFsKZQsJumePjY2Z7LvOnp6eDvnSqVpaWjK5oaHB5NvbW5Ofn58Tn9NXsLKFULYQyhYSdM8eGRkxubq6OuTpM2NiYsI7vri4aPLT01OCs/k6VrYQyhZC2UKC7tnt7e0Fxy4uLkK+VFA1NTUmDw8Pmzw1NWXy+/u7yZWV9q/x9PQ04OzCYWULoWwhqX1kN/SjJ7q6ukweGhoyuampyeSqqqr88eTkpBlzb82+vr6afHR0ZPLb25vJ7iXm9fV1gVmXFitbCGULoWwhqe3ZdXV1P/rznZ2dJp+dnZn88fFh8svLi8mXl5f549XVVTN2cnJi8t7ensmPj48mu5de9/f3haadKaxsIZQthLKFBN2z3evTPx/hsby8bMYWFhaKOndHR4fJs7OzJl9dXZl8eHhY1Pl95ubmvONbW1vBXitJrGwhlC2EsoUEfTSWa35+Pn/c29v7k1P9ZXR0NOj5fDY3N73j7tuU3J815P8fPsOjsRBFEWVLoWwhie7Z5crdw9mzkTmULYSyhfC1EQG4XwvR1tZmcpp7tg8rWwhlC6FsIezZAbi/q8jqI8KyOSskgrKFULYQ9uwE9PT0mLy2tlaaiThY2UIoWwhlC2HPDiArX5n8GVa2EMoWQtlC2LO/YWdnx+TPHm+ZFaxsIZQthLcSlxneSowoiihbCmULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQthDv/WyUF1a2EMoWQtlCKFsIZQuhbCG/AL/lxA6qHEzUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADrklEQVR4nO3dvyt9cRzH8XMkZSEkKSUDNiYDA4oku4lsKP+GSZlNBgOj2AwmE8LodylisClFSl3byfvz5Xy59/M5516v52M6r07O+eTVx8fh3HPiQqEQQUNV3gNAdihbCGULoWwhlC2EsoVUp+2M45jrsgpTKBTi7/Yxs4VQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQtpDU25JQnJGREZM3NjZMHhoaMvny8jL4mKKImS2FsoVQtpCga/bg4GCy3dTUZPZtbW2FPHWu+vr6TD46OsppJBYzWwhlC6FsIUHX7OHh4WS7s7PT7PtLa3ZVlZ0zHR0dJre3t5scx99+QicoZrYQyhZC2UKCrtkzMzPJ9v7+fshT5aq1tdXk2dlZk9fX102+uLgIPqavMLOFULYQyhYSdM12rz//qtXV1dT919fXGY0knUYbiKKIsqVQthCva3ZPT4/JLS0tPg9fturr61P37+7uZjSSdMxsIZQthLKFeF2zJyYmTK6trfV5+LLh/i7i/v/a9fDwEHI4P8bMFkLZQihbiNc1u7u7+9t9p6enPk+Vq+XlZZPdNfzq6srk5+fn4GP6CWa2EMoWQtlCMvt8drl83ukrdXV1Jo+Pj5s8PT1t8tjYWOrxFhcXTX56eip+cB4xs4VQtpDMfow3NjaW9PW9vb0mux+hGR0dNbmtrc3kmpqaZHtqasrsc2+fen19Nfnw8NDkt7c3k6ur7bfx5OQkKkfMbCGULYSyhcRpL0v/7Rv7VlZWTJ6fn0+23cuPu7u73xz6n1ue3DX7/f3d5JeXF5PPzs6SbXcNPj4+Nnlvb8/kx8dHk+/v701uaGgw+fPvB1njjX2IooiypVC2EK/X2QsLCybf3t4m2wMDAyUd213jt7e3TT4/Pzf54OCgpPN9Njc3Z3Jzc7PJNzc33s4VEjNbCGULoWwhQf82vrS0FPLwmXEfKe3a3NzMaCSlYWYLoWwhlC2E10Z4UCmP5mRmC6FsIZQthLKFULYQyhZC2UK4zi6Ce/9bV1eXyT7/l+4TM1sIZQuhbCGs2UVw77WvlEdtV8Yo4QVlC6FsIazZHvT395u8traWz0D+g5kthLKFULYQ1uwi5PVK5FIxs4VQthB+jP/Azs6OyZOTkzmNpDTMbCGULYSyhXh9NBbyx6OxEEURZUuhbCGULYSyhVC2EMoWQtlCKFsIZQuhbCGpfxvH38LMFkLZQihbCGULoWwhlC3kAw+zohFeAXUJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    out_aug = image_augmentation(img_tensor)\n",
    "    plot_image(out_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6005c",
   "metadata": {},
   "source": [
    "### 4.2 We can add the `image_augmentation` pipeline easily to the `preprocess` and rebuild `train_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b49e0712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"train_model\":\n",
       "\n",
       "   \u001b[32m   │└─────────────────────────────┐\n",
       "      │                              │\n",
       "      ▼ img                          ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor            \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                              │\n",
       "      ▼ args                         ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28)          \u001b[32m/\u001b[0m Identity()       \n",
       "   \u001b[32m   │                              │\n",
       "      ▼ args                         ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0m\u001b[32m[\u001b[0mimage_augmentation: ..\u001b[32m>>\u001b[0m..\u001b[32m]\u001b[0m \u001b[32m/\u001b[0m Identity()       \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mBatchify(dim=0)             \n",
       "   \u001b[32m   │└─────────────────────────────┐\n",
       "      │                              │\n",
       "      ▼ x                            ▼ args\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mSimpleNet()                  \u001b[32m/\u001b[0m type(torch.int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_with_augmentation = (\n",
    "    convert_to_tensor / convert_to_tensor\n",
    "    >> padl.same.reshape(-1, 28, 28) / padl.Identity()\n",
    "    >> image_augmentation / padl.Identity()\n",
    ")\n",
    "\n",
    "train_model = (\n",
    "    preprocess_with_augmentation\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.same.type(torch.long)\n",
    ")\n",
    "\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd544eca",
   "metadata": {},
   "source": [
    "### 4.3 Retraining the model with `image_augmentation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cffaae3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0; Step: 0; loss: 0.297992080450058\n",
      "Epoch:0; Step: 10; loss: 0.26691189408302307\n",
      "Epoch:0; Step: 20; loss: 0.3031817078590393\n",
      "Epoch:0; Step: 30; loss: 0.2840678095817566\n",
      "Epoch:0; Step: 40; loss: 0.2174217253923416\n",
      "Epoch:0; Step: 50; loss: 0.29895099997520447\n",
      "Epoch:0; Step: 60; loss: 0.23612485826015472\n",
      "Epoch:0; Step: 70; loss: 0.1967872977256775\n",
      "Epoch:0; Step: 80; loss: 0.19700726866722107\n",
      "Epoch:0; Step: 90; loss: 0.1855168491601944\n",
      "Epoch:0; Step: 100; loss: 0.20173048973083496\n",
      "Epoch:0; Step: 110; loss: 0.266267865896225\n",
      "Epoch:0; Step: 120; loss: 0.2561732530593872\n",
      "Epoch:0; Step: 130; loss: 0.20615340769290924\n",
      "Epoch:0; Step: 140; loss: 0.19320911169052124\n",
      "Epoch:0; Step: 150; loss: 0.19195593893527985\n",
      "Epoch:0; Step: 160; loss: 0.1940772980451584\n",
      "Epoch:0; Step: 170; loss: 0.14146924018859863\n",
      "Epoch:0; Step: 180; loss: 0.2638438045978546\n",
      "Epoch:0; Step: 190; loss: 0.10949670523405075\n",
      "Epoch:0; Step: 200; loss: 0.1676878184080124\n",
      "Epoch:0; Step: 210; loss: 0.1917532980442047\n",
      "Epoch:0; Step: 220; loss: 0.1770448088645935\n",
      "Epoch:0; Step: 230; loss: 0.05187014117836952\n",
      "Epoch:1; Step: 0; loss: 0.2013569176197052\n",
      "Epoch:1; Step: 10; loss: 0.1647728681564331\n",
      "Epoch:1; Step: 20; loss: 0.206532821059227\n",
      "Epoch:1; Step: 30; loss: 0.2356492131948471\n",
      "Epoch:1; Step: 40; loss: 0.18055224418640137\n",
      "Epoch:1; Step: 50; loss: 0.24503085017204285\n",
      "Epoch:1; Step: 60; loss: 0.16183550655841827\n",
      "Epoch:1; Step: 70; loss: 0.13277608156204224\n",
      "Epoch:1; Step: 80; loss: 0.1643599569797516\n",
      "Epoch:1; Step: 90; loss: 0.13084468245506287\n",
      "Epoch:1; Step: 100; loss: 0.15252256393432617\n",
      "Epoch:1; Step: 110; loss: 0.1686607003211975\n",
      "Epoch:1; Step: 120; loss: 0.24043047428131104\n",
      "Epoch:1; Step: 130; loss: 0.19375978410243988\n",
      "Epoch:1; Step: 140; loss: 0.1884770691394806\n",
      "Epoch:1; Step: 150; loss: 0.18303777277469635\n",
      "Epoch:1; Step: 160; loss: 0.14761728048324585\n",
      "Epoch:1; Step: 170; loss: 0.13505887985229492\n",
      "Epoch:1; Step: 180; loss: 0.2550538182258606\n",
      "Epoch:1; Step: 190; loss: 0.11397549510002136\n",
      "Epoch:1; Step: 200; loss: 0.17874032258987427\n",
      "Epoch:1; Step: 210; loss: 0.2102859616279602\n",
      "Epoch:1; Step: 220; loss: 0.15638814866542816\n",
      "Epoch:1; Step: 230; loss: 0.035996198654174805\n",
      "Epoch:2; Step: 0; loss: 0.15469597280025482\n",
      "Epoch:2; Step: 10; loss: 0.1573871374130249\n",
      "Epoch:2; Step: 20; loss: 0.1974383145570755\n",
      "Epoch:2; Step: 30; loss: 0.18735834956169128\n",
      "Epoch:2; Step: 40; loss: 0.17936943471431732\n",
      "Epoch:2; Step: 50; loss: 0.2709156274795532\n",
      "Epoch:2; Step: 60; loss: 0.16107088327407837\n",
      "Epoch:2; Step: 70; loss: 0.13762980699539185\n",
      "Epoch:2; Step: 80; loss: 0.1773320436477661\n",
      "Epoch:2; Step: 90; loss: 0.11420335620641708\n",
      "Epoch:2; Step: 100; loss: 0.14893615245819092\n",
      "Epoch:2; Step: 110; loss: 0.19945044815540314\n",
      "Epoch:2; Step: 120; loss: 0.1932283639907837\n",
      "Epoch:2; Step: 130; loss: 0.194364532828331\n",
      "Epoch:2; Step: 140; loss: 0.19938644766807556\n",
      "Epoch:2; Step: 150; loss: 0.15005609393119812\n",
      "Epoch:2; Step: 160; loss: 0.14573384821414948\n",
      "Epoch:2; Step: 170; loss: 0.127245232462883\n",
      "Epoch:2; Step: 180; loss: 0.21859516203403473\n",
      "Epoch:2; Step: 190; loss: 0.11264535784721375\n",
      "Epoch:2; Step: 200; loss: 0.1532493382692337\n",
      "Epoch:2; Step: 210; loss: 0.19411787390708923\n",
      "Epoch:2; Step: 220; loss: 0.16154716908931732\n",
      "Epoch:2; Step: 230; loss: 0.031887926161289215\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 3\n",
    "num_workers = 0\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(mnist_train_dataset, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3aeb9",
   "metadata": {},
   "source": [
    "### 4.4 Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cadda40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 0.9718\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(mnist_test_dataset, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ len(mnist_test_dataset)\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77148c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "padl_env",
   "language": "python",
   "name": "padl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
