{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fcbb10",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing `padl` and most importantly `transform` decorator used to change any `callable` to `padl.Transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd560eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "import padl\n",
    "from padl import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e86d14",
   "metadata": {},
   "source": [
    "## Kaggle Digit Recognizer dataset:\n",
    "Kaggle Digit Recognizer dataset is used in this notebook. It can be easily downloaded from the kaggle link below.\n",
    "\n",
    "https://www.kaggle.com/c/digit-recognizer\n",
    "\n",
    "Details on the structure of the data can be read from the link above. Important information on the data structure is given in exerpt below\n",
    "\n",
    "> The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "\n",
    "### 0. Reading `csv` files for training and testing\n",
    "Note: `test.csv` does not contain data label in kaggle dataset. It is inteded to be used for submission to kaggle competition. Here, we can use it for quick inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = 'mnist/train.csv'\n",
    "test_csv = 'mnist/test.csv'\n",
    "\n",
    "with open(train_csv) as f:\n",
    "    train_data = f.readlines()\n",
    "train_array = torch.tensor([list(map(int, line.split(','))) for line in train_data[1:]])\n",
    "\n",
    "\n",
    "with open(test_csv) as f:\n",
    "    test_data = f.readlines()\n",
    "test_array = torch.tensor([list(map(int, line.split(','))) for line in test_data[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd509bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data shape:', train_array.shape)\n",
    "print('Test data shape:', test_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5e057",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 1. Plot few images to check the data\n",
    "\n",
    "`load_image` is a normal python function that takes in an image tensor and uses `matplotlib.pyplot` to plot the image. With `@transform` decorator, we can easily convert it to `padl.transform`. This allows us to use `padl` functional api and build data pipeline easily and quickly. \n",
    "\n",
    "Quick recap to `padl` operators:\n",
    "- `>>`: Compose operator: $(f_1 >> f_2)(x) \\rightarrow f_2(f_1(x))$\n",
    "- `+`: Rollout operator: $(f_1 + f_2) (x) \\rightarrow (f_1(x), f_2(x))$\n",
    "- `/`: Parallel operator: $(f_1 / f_2)((x_1,x_2)) \\rightarrow (f_1(x_1), f_2(x_2))$\n",
    "- `-`: Name operator: Names a transform so that its output can be accesed by given name or the transform itself can be accessed by its name from the pipeline:  \n",
    "    - $((f_1 - \\text{'zulu'})+f_2)(x) \\rightarrow \\text{Namedtuple}(\\text{'zulu'}:f_1(x), \\text{'out_1'}:f_2(x))$\n",
    "    - $((f_1 - \\text{'zulu'})+f_2)[\\text{'zulu'}] = f_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def load_image(img_tensor):\n",
    "    fig= plt.figure(figsize=(2,2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img_tensor, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75656124",
   "metadata": {},
   "source": [
    "### 1.1 Building a simple ploting pipeline using `padl` operators\n",
    "\n",
    "Description of inbuilt transforms used.\n",
    "\n",
    "- `padl.this` is a self reflexive trasform that allows for a quick mutation of input. \n",
    "\n",
    "        Example: padl.this[0]([1,2,3]) = 1\n",
    "\n",
    "- `padl.Identity()` is a simple transform that does exactly as it sounds, passes the input on as it is. \n",
    "\n",
    "        Example: padl.Identity()([1,2,3]) = [1,2,3]\n",
    "\n",
    "\n",
    "\n",
    "Description of `transform` pipeline defined below.\n",
    "- `reshape_load`: Takes in a `Torch.tensor` of length of `784`. `tensor` is then reshaped to image size of `28x28` and is plotted using `load_image` transform defined above. \n",
    "- `plot_train_datapoint`: \n",
    "    - First step is a `rollout` that splits datapoint of length `785` to two tensors of size `1` and `784`\n",
    "    - Second step is a `parallel` that passes first output of earlier step as it is (`padl.Identity()`) and passes second output to `reshape_load`\n",
    "    - In second step, transforms are also named by `-`, so the components of output is also accesible by using transform name.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_load = (padl.this.reshape(28, 28) >> load_image)\n",
    "\n",
    "plot_train_datapoint = (\n",
    "    padl.this[0] + padl.this[1:]\n",
    "    >> (padl.Identity() - 'label')/ (reshape_load - 'image')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594cc416",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_train_datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    output = plot_train_datapoint(train_array[np.random.randint(len(train_array))])\n",
    "    print(f'Label : {output.label}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06535ca",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 2. Model\n",
    "We will build a simple `Unet` to classify `MNIST` handwritings. In the cell below, a simple pytorch net is defined with just one added decorator `@transform`. This is enough to wrap the pytorch model into `padl.Transform` and use it with other transform to build a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6434c1",
   "metadata": {},
   "source": [
    "### 2.1 Simple Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models.resnet \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "        \n",
    "@transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv 1\n",
    "        # size : input: 28x28x1 -> output : 26 x 26 x 32\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 2\n",
    "        # size : input: 26x26x32 -> output : 24 x 24 x 32\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 3\n",
    "        # size : input: 24x24x32 -> output : 12 x 12 x 32\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 4\n",
    "        # size : input : 12 x 12 x 32 -> output : 8 x 8 x 64\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Conv 5\n",
    "        # size : input: 8x8x64 -> output : 4 x 4 x 64 -> Linearize = 1024\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "        \n",
    "        # FC 1 \n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        \n",
    "        # FC 2\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2265f",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing\n",
    "\n",
    "We need to take the input of `train_array` which is of shape `785` and need to reshape it as an `28x28` image. \n",
    "The `preprocess` pipeline below, again splits the `tensor` into two tensor of size `784` for image and size `1` for labels. Image is then further  reshaped to `-1, 28, 28`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = (\n",
    "    padl.this.type(torch.FloatTensor)\n",
    "    >> padl.this[1:] + padl.this[0]\n",
    "    >> padl.this.reshape(-1, 28, 28) / padl.Identity()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec776202",
   "metadata": {},
   "source": [
    "### 2.3 Instantiating the network and loss function\n",
    "\n",
    "Initialising instances of `SimpleNet` and `loss` function. Loss function here is a wrapped `torch` negative log likelihood loss which is again wrapped easily with same `transform` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a699bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()\n",
    "loss_func = transform(F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616132d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25508e",
   "metadata": {},
   "source": [
    "### 2.4  Building the training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691e6e7",
   "metadata": {},
   "source": [
    "`train_model` is now composed (`>>`) with the transforms already defined.\n",
    "- preprocess: preprocessing transform defined above\n",
    "- Batchify: Batchify is a inbuilt `transform` that marks end of preprocess (dataloading) and that adds batch dimension to the inputs. Batchify also moves the input tensors to device specified for the model\n",
    "- simplenet: Instance of SimpleNet\n",
    "- padl.this: A self reflexive trasform that allows for a quick mutation of input.\n",
    "\n",
    "`train_model` is then sent to the intended device. It is by default in `cpu`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device to be used: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model = (\n",
    "    preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.this.type(torch.long)\n",
    ")\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d982878",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 3. Training and validating the `train_model`\n",
    "\n",
    "Training is not much different than the normal torch training steps, except dataloading and training is made even simplier by `train_apply`. It is one of the three inbuilt methods along with `infer_apply` and `eval_apply` that handles the stage context of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95e62b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 2\n",
    "num_workers = 4\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(train_array, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b68ed",
   "metadata": {},
   "source": [
    "### 3.1 Accuracy of the model\n",
    "\n",
    "We can quickly build a `validation_model` by adding a further step to `train_model` to get the number associated with the maximum confidence predicted by the model. \n",
    "\n",
    "\n",
    "Note: As we don't have a separate validation dataset with labels, we will have to use the same train data to `validate` the model in this example.\n",
    "\n",
    "\n",
    "First, lets look at the format of the prediction by `infer_apply`ing one of datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b39dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.infer_apply(train_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8105d54",
   "metadata": {},
   "source": [
    "`train_model` predicts a tensor of confidence associated for the 10 numbers, and the index associated with the maximum of these confidence is the prediction by the model. Thus, we can add another transform to the same `train_model` to get that index associated with maximum of the confidence. \n",
    "\n",
    "Note that the new `validation_model` is a new instance of Transform but it contains same objects as `train_model` with added two new `transform`s. All the transform objects that are already in `train_model` is in `device` as assigned above, but the main `validation_model` object itself will have by default `cpu` device assigned. Thus, to move it to (or assign it with) correct device, we have to again call `pd_to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_model = (\n",
    "    train_model\n",
    "    >> padl.transform(lambda x: x.max(1).indices) / padl.Identity()\n",
    ")\n",
    "\n",
    "# We need to send the validation_model to device again\n",
    "validation_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27736457",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(train_array, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ train_array.shape[0]\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f570",
   "metadata": {},
   "source": [
    "Not a bad accuracy of `~0.95` for a quick train model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fe164",
   "metadata": {},
   "source": [
    "### 3.2 Infer few images from `test.csv` or test data\n",
    "\n",
    "Although we do not have labels for images in test data, we can still infer and verify the predictions ourselves. For that, we can again use model object `simplenet` that we have trained by using `train_model` and now stack it with other `transform`s to build an infer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_infer_datapoint = reshape_load - 'image'\n",
    "\n",
    "valid_preprocess =(\n",
    "    padl.this.type(torch.FloatTensor)\n",
    "    >> padl.this.reshape(28, 28)\n",
    ")\n",
    "infer_model = (\n",
    "    valid_preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> padl.this.unsqueeze(1) \n",
    "    >> simplenet\n",
    "    >> padl.transform(lambda x: x.max(1).indices)\n",
    ")\n",
    "infer_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7d6ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    data_point = test_array[np.random.randint(len(test_array))]\n",
    "    plot_infer_datapoint(data_point)\n",
    "    print(f'Prediction: {infer_model.infer_apply(data_point).item()}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab222191",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 4. Using further image augmentation on training\n",
    "\n",
    "We can easily use some of the `torchvision.transforms` for image augmentation and add it to our preproccessing of image to help with training. Lets add a couple of augmentations to our training: `GaussianBlur` and `RandomRotation`\n",
    "We need to wrap the call to these `torchvision.transforms` with our `padl.transform` before instantiating them, and that is all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ba923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_blur = transform(T.GaussianBlur)(kernel_size=(3,3), sigma=0.1)\n",
    "\n",
    "rotate_img = transform(T.RandomRotation)(degrees=(-15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a0a2f",
   "metadata": {},
   "source": [
    "Now we can again use `padl`'s functional api to build an `image_augmentation` pipeline.\n",
    "\n",
    "Note: `torchvision.transforms` expect images with channels but our images are just in grayscale. So, we need to unsqueeze our image tensor here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_augmentation = (\n",
    "    padl.this.unsqueeze(0)\n",
    "    >> rotate_img\n",
    "    >> gaussian_blur\n",
    "    >> padl.this[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0dc1b",
   "metadata": {},
   "source": [
    "####  Sample of image augmentation\n",
    "\n",
    "Lets try the augmentation on one of image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209547d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = infer_model[:2](test_array[0])\n",
    "plot_infer_datapoint(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4d5f5",
   "metadata": {},
   "source": [
    "Augmented images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    out_aug = image_augmentation(out)\n",
    "    plot_infer_datapoint(out_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6005c",
   "metadata": {},
   "source": [
    "### 4.2 We can add the `image_augmentation` pipeline easily to the `preprocess` and rebuild `train_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_with_augmentation = (\n",
    "    padl.this.type(torch.FloatTensor)\n",
    "    >> padl.this[1:] + padl.this[0]\n",
    "    >> padl.this.reshape(-1, 28, 28) / padl.Identity()\n",
    "    >> image_augmentation / padl.Identity()\n",
    ")\n",
    "\n",
    "\n",
    "train_model = (\n",
    "    preprocess_with_augmentation\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.this.type(torch.long)\n",
    ")\n",
    "\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd544eca",
   "metadata": {},
   "source": [
    "### 4.3 Retraining the model with `image_augmentation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffaae3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 3\n",
    "num_workers = 4\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(train_array, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3aeb9",
   "metadata": {},
   "source": [
    "### 4.4 Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(train_array, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ train_array.shape[0]\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenen_env",
   "language": "python",
   "name": "tenen_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
