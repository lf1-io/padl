{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b96b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may also need to install torchvision and matplotlib\n",
    "# !pip install matplotlib\n",
    "# !pip install torchvision==0.11.0\n",
    "\n",
    "# These might be useful if there are errors regarding ipywidgets while downloading torchvision.datasets\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fcbb10",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing `padl` and most importantly `transform` decorator used to change any `callable` to `padl.Transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528c0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd560eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import padl\n",
    "from padl import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e86d14",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "MNIST dataset available through torchvision is used in this notebook. The dataset can be separately downloaded from MNIST website or can be loaded as given below. \n",
    "\n",
    "More details on torchvision's MNIST dataset can be found here: https://pytorch.org/vision/stable/datasets.html#mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef381508",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_dataset = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "mnist_test_dataset = torchvision.datasets.MNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5e057",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 1. Plot few images to check the data\n",
    "\n",
    "`plot_image` is a normal standard function that takes in an image tensor and uses `matplotlib.pyplot` to plot the image. With `@transform` decorator, we can easily convert it to `padl.transform`. This allows us to use `padl` functional api and build data pipeline easily and quickly. \n",
    "\n",
    "Quick recap to `padl` operators:\n",
    "- `>>`: Compose operator: $(f_1 >> f_2)(x) \\rightarrow f_2(f_1(x))$\n",
    "- `+`: Rollout operator: $(f_1 + f_2) (x) \\rightarrow (f_1(x), f_2(x))$\n",
    "- `/`: Parallel operator: $(f_1 / f_2)((x_1,x_2)) \\rightarrow (f_1(x_1), f_2(x_2))$\n",
    "- `-`: Name operator: Names a transform so that its output can be accesed by given name or the transform itself can be accessed by its name from the pipeline:  \n",
    "    - $((f_1 - \\text{'zulu'})+f_2)(x) \\rightarrow \\text{Namedtuple}(\\text{'zulu'}:f_1(x), \\text{'out_1'}:f_2(x))$\n",
    "    - $((f_1 - \\text{'zulu'})+f_2)[\\text{'zulu'}] = f_1$\n",
    "    \n",
    "Note that outputs of `Rollout` and `Parallel` are tuples.\n",
    "\n",
    "For more details on operators and building compound transforms, refer to the documentation: https://lf1-io.github.io/padl/gettingstarted.html#defining-compound-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a60c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def plot_image(img_tensor):\n",
    "    fig= plt.figure(figsize=(2,2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img_tensor, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75656124",
   "metadata": {},
   "source": [
    "### 1.1 Building a simple ploting pipeline using `padl` operators\n",
    "\n",
    "Description of inbuilt transforms used.\n",
    "\n",
    "- `padl.this` is a self reflexive trasform that allows for a quick mutation of input. \n",
    "\n",
    "        Example: padl.this[0]([1,2,3]) = 1\n",
    "\n",
    "- `padl.Identity()` is a simple transform that does exactly as it sounds, passes the input on as it is. \n",
    "\n",
    "        Example: padl.Identity()([1,2,3]) = [1,2,3]\n",
    "\n",
    "\n",
    "\n",
    "Description of `transform` pipeline defined below.\n",
    "- `convert_plot`: Takes in a `PIL.Image` that is converted to numpy array, and then is plotted using `plot_image` transform defined above. \n",
    "- `plot_datapoint`: \n",
    "    - It is a `parrallel` that passes first part of input to convert_plot and passes second part of input as it is with `padl.Identity`\n",
    "    - transforms are also named by `-`, so the output is a named tuple, with elements named as the transform name.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012b6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def img_to_array(img):\n",
    "    return np.asarray(img)\n",
    "\n",
    "convert_plot = (\n",
    "    img_to_array\n",
    "    >> plot_image\n",
    ")\n",
    "\n",
    "plot_datapoint = (convert_plot - 'image')/ (padl.Identity() - 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594cc416",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEdElEQVR4nO2dSyhtURzG17lkQB6ZUEpiQEgmKClJkmLgMVFGZEQZmZgZkPIYiIGRMpGhx4SB10ApeUyUOZl5v8O5s935Vvfug/Owz/m+32h9rfbZq36t87eOvfby+f1+Izj489sDENFDsomQbCIkmwjJJkKyiUh06/T5fFqXxRh+v9/3vz7NbCIkmwjJJkKyiZBsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjJJkKyiZBsIiSbCNfHkmKZhIQEyOnp6V++tr+/H3JycjLkwsJCyH19fZAnJiYgd3Z2Qn55eYE8NjbmtIeHh788zu+imU2EZBMh2UR4tmbn5uZCTkpKglxdXQ25pqYGckZGBuT29vawje38/Bzy9PQ05NbWVsj39/eQT05OIO/s7IRtbG5oZhMh2URINhE+tzcvRHP7T3l5OeTNzU3I31knh5vPz0/I3d3dkB8eHlyvv7y8hHx9fQ357OwshNEh2v4jjDGSTYVkE+GZmp2ZmQl5f38fcn5+ftjuZX/2zc0N5Lq6Oshvb2+Qf/Pvh2CoZgtjjGRTIdlEeOa38aurK8iDg4OQm5ubIR8dHUG2f5+2OT4+dtoNDQ3Q9/j4CLmkpATywMCA62fHCprZREg2EZJNhGfW2cFIS0uDbP+PeG5uDnJPTw/krq4up724uBjm0XkHrbOFMUayqZBsIjyzzg7G3d2da//t7a1rf29vr9NeWlqCPvv/1fGKZjYRkk2EZBMRM+vsYKSkpEBeXV2FXFtb67Sbmpqgb2NjI3IDizJaZwtjjGRTIdlExE3NtikoKIB8eHjotO1nzra2tiAfHBxAnp2dhezlUw5Vs4UxRrKpiNuvcZvAbbTz8/PQl5qa6nrt0NAQ5IWFBcj29p7fRF/jwhgj2VRINhE0NTuQ0tJSyFNTU5Dr6+tdr7cfgRoZGYF8cXERwuhCQzVbGGMkmwrJJoKyZtvYr9FqaWmBbK/LfT4si/YrQeztRdFENVsYYySbCskmQjX7C7y+vkJOTMQnsN/f3yE3NjZC3t7ejsi4/oVqtjDGSDYVkk1EzGz/CSdlZWWQOzo6IFdUVEC2a7TN6ekp5N3d3RBGFzk0s4mQbCIkm4i4rdn2cUyBxze1tbVBX3Z29rc+++PjA7L9DJpXtwBrZhMh2URINhExW7PtOmsfgWgfsZiXl/fje9nbgexnzlZWVn782dFEM5sIySZCsonwbM3OysqCXFxcDHlmZgZyUVHRj+9lHyMxPj4OeXl5GbJX19HB0MwmQrKJkGwifq1m20c72fun7OMaQz3qaW9vz2lPTk5C3/r6OuTn5+eQ7uVVNLOJkGwiJJuIiNbsqqoqp20f3VRZWQk5JycnpHs9PT1Bto9+Gh0dddr20U4saGYTIdlERPRrPPB1VIHtr2A/nru2tgbZ3nJjL6fstxgKzWwqJJsIySZCW3bjDG3ZFcYYyaZCsomQbCIkmwjJJkKyiZBsIiSbCMkmQrKJcP1tXMQXmtlESDYRkk2EZBMh2URINhF/AVStNDfc/xylAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = plot_datapoint(mnist_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c394d4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76f0ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAENklEQVR4nO2dPSh1YQDHnyu3RAaRlEkZxKLkozCIWNlIkjIoZDApZZFFUTKIQRbEIClKFot8xEQpX4skiRCj+25v/k+c1/EeznP9/7/Jr3M/nvp57uN8ODcSi8WM4CAh7AGIn0OxiVBsIhSbCMUmQrGJSPTaGIlEtF8WZ8RischH2zSziVBsIhSbCMUmQrGJUGwiFJsIxSZCsYlQbCIUmwjFJkKxiVBsIhSbCMUmQrGJUGwiPC9LimeysrLAq6urP3xsNBoFn56eBr+/vwevqakBPzg4+MoQfxzNbCIUmwjFJiLi9V+cLl9KnJiIf250dHSANzc3g5eUlHz4Ws/Pz+CRCF6Nm5ycDH5xcQFeV1cHfnZ29uF7fTe6lFgYYxSbCsUmIm73s3t7e8GHhoY+/dyJiQnwyclJ8JSUFPD5+XnwnJwc8NTU1E+/d5hoZhOh2EQoNhFxs2a3tbWB9/f3ez7+5eUFfHBw8O/Pw8PDsO319dXztU5OTsCzs7PBBwYGwBsaGjxfLyw0s4lQbCIUmwhn1+y0tDTwpqYmcPt4tU1fXx/4+Ph4MAN7h9zc3G977SDRzCZCsYlQbCKcPZ9dVVUFvrGx4ev59r7w9fX1l8dSVFQEvru7C26f366oqAjsvf2i89nCGKPYVCg2Ec7uZ789lv0ZVlZWwJ+engIby9XVled2+/z2wsICeH19Pfjd3V0g4/KLZjYRik2EYhPh7Jrtl83NTXD7WvD/4ebmBry9vR18dHQUvLy8HLy0tBR8bW0tsLH5QTObCMUmQrGJcObYuH0sfHZ2FjwzMxPcvgbN3s8+PDwMcHTe7O/vgxcWFoJvbW2BV1ZWfttYdGxcGGMUmwpndr3y8/PB7Y9tm6WlJfDj4+PAxxQUe3t7YQ/BGKOZTYViE6HYRDizZv9mxsbGwh6CMUYzmwrFJkKxiVBsIhSbCMUmQrGJcGY/e3l5Gdy+PaV97Fz4RzObCMUmQrGJcGbNvry8BH98fAxpJL8XzWwiFJsIxSbCmTW7trYW3OXbTaWnp4MnJSWFNBJ/aGYTodhEKDYRzqzZ6+vr4Kenp+AZGRk/ORzA/lqIubk58Ly8vJ8czpfRzCZCsYlQbCKcWbNdZmpqCtz+eubV1VXw7u5ucPu4f1hoZhOh2EQoNhHOrtkzMzPgZWVl4K2treD210T4IRqNgo+MjIAXFBSAd3V1gS8uLoLf3t5+eSzfiWY2EYpNhGIT4cytsWwSEvD3sKWlBdy+haRNZ2cn+Ntr2oqLi2FbT08P+MPDA3hjYyP49va253uHiW6NJYwxik2Fsx/j/+Lo6Ajcz2lG+9t77NtgnJ+fg+/s7PgcXXjoY1wYYxSbCsUmIm7XbPE+WrOFMUaxqVBsIhSbCMUmQrGJUGwiFJsIxSZCsYlQbCI8j42L34VmNhGKTYRiE6HYRCg2EYpNxB+7e/FW5tAR7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEYElEQVR4nO2dPSh9fxzHv9dTeQoDJgMWpRCL1UgZyEA2g6IYPAyU0WYwmDyGlFAMyEMp2aSYkUQZKVEi7n+7eX/qHg//e+/vHO/3azrvvvceX736nk8f53uOUDgcdoKDpH89AZE4JJsIySZCsomQbCIkm4gUr8FQKKS+LGCEw+FQtDGtbCIkmwjJJkKyiZBsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjJJsLzfvZfpaysDPLk5CTkmpoayJWVlZAvLy/jM7E4o5VNhGQTQXMZb2pqihzPzMzAWG5urud3e3t7IW9vb0Pe2dn5f5NLEFrZREg2EZJNRMjrKc4gbyWempqC3NHRETkOhaLutv0VdXV1kA8PD2N6/p+grcTCOSfZVEg2EYHtsz/3zc45Nzw8DLm6uhry5zr9/PwMY3t7e5BtH23PXVxcDDkpKRhrJhizFDFBsomQbCJ8W7Pz8vIgT0xMQG5tbYVse+enpyfIc3NzkeOxsTEYu729hWxvgWZmZkJ+e3uD/PDw4IKAVjYRkk2EZBPh25pdVVUFua2tzfPzu7u7kDs7OyHbuvyZ9PR0yMvLy5ALCgogd3d3Qz47O/Ocm1/QyiZCsomQbCJ8W7Ptdt2+vj7Itk4eHR1Bfn9/j3puW6MXFhYgV1RUQF5aWoI8Ozsb9dx+RiubCMkmQrKJ+LN70LwYHR2FPDQ0BHl9fR1yT08P5Lu7u/hMLAZoD5pwzkk2FZJNxJ+t2baXnp6ejhy3tLTA2OvrK2T7iO7V1VWMZxc/VLOFc06yqZBsIv5Mza6trYVsn/UqLy//9rlubm4g2z1ma2trkMfHxyHbfemJRDVbOOckm4rAXsbtNqX5+XnIKSnR795eX19Dvr+/h2xbsfz8fMilpaWQLy4uINfX10NOZOumy7hwzkk2FZJNhG+3JX1FWloaZNsubW1tQV5dXY0cn5ycwNjLy4vnz0pNTYU8MDAAeWRkBPLg4CDkrq4uz/MnCq1sIiSbCMkmIrB9tp84ODiAbN9qnJOTk7C5qM8WzjnJpkKyiQhsn/0vycjIgFxUVAT5/Pw8kdP5NlrZREg2EZJNhGr2L+jv74ds72+3t7cncjrfRiubCMkmQrKJoKnZn+9/20eDkpOTPb9rX4VlX/lh97Cdnp7+ZopxRyubCMkmQrKJ8E3Nzs7Ohtzc3AzZ7gNrbGyEbOuwpbCwMHJcUlICY/YV019xfHwMuaGhAbKt4X5BK5sIySZCsonwzR60x8dHyFlZWT/6vv23EV6/11ff/fj4gGwf0bX7wP1Uo7UHTTjnJJsKySbCN332T2psrNnc3IS8uLgIeWVlJZHTiRta2URINhGSTYRv+mz7HpKNjQ3I9m/jFvv6qv39/aiftX2zzUFGfbZwzkk2Fb65jIvYoMu4cM5JNhWSTYRkEyHZREg2EZJNhGQTIdlESDYRkk2EZBMh2URINhGSTYTn/Wzxt9DKJkKyiZBsIiSbCMkmQrKJ+A+nwB7iOlF7agAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEsElEQVR4nO2dvSt9cRzHP0cWlAgDk8FCsmASg8Ug5CGjQrFQJhKjDBZuKU+liJIweCoGefgbRInBYrBgMLq/7ebz6XfPcZz7cK73+zWdd+eeh7z63s/9nu/5fjnRaFQIBlnpvgGSOigbCMoGgrKBoGwgKBuIbLedjuOwX5ZhRKNRJ94+tmwgKBsIygaCsoGgbCAoGwjKBoKygaBsICgbCMoGgrKBoGwgKBsIygaCsoGgbCAoGwjX15L+KuXl5Sr39/erPD09rXJWlm4TX19fKi8vL6t8f3+vsuPoN4U2NjZUfn9/d7/hBMGWDQRlA0HZQDhuszjD/CpxSUmJynV1da6fj0Qise2cnBy1r7S01PVYW3P9zny1x29ubqpsfzMEga8SExGhbCgoG4jQ9rNtTW5vb1d5aGhI5draWtfzfa+b6V5tori4OC3XZcsGgrKBoGwgQlOzbY0+OztTuaamJmHXmpmZUbm7u1vlysrKhF3rfxwcHCT1/PFgywaCsoGgbCBCU7NtP9pvjT48PFR5YWFB5evr67jH1tfXq7yysqLy4uKiyrOzsyqPj4+73ltvb6/K+/v7rp9PFmzZQFA2EJQNRGjGs09OTlRuaWlx/fzS0pLKExMTKn9+fv742nY82x5bWFio8s3Njcr5+fkq298HbW1tP76XoHA8m4gIZUNB2UCEpp/tl+3tbZX91GjLy8uLyvY5/dHRkcq2xj88PKicyhrtB7ZsICgbCMoGIjQ1e29vT2Wvfrat2a2trSrb+VZ+mJqaUrmqqkrl29tblefm5n59rVTClg0EZQNB2UCE5tm47duen5+r7Hd8244Z2zHl7wwODqq8urqqsp2fXVFRofLj46Ove0smfDZORISyoQjN17jFTpG5uLhQ2XaHvHh6eopt227eyMiIyrm5uSo/Pz+r3NDQoLJ93JpO+DVORISyoaBsIEJbsy19fX0qT05OqlxWVqZyXl6eykGm7FZXV6sc5FFssmHNJiJC2VBQNhAZU7O9sMtLra2tqRykZmdnh2Yk2BPWbCIilA0FZQOROcXIAzsMaZeQ/L7fLiHtxeXlpcodHR0qp2oJ6aCwZQNB2UBQNhAZ28+2rzEdHx+rbJe3/F7DbQ22Y+P23Lb+n56eqjwwMKDy6+trnLtOPuxnExGhbCgoG4iMqdkFBQUq2+k/XtOF1tfXY9tjY2NqX1NTk8pbW1sq22U27N/MTunt7Ox0vZdkwppNRISyoaBsIDKmZo+Ojqo8Pz/v63g/Y9J2KUx7bfs3e3t7U7mrq0tlt6U0Ew1rNhERyoaCsoEIbc22z6N3d3dVtnXRi+bm5tj21dWVr2Pts/TGxkbXz/uZLpxoWLOJiFA2FJQNRGhrdk9Pj8o7OzuBzvfx8RHb9jvebJ/LFxUV+To+le+ds2YTEaFsKCgbiNDWbNvPtv9uaXh4+Nfn8zvXy96L1/H2N4FdsjqZsGYTEaFsKEI7/cd+VdpVB+/u7nydLxKJBL6neNivba9XpNIFWzYQlA0EZQMR2q4X+R3sehERoWwoKBsIygaCsoGgbCAoGwjKBoKygaBsICgbCNdn4+RvwZYNBGUDQdlAUDYQlA0EZQPxD9poVi3Oz/ZbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC9UlEQVR4nO3dvy40URiA8Xe+KDQKjYhSdNQaJK5AoVWJ6CWiI4q9BRQqjUQjbkHhCjRoVHqhUtmvkGy8GybxZ4/ZeZ5ftW+mOfLk5GRnx27V7XZDDP/+egEqx9ggxgYxNoixQYwNMlJ3saoq35cNmW63W312zZ0NYmwQY4MYG8TYIMYGMTaIsUGMDWJsEGODGBvE2CDGBjE2SO3n2Xqzvb2d5p2dnTRPTk6WXM63ubNBjA1S1f1HiI8lvbm+vk7zy8tLmufn50sup5aPJSkijI1ibBDfen1gY2MjzTMzM2nudDoll/Nr3NkgxgYxNohn9gdWVlbS3H8v4vDwsORyfo07G8TYIMYG8cyOiNHR0TSPj4+n+fT0NM3Pz88DX9MguLNBjA1ibBDP7IiYmppK88LCQppvb29LLmdg3NkgxgYxNohndkQcHx//6PqwcGeDGBvE2CA+Nx4Rr6+vab65uUnz4uJimh8fHwe+pu/yuXFFhLFRjA2CfJ89PT2d5qrKx9zBwUGam3xGf4U7G8TYIMYGQZ7Ze3t7ae6/1/Dw8FByOcW4s0GMDYK5XTo3N9d7fXV1la5dXFykeX19vciaBsHbpYoIY6MYGwTz1uv9LdKxsbF0bVi/NuOr3NkgxgYxNgjmzF5bW/v02v39fcGV/B13NoixQYwN0toze3NzM82rq6u910dHR6WX0wjubBBjgxgbpLVndv9POTw9PfVen5+fl15OI7izQYwNYmyQ1pzZIyP5T+n/F5/3z51dXl6WWFLjuLNBjA1ibJDWnNlLS0tpXl5eTvP+/n7B1TSTOxvE2CDGBmnNmb27u1t7/e7urtBKmsudDWJsEGODtObMnp2drb3+/uuuJiYm0rVh/bnFr3JngxgbxNggrTmzt7a20tz/k4onJye912dnZwVW1DzubBBjg2C+GovCr8ZSRBgbxdggxgYxNoixQYwNYmwQY4MYG8TYIMYGMTaIsUGMDVL7ebbaxZ0NYmwQY4MYG8TYIMYG+Q/M7YZ2zDu8OwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Label : 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAACmUlEQVR4nO3cQXLaQBBA0VYq9wJOBpwMOJmy8cLqmGlkbGmk/99OZYdQ9WtoDwwaxnEMMfxZ+wloOcYGMTaIsUGMDWJskL+tHw7D4L5sY8ZxHJ79zJUNYmwQY4MYG8TYIMYGMTaIsUGMDWJsEGODGBvE2CDGBjE2iLFBjA1ibBBjgxgbxNggxgZpHiXesuPxOLk+n89Pf1a5Xq+T68vl8s1ntS5XNoixQYwNspuZnefo5xmd3e/3yXU1w1uP9dX/3StXNoixQYwNspmZnefq7XabXOc53Nob58c6nU7Nx567L88zPD+3fL0UVzaIsUGMDTK07nDY02028hzN8txd0ty/J37zuXqbDUWEsVGMDdLtPrva6+Z99JJan5W/8vtrcWWDGBvE2CDdzOw81/J13quu+Rly9VyzYXi69V2UKxvE2CDGBul2ZmePx2OZJ/KCal+95nsALa5sEGODGBukm5ldORwOk+tqH95Svbf97nvZvZ4jd2WDGBukm2NJ1dGeueZ8xacaAdXL+pLHjioeS1JEGBvF2CDdzOxszldw35Xf3qy2eVkvH2FGOLP1wdggxgbpdmZXqrck89z9/BFp9W+rvxd6vlWWM1sRYWwUY4Ns5iPO7DfnZJ73e+HKBjE2iLFBNjuzf1Ke/9WRp7VubfUuVzaIsUGMDeLM/gZntrpnbBBjgzizo34vvKevC7/DlQ1ibBBjg2z2DNo75n6vrKdz4RXPoCkijI1ibBDkPruXW0IvzZUNYmwQY4MgZ/Zc+YxaT9/tmsOVDWJsEF/GX7DVl+3MlQ1ibBBjgziz4/+jwXs5hpS5skGMDWJsEOSxpD3zWJIiwtgoxgYxNoixQYwNYmyQ5j5b++LKBjE2iLFBjA1ibBBjg/wDHvq1LiYNn1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Plotting some of the train datapoints\n",
    "\n",
    "for _ in range(5):\n",
    "    output = plot_datapoint(mnist_train_dataset[np.random.randint(len(mnist_train_dataset))])\n",
    "    print(f'Label : {output.label}')\n",
    "    # showing image\n",
    "    plt.show()\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06535ca",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 2. Model\n",
    "We will build a simple `Unet` to classify `MNIST` handwritings. In the cell below, a simple pytorch net is defined with just one added decorator `@transform`. This is enough to wrap the pytorch model into `padl.Transform` and use it with other transform to build a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6434c1",
   "metadata": {},
   "source": [
    "### 2.1 Simple Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ad5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "        \n",
    "@transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv 1\n",
    "        # size : input: 28x28x1 -> output : 26 x 26 x 32\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 2\n",
    "        # size : input: 26x26x32 -> output : 24 x 24 x 32\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 3\n",
    "        # size : input: 24x24x32 -> output : 12 x 12 x 32\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv 4\n",
    "        # size : input : 12 x 12 x 32 -> output : 8 x 8 x 64\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Conv 5\n",
    "        # size : input: 8x8x64 -> output : 4 x 4 x 64 -> Linearize = 1024\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "        \n",
    "        # FC 1 \n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        \n",
    "        # FC 2\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2265f",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing\n",
    "\n",
    "The `preprocess` pipeline below, again splits the datapoint to two different pipelines for `PIL.Image` and `int` label. First part of parralel pipeline, converts the image into torch tensor of type float, and second part passes the label as it is. The label later will be used for the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb38956",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def convert_to_tensor(img):\n",
    "    arr = np.asarray(img)\n",
    "    return torch.tensor(arr).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "preprocess = (\n",
    "    convert_to_tensor / convert_to_tensor\n",
    "    >> padl.this.reshape(-1, 28, 28) / padl.Identity()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec776202",
   "metadata": {},
   "source": [
    "### 2.3 Instantiating the network and loss function\n",
    "\n",
    "Initialising instances of `SimpleNet` and `loss` function. Loss function here is a wrapped `torch` negative log likelihood loss which is again wrapped easily with same `transform` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a699bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()\n",
    "loss_func = transform(F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bce20bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mSimpleNet()\u001b[0m:\n",
       "\n",
       "   SimpleNet(\n",
       "     (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "     (batchnorm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "     (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "     (batchnorm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "     (batchnorm4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv5): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "     (batchnorm5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv5_drop): Dropout2d(p=0.5, inplace=False)\n",
       "     (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "     (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "   )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "616132d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mnll_loss\u001b[0m:\n",
       "\n",
       "   def nll_loss(\n",
       "       input: Tensor,\n",
       "       target: Tensor,\n",
       "       weight: Optional[Tensor] = None,\n",
       "       size_average: Optional[bool] = None,\n",
       "       ignore_index: int = -100,\n",
       "       reduce: Optional[bool] = None,\n",
       "       reduction: str = \"mean\",\n",
       "   ) -> Tensor:\n",
       "       r\"\"\"The negative log likelihood loss.\n",
       "\n",
       "       See :class:`~torch.nn.NLLLoss` for details.\n",
       "\n",
       "       Args:\n",
       "           input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
       "               in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n",
       "               in the case of K-dimensional loss. `input` is expected to be log-probabilities.\n",
       "           target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
       "               or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
       "               K-dimensional loss.\n",
       "           weight (Tensor, optional): a manual rescaling weight given to each\n",
       "               class. If given, has to be a Tensor of size `C`\n",
       "           size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "               the losses are averaged over each loss element in the batch. Note that for\n",
       "               some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
       "               is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "               when reduce is ``False``. Default: ``True``\n",
       "           ignore_index (int, optional): Specifies a target value that is ignored\n",
       "               and does not contribute to the input gradient. When :attr:`size_average` is\n",
       "               ``True``, the loss is averaged over non-ignored targets. Default: -100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25508e",
   "metadata": {},
   "source": [
    "### 2.4  Building the training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691e6e7",
   "metadata": {},
   "source": [
    "`train_model` is now composed (`>>`) with the transforms already defined.\n",
    "- preprocess: preprocessing transform defined above\n",
    "- Batchify: Batchify is a inbuilt `transform` that marks end of preprocess (dataloading) and that adds batch dimension to the inputs. Batchify also moves the input tensors to device specified for the model\n",
    "- simplenet: Instance of SimpleNet\n",
    "- padl.this: A self reflexive trasform that allows for a quick mutation of input.\n",
    "\n",
    "`train_model` is then sent to the intended device. It is by default in `cpu`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8134bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to be used:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device to be used: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c73d5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m:\n",
       "\n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ img                 ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor   \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                     │\n",
       "      ▼ args                ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28) \u001b[32m/\u001b[0m padl.Identity()  \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)    \n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ x                   ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mSimpleNet()         \u001b[32m/\u001b[0m type(torch.int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model = (\n",
    "    preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.this.type(torch.long)\n",
    ")\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d982878",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 3. Training and validating the `train_model`\n",
    "\n",
    "Training is not much different than the normal torch training steps, except dataloading and training is made even simplier by `train_apply`. It is one of the three inbuilt methods along with `infer_apply` and `eval_apply` that handles the stage context of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f95e62b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0; Step: 0; loss: 2.3085103034973145\n",
      "Epoch:0; Step: 10; loss: 2.054443359375\n",
      "Epoch:0; Step: 20; loss: 1.706283688545227\n",
      "Epoch:0; Step: 30; loss: 1.4256861209869385\n",
      "Epoch:0; Step: 40; loss: 1.1271275281906128\n",
      "Epoch:0; Step: 50; loss: 1.0487807989120483\n",
      "Epoch:0; Step: 60; loss: 0.7397987842559814\n",
      "Epoch:0; Step: 70; loss: 0.6409561634063721\n",
      "Epoch:0; Step: 80; loss: 0.6292180418968201\n",
      "Epoch:0; Step: 90; loss: 0.5615631341934204\n",
      "Epoch:0; Step: 100; loss: 0.47212377190589905\n",
      "Epoch:0; Step: 110; loss: 0.5419679880142212\n",
      "Epoch:0; Step: 120; loss: 0.5239319205284119\n",
      "Epoch:0; Step: 130; loss: 0.4683115482330322\n",
      "Epoch:0; Step: 140; loss: 0.4360449016094208\n",
      "Epoch:0; Step: 150; loss: 0.4210889935493469\n",
      "Epoch:0; Step: 160; loss: 0.37823283672332764\n",
      "Epoch:0; Step: 170; loss: 0.316006064414978\n",
      "Epoch:0; Step: 180; loss: 0.40278348326683044\n",
      "Epoch:0; Step: 190; loss: 0.27032557129859924\n",
      "Epoch:0; Step: 200; loss: 0.32978716492652893\n",
      "Epoch:0; Step: 210; loss: 0.34087681770324707\n",
      "Epoch:0; Step: 220; loss: 0.31426024436950684\n",
      "Epoch:0; Step: 230; loss: 0.12959186732769012\n",
      "Epoch:1; Step: 0; loss: 0.31286484003067017\n",
      "Epoch:1; Step: 10; loss: 0.2880234122276306\n",
      "Epoch:1; Step: 20; loss: 0.338689923286438\n",
      "Epoch:1; Step: 30; loss: 0.34207677841186523\n",
      "Epoch:1; Step: 40; loss: 0.2654731869697571\n",
      "Epoch:1; Step: 50; loss: 0.3607393503189087\n",
      "Epoch:1; Step: 60; loss: 0.26549816131591797\n",
      "Epoch:1; Step: 70; loss: 0.2484167218208313\n",
      "Epoch:1; Step: 80; loss: 0.285735547542572\n",
      "Epoch:1; Step: 90; loss: 0.26167428493499756\n",
      "Epoch:1; Step: 100; loss: 0.2671133577823639\n",
      "Epoch:1; Step: 110; loss: 0.33493226766586304\n",
      "Epoch:1; Step: 120; loss: 0.3299217224121094\n",
      "Epoch:1; Step: 130; loss: 0.29857489466667175\n",
      "Epoch:1; Step: 140; loss: 0.2915211021900177\n",
      "Epoch:1; Step: 150; loss: 0.3016079366207123\n",
      "Epoch:1; Step: 160; loss: 0.25527042150497437\n",
      "Epoch:1; Step: 170; loss: 0.23224493861198425\n",
      "Epoch:1; Step: 180; loss: 0.3431799113750458\n",
      "Epoch:1; Step: 190; loss: 0.20803454518318176\n",
      "Epoch:1; Step: 200; loss: 0.24521981179714203\n",
      "Epoch:1; Step: 210; loss: 0.2634449303150177\n",
      "Epoch:1; Step: 220; loss: 0.24938784539699554\n",
      "Epoch:1; Step: 230; loss: 0.09175731986761093\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 2\n",
    "num_workers = 0\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(mnist_train_dataset, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b68ed",
   "metadata": {},
   "source": [
    "### 3.1 Accuracy of the model\n",
    "\n",
    "We can quickly build a `validation_model` by adding a further step to `train_model` to get the number associated with the maximum confidence predicted by the model. \n",
    "\n",
    "\n",
    "Note: As we don't have a separate validation dataset with labels, we will have to use the same train data to `validate` the model in this example.\n",
    "\n",
    "\n",
    "First, lets look at the format of the prediction by `infer_apply`ing one of datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b39dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m:\n",
       "\n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ img                 ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor   \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                     │\n",
       "      ▼ args                ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28) \u001b[32m/\u001b[0m padl.Identity()  \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)    \n",
       "   \u001b[32m   │└────────────────────┐\n",
       "      │                     │\n",
       "      ▼ x                   ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mSimpleNet()         \u001b[32m/\u001b[0m type(torch.int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "556e1b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.2247, -7.3218, -5.8183, -1.4469, -8.6301, -0.3001, -7.0715, -6.6834,\n",
       "          -4.1835, -7.0959]]),\n",
       " tensor([5]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.infer_apply(mnist_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8105d54",
   "metadata": {},
   "source": [
    "`train_model` predicts a tensor of confidence associated for the 10 numbers, and the index associated with the maximum of these confidence is the prediction by the model. Thus, we can add another transform to the same `train_model` to get that index associated with maximum of the confidence. \n",
    "\n",
    "Note that the new `validation_model` is a new instance of Transform but it contains same objects as `train_model` with added two new `transform`s. All the transform objects that are already in `train_model` is in `device` as assigned above, but the main `validation_model` object itself will have by default `cpu` device assigned. Thus, to move it to (or assign it with) correct device, we have to again call `pd_to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27a6b4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m:\n",
       "\n",
       "   \u001b[32m   │└───────────────────────────┐\n",
       "      │                            │\n",
       "      ▼ img                        ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor          \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                            │\n",
       "      ▼ args                       ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28)        \u001b[32m/\u001b[0m padl.Identity()  \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)           \n",
       "   \u001b[32m   │└───────────────────────────┐\n",
       "      │                            │\n",
       "      ▼ x                          ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mSimpleNet()                \u001b[32m/\u001b[0m type(torch.int64)\n",
       "   \u001b[32m   │                            │\n",
       "      ▼ x                          ▼ args\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mlambda x: x.max(1).indices \u001b[32m/\u001b[0m padl.Identity()  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_model = (\n",
    "    train_model\n",
    "    >> padl.transform(lambda x: x.max(1).indices) / padl.Identity()\n",
    ")\n",
    "\n",
    "# We need to send the validation_model to device again\n",
    "validation_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27736457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 0.949\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy over the test dataset\n",
    "\n",
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(mnist_test_dataset, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ len(mnist_test_dataset)\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f570",
   "metadata": {},
   "source": [
    "Not a bad accuracy of `~0.95` for a quick train model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fe164",
   "metadata": {},
   "source": [
    "### 3.2 Infer few images from test data\n",
    "\n",
    "Although we do not have labels for images in test data, we can still infer and verify the predictions ourselves. For that, we can again use model object `simplenet` that we have trained by using `train_model` and now stack it with other `transform`s to build an infer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "278c177b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m:\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m0: \u001b[0m__getitem__(0)            \n",
       "   \u001b[32m   │\n",
       "      ▼ img\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mconvert_to_tensor         \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mBatchify(dim=0)           \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0munsqueeze(1)              \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mSimpleNet()               \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m5: \u001b[0mlambda x: x.max(1).indices"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_preprocess =(\n",
    "    padl.this[0]\n",
    "    >> convert_to_tensor\n",
    ")\n",
    "infer_model = (\n",
    "    infer_preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> padl.this.unsqueeze(1) \n",
    "    >> simplenet\n",
    "    >> padl.transform(lambda x: x.max(1).indices)\n",
    ")\n",
    "\n",
    "infer_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dca7d6ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEB0lEQVR4nO2cvS80URSHz7xRic5HSUmiRIdWdIQSvXI7H9EJCvEH6BQ6QUspGo2OpaUStJT7Nm82e47dmey7H7Pr9zzV/DLF3vXk7nHm3jtJqVQy0OBP3gOA9oFsIZAtBLKFQLYQyBaiJ+1mkiT0ZV1GqVRKat1jZguBbCGQLQSyhUC2EMgWAtlCIFsIZAuBbCGQLQSyhUC2EMgWAtlCIFsIZAuBbCGQLQSyhUC2EMgWAtlCIFsIZAuBbCFSj/90E2NjYy7Pz8+7PDo6Wr6enZ2tec/MLEn8CZpisejy+vq6yzc3N/UNNieY2UIgWwhkC5GkvS2pk4/sbm9vu7yxseFyb2+vy5XfM9bk+DfIuv/6+uryx8eHy6urqy4/PT1Zu+DILpgZsqVAthAdW7NXVlZcPjk5cbneunt+fl6+vr6+rmssi4uLLs/NzaV+9v39vctTU1N1fV4jULPBzJAtBbKF6JiaHevi2dmZy1k1+fHx0eWDgwOXLy4uytdfX1//Pc5qY1tYWHD5+/vb5bW1tZpjaTbUbDAzZEuBbCFyW88uFAouHx4euhxrcmR5ednlVtbByOXlpcvx/42+vj6X43P6vGBmC4FsIZAtRG599tvbm8v9/f0uf35+uhzXiOt9vt1MYg2+u7tzeWhoyOX4bPzl5aU1AzP6bPgHsoVAthC59dmxJg8MDLgc14TzrNFxT3rl2rjZz33nsQ+Pe9TygpktBLKFyO1nPP70xRawnY8/48/00tKSy/VsUzYzm56edjmWqFa2Xmkws4VAthDIFiK3mp21hLm7u+tyrJOnp6cuv7+/uzwyMuLyzMxM+Xpra8vdyzqym7UlKhJrcl41OsLMFgLZQiBbiNyWOLO24zZ6bHZ4eNjlyiXURo/sZt3f2dlxeX9/39oFS5xgZsiWAtlC5FazBwcHXd7c3HS5si82+/l8OfbR8XvEIz6Vr7qI9T0+h499cTwuHMceP3t8fLzmZ7caajaYGbKlQLYQHXNkN4tYs2MfHUmr2VlMTEy4HLcKZ/XZPT35vSWUmg1mhmwpkC1E17yCOvbGrdyeG/voWKNjPj4+btlYmgkzWwhkC4FsIbqmZreTuLae9iyim2BmC4FsIZAtBDW7CnEPelaffXt72/IxNQNmthDIFgLZQnTNenY7eXh4cDmeBXt+fnY5vvqq0VdcNwLr2WBmyJYC2ULQZ1chq6++urpyOc8aXQ/MbCGQLQQ/41WI7WjMWUeTOuWNhhFmthDIFgLZQlCzqxBbrfgG5b29PZc7tUZHmNlCIFsIZAtBza7C5OSky0dHRy4Xi8V2DqdpMLOFQLYQyBaCbUm/DLYlgZkhWwpkC5Fas+F3wcwWAtlCIFsIZAuBbCGQLcRf+C1NmG9XdMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 9\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEKUlEQVR4nO2dzSt0YRjGr/OmRJRY+IiyYKMoJGVFohQLOyUrFvwHkizYUxY2NqTsGBtZWCilbFjIxsdKREopSlbz7qa5n/edx9eZc8Zc1291rp5znDu/nrlnzjnPTJBMJiE4+BN3ASI6JJsIySZCsomQbCIkm4gC32AQBPpc9stIJpNBpjHNbCIkmwjJJkKyiZBsIiSbCMkmQrKJkGwiJJsIySbCe22clcLCQpO7urpMnpubM7m9vd3k2dlZk5eXl0Os7vtoZhMh2URINhHq2QCmp6dN7u3tNbmnp8d7/M3NjclNTU3hFBYymtlESDYRlC/jOzs7Jg8NDZnsrpK5vb01eWVlxeSNjQ2T7+/vf1hhdtDMJkKyiZBsImh69uDgYGp7YGDAjL2+vpo8Pz9v8tramslPT0/ec7mXT09OTj5bZlbRzCZCsomQbCIC3zcv/OblP/X19SYfHR2ltquqqszY5uamyWNjY1mrK9to+Y8AINlUSDYRefs52+3LlZWVqW33fcrp6WkkNcWNZjYRkk2EZBORtz37KzQ2NsZdQiRoZhMh2URINhF527MfHh5MTn8urLq62oxNTExk3BcAFhYWQq4uHjSziZBsIiSbiLy9n+2S/gza9va2GSsosG9d7u7uTHafWTs/Pw+5uvDQ/WwBQLKpkGwiaHp2OsfHxyZ3dHSY7P5PDg4OTE7v/wDw/v4eYnU/Qz1bAJBsKiSbCMqe7V4bd3tyQ0OD9/i+vj7v8XGini0ASDYVlC/jLqOjoyavr69793eX+JaVlYVd0rfRy7gAINlUSDYR6tkASktLTT48PDS5paXFe7zbs19eXkKp6zuoZwsAkk2FZBORt48SfwW3x15eXprc3NzsPd59FHlpaSmcwkJGM5sIySZCsomIrGfX1dWZPDU1ZfLMzExUpYTO1dVV3CV8Cs1sIiSbCMkmIqvXxtO/inlvb8+MlZeXm7y6umqy29OzSU1NjckXFxcmFxUVeY93lw/Fia6NCwCSTYVkExFZs3HvGQeBbS2Tk5Mmu0tshoeHTb6+vjb5+fnZe/7i4uLUdkVFhRlLJBIZ9/0fu7u73vFcRTObCMkmQrKJiOwZNLcnLy4umuz+Gr2vLuDf69FnZ2fe/Wtra1PbnZ2dZsx9/+Ce2/1ajf7+fpMfHx+9544Sfc4WACSbCskmIrbnxtva2kx2f27po2WzLh/13a8cu7W1ZfL4+LjJcT4X/hHq2QKAZFMh2UTkzFovd71USUmJySMjIyan/3QTAHR3d5vc2tqa8VzuvXP3Xvv+/r7Jb29vGf9WrqGeLQBINhWSTUTO9GwRDurZAoBkUyHZREg2EZJNhGQTIdlESDYRkk2EZBMh2URINhGSTYRkEyHZREg2EZJNhGQT4X0sSeQXmtlESDYRkk2EZBMh2URINhF/AcTdEOPIGnUwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 3\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEJ0lEQVR4nO2dPSh1YQDHn/Pmq5B8lAWDFEkWg2IgH4kMFjFRYpJsFosyGCwyYTBJyUdkEZFVEYPPQaw+FoMoH/fdbu//qXu673U/nuv//03n19E5p349Huee81xeIBAwgoM/ib4AET8UmwjFJkKxiVBsIhSbiBS/nZ7n6b4syQgEAl6ofRrZRCg2EYpNhGITodhEKDYRik2EYhOh2EQoNhGKTYRiE6HYRCg2EYpNhGITodhEKDYRik2EYhOh2EQoNhG+rxK7RENDA3hPTw94a2sreFlZGfjh4WFwe3NzE/bNzs6C/9aVrRrZRCg2EYpNhOc3PyVy+U99fT341tYWeG5ubtTOtbOzAz4wMAD+8PAQtXPFGi3/EcYYxaZCsYlwds7e29sDb2pqAv/+/gZfWFgAv7u7Ax8aGgpuFxcXw7709HTwi4sL8NraWvC3t7dQl51wNGcLY4xiU6HYRDgzZxcVFYGfn5+DZ2dng/f29oKvrq6Gfa6+vj7wxcVFcM/Daa+/vx98aWkp7HPFG83Zwhij2FQoNhHOzNnb29vgHR0d4Pv7++Dt7e3gX19fEZ97fn4efHBwEPzm5ga8srIy4nPFGs3Zwhij2FQoNhHOvIOWkZHhu//j4wP8J3O0/b5aS0uL788/Pj5GfC6X0MgmQrGJUGwinJmzT09Pwe3n16WlpeA5OTngLy8vvsevrq4Obi8vL8O+vLw88LOzM/Du7m7fYycLGtlEKDYRik2EM5+N22u5dnd3wVNS8M+LiYkJ8MnJSd/jV1RUBLft++ynpyfwg4MDcPs+u6qqCjw/Px/88vLS9/ixRJ+NC2OMYlPhzK9xm5mZGfCRkRHw9/d3cPvVofX1dfCfLMO1H6eurKyAZ2Zmgq+trYHby4tjiX6NC2OMYlOh2EQ483Gpzfj4OLi93Gd0dBTcnkfteXN6ejq4fXV1BfteX1/BOzs7fY+VmpoK/vn5Cf7vV3q4hEY2EYpNhGIT4ex9to39SHNjYwO8sbEx7GNdX1+D23N2eXk5eFZWlu/x5ubmwIeHh8O+lmij+2xhjFFsKhSbCGfvs23s147sx5RtbW3gXV1d4HV1dSGPbT+yvL29BX9+fgYvKSkBPzo6Cnlsl9DIJkKxiVBsIpLmPjuW1NTUgJ+cnIBPTU2Bj42NgdtzemFhYRSv7v/QfbYwxig2FYpNhObsMLC/grqgoABcc7ZwDsUmQrGJSJrPxhNJWlqa7/77+/v4XMgP0cgmQrGJUGwiNGdHAftfRbmKRjYRik2EYhOh2EQoNhGKTYRiE6HYRCg2EYpNhGITodhEKDYRik2EHnGGgf0ffJubm8GPj4/jeTkRo5FNhGITodhEaPnPL0PLf4QxRrGpUGwiFJsIxSZCsYlQbCJ877PF70IjmwjFJkKxiVBsIhSbCMUm4i9zTx2T/n1MRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 9\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAACnUlEQVR4nO3dXZLaMBAA4XEq9wJOBpwMOJnzClMbqbySbZnu740i2Z/qErOyjZnmeQ4x/Nn7B9B2jA1ibBBjgxgbxNggf0tPTtPkvuxg5nme/vecKxvE2CDGBjE2iLFBjA1ibBBjgxgbxNggxgYxNoixQYwNYmyQ4vlsqnx59fP5/Hh8v9+Lz4/KlQ1ibBBfxiPidrsVnz+fz8XHl8vl4/GoL+uubBBjgxgbZCq9i/NbLyXOM/p6vXb9+nvOcC8lVkQYG8XYIJh99pI7TNRmbN5n154fZd/tygYxNoixQb52Zj8ej1//3zxzp+lz65q/dv73p9Pp1997Ta5sEGODGBvksDM7z8l8fLu2Fy7Jlx1lr9er+L1qj/fad7uyQYwNYmyQw57P7nk35TxD8/no1p8l/w1Qu+atheezFRHGRjE2yLAze819dO9rxJb+/ZCPtffkzFZEGBvF2CDDHhuvHV9eas05eRSubBBjgxgbZJh9dp7JLdeQRWw7o91nazjGBhlm69X6sr30tOSe8inONU95vnNlgxgbxNggu83s1jl1lNtR/cRLibU6Y4MYG2Szw6VHPhyatf4uHi7V6owNYmyQzfbZrZcV7Xnsu3VGj3IMwJUNYmwQY4MMcz47G+kTdpbeorr1LcBrcWWDGBvE2CDDzuwt9T5uX7u11l5c2SDGBjE2CGZmv8/l1lt2jLqPrnFlgxgbxNggw16Dludivu1zTf7ohpbz6Uea0V6DpogwNoqxQXa7zcbaH2vcYstbRvfmzFZEGBvF2CDDvj+79wx/3yuPdH3bllzZIMYGGeYOh+rDrZciwtgoxgYxNoixQYwNYmwQY4MYG8TYIMYGMTaIsUGMDWJskOL5bH0XVzaIsUGMDWJsEGODGBvkH6jm1rEHH5H8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAENklEQVR4nO2dzSttURjG33N9pETKTAYywMhHGZmKZOAjHwMDTIyMDJSJsYko/AWSEqUIGWBmJoSUgUgdJUWMjM4d3JJn3Xu3s8/nOp7nNzpP++yzVn6t/dp7r712JBaLmeDgV7Y7IDKHZBMh2URINhGSTYRkE5EftDESiei8LMeIxWKR/23TyCZCsomQbCIkmwjJJkKyiZBsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjJJkKyiQicluQz1dXVkFtbW+Ped3x8HHJDQwPkqakpyOvr65Bvb2/jbssnNLKJkGwiJJuISNBTnD5PJZ6YmIA8OzubtrYuLi4gd3Z2Qo5Go2lrOyyaSizMTLKpkGwicqZm9/b2Ql5dXYVcWFiYsb6cnp5Cdmv409NTxvriopotzEyyqZBsIryt2TU1NZAvLy8h5+Xlpa3t4+NjyC0tLYHfPzw8hNzW1pbyPsWLarYwM8mmQrKJ8PZ+diSCpSdsjX5+foY8Ojr6+fn9/T1w34KCAsibm5uQS0pKIJeWlgbmt7e3wPYyhUY2EZJNhGQT4W3NHhgYSGr/hYUFyHt7ewn/1v7+PuT+/n7Izc3NkLu6uiCvrKwk3HYq0cgmQrKJkGwivLk2XlVVBXlnZwdyXV1d4P4PDw+Qa2trIX98fCTct7W1NchuzXZ5fX2FXF5ennDbYdG1cWFmkk2FZBPhzXl2T08P5O9qtMvc3BzkZGp0shQVFWWt7SA0somQbCIkmwhvavbMzEyo77tz0ra2tlLZHcC9t56raGQTIdlEeHMYD8v19TXku7u7tLX1U15Oq5FNhGQTIdlE5GzNTieNjY2Qv3v8x2VpaSmFvUkdGtlESDYRkk2EavY/+PqokJlZRUVFqP3dpbR8QSObCMkmQrKJ8KZmX11dQW5qaspST/6eIvUdLy8vkB8fH1PYm9ShkU2EZBMh2UR4U7P7+vog7+7uQg47tTgM7tTfsNOQ3PPqg4ODpPuUDjSyiZBsIiSbCG9q9v39PWR3aSsX95Fc95HfoDlpxcXFkDc2NiBXVlYGtp2raGQTIdlESDYR3tRsl+npachHR0eQ6+vrIbtLTnZ3d0P+ek96cnIStrW3tyfcTzOzm5ubpPbPFBrZREg2EZJNhDdLY7m4r41wrz/n5wf/u3F+fg7563l52GUw3L/R9vY25JGREcjZXHJaS2MJM5NsKiSbCG9rtos7L2xsbAxyR0dH2trO5vKUYVHNFmYm2VRINhE5U7NdysrKIC8uLkIeGhqK+7ei0Shk9zUR8/PzkN057j6hmi3MTLKpyNnDuIv7Fr3BwUHIw8PDn5+Xl5dh29nZGeSTk5PUdi6D6DAuzEyyqZBsIn5MzRZ/UM0WZibZVEg2EZJNhGQTIdlESDYRkk2EZBMh2URINhGB18bFz0IjmwjJJkKyiZBsIiSbCMkm4jdhDQNbt78sIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    data_point = mnist_test_dataset[np.random.randint(len(mnist_test_dataset))]\n",
    "    output = plot_datapoint(data_point)\n",
    "    plt.show()\n",
    "    print(f'Prediction: {infer_model.infer_apply(data_point).item()}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab222191",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid\"> </hr>\n",
    "\n",
    "\n",
    "### 4. Using further image augmentation on training\n",
    "\n",
    "We can easily use some of the `torchvision.transforms` for image augmentation and add it to our preproccessing of image to help with training. Lets add a couple of augmentations to our training: `GaussianBlur` and `RandomRotation`\n",
    "We need to wrap the call to these `torchvision.transforms` with our `padl.transform` before instantiating them, and that is all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "543ba923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f645938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_blur = transform(T.GaussianBlur)(kernel_size=(3,3), sigma=0.1)\n",
    "\n",
    "rotate_img = transform(T.RandomRotation)(degrees=(-15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a0a2f",
   "metadata": {},
   "source": [
    "Now we can again use `padl`'s functional api to build an `image_augmentation` pipeline.\n",
    "\n",
    "Note: `torchvision.transforms` expect images with channels but our images are just in grayscale. So, we need to unsqueeze our image tensor here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f39aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_augmentation = (\n",
    "    padl.this.unsqueeze(0)\n",
    "    >> rotate_img\n",
    "    >> gaussian_blur\n",
    "    >> padl.this[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0dc1b",
   "metadata": {},
   "source": [
    "####  Sample of image augmentation\n",
    "\n",
    "Lets try the augmentation on one of image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "209547d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADrklEQVR4nO3dvyt9cRzH8XMkZSEkKSUDNiYDA4oku4lsKP+GSZlNBgOj2AwmE8LodylisClFSl3byfvz5Xy59/M5516v52M6r07O+eTVx8fh3HPiQqEQQUNV3gNAdihbCGULoWwhlC2EsoVUp+2M45jrsgpTKBTi7/Yxs4VQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQtpDU25JQnJGREZM3NjZMHhoaMvny8jL4mKKImS2FsoVQtpCga/bg4GCy3dTUZPZtbW2FPHWu+vr6TD46OsppJBYzWwhlC6FsIUHX7OHh4WS7s7PT7PtLa3ZVlZ0zHR0dJre3t5scx99+QicoZrYQyhZC2UKCrtkzMzPJ9v7+fshT5aq1tdXk2dlZk9fX102+uLgIPqavMLOFULYQyhYSdM12rz//qtXV1dT919fXGY0knUYbiKKIsqVQthCva3ZPT4/JLS0tPg9fturr61P37+7uZjSSdMxsIZQthLKFeF2zJyYmTK6trfV5+LLh/i7i/v/a9fDwEHI4P8bMFkLZQihbiNc1u7u7+9t9p6enPk+Vq+XlZZPdNfzq6srk5+fn4GP6CWa2EMoWQtlCMvt8drl83ukrdXV1Jo+Pj5s8PT1t8tjYWOrxFhcXTX56eip+cB4xs4VQtpDMfow3NjaW9PW9vb0mux+hGR0dNbmtrc3kmpqaZHtqasrsc2+fen19Nfnw8NDkt7c3k6ur7bfx5OQkKkfMbCGULYSyhcRpL0v/7Rv7VlZWTJ6fn0+23cuPu7u73xz6n1ue3DX7/f3d5JeXF5PPzs6SbXcNPj4+Nnlvb8/kx8dHk+/v701uaGgw+fPvB1njjX2IooiypVC2EK/X2QsLCybf3t4m2wMDAyUd213jt7e3TT4/Pzf54OCgpPN9Njc3Z3Jzc7PJNzc33s4VEjNbCGULoWwhQf82vrS0FPLwmXEfKe3a3NzMaCSlYWYLoWwhlC2E10Z4UCmP5mRmC6FsIZQthLKFULYQyhZC2UK4zi6Ce/9bV1eXyT7/l+4TM1sIZQuhbCGs2UVw77WvlEdtV8Yo4QVlC6FsIazZHvT395u8traWz0D+g5kthLKFULYQ1uwi5PVK5FIxs4VQthB+jP/Azs6OyZOTkzmNpDTMbCGULYSyhXh9NBbyx6OxEEURZUuhbCGULYSyhVC2EMoWQtlCKFsIZQuhbCGpfxvH38LMFkLZQihbCGULoWwhlC3kAw+zohFeAXUJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapoint = mnist_train_dataset[2]\n",
    "\n",
    "# plot datapoint \n",
    "plot_datapoint(datapoint)\n",
    "\n",
    "# convert datapoint's image to tensor\n",
    "img_tensor = convert_to_tensor(datapoint[0])\n",
    "\n",
    "# tensor shape\n",
    "print('tensor shape:', img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4d5f5",
   "metadata": {},
   "source": [
    "Augmented images: Running image augmentation on same image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05fa5c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADxklEQVR4nO3dPS8sYRjG8RkRiUhIKBQkokCnU1AgISIalcpLVAofQav1ARQiCgqFoFKoFIJ46RAkEm+FTiIhEsmebuN+jt0MO/PMrOv/q/bKyM5zzpV7H7s7dsNcLhdAQ0XaC4A/lC2EsoVQthDKFkLZQiqLHQzDkOdlZSaXy4WFjjHZQihbCGULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQihbSNHLkuLU29trckNDg8mbm5u+liKLyRZC2UIoW4i3Pbu/v9/ktrY2k//ynj0wMGDy2tqayX19ffnbV1dXia2DyRZC2UIoW4i3PXtqasrkg4MDX6dOXVdXl8nHx8eprIPJFkLZQihbSKJ79u3tbf72/v6+OTY5OZnkqVNVUWFnqLW11eSWlhaTw7DgX9nGiskWQtlCKFtIrHt2Z2enyY2NjXHefWY1NTWZ/PDwYPLq6qrJ7v+TL0y2EMoWQtlCYt2zR0ZGTK6uro7z7jNraWmp6PGbmxtPKymOyRZC2UIoW0ise3ZHR0fBY+fn53GeKlXu6wd1dXVFf353dzfJ5UTGZAuhbCGULcTbNWhpXXeVhIWFBZPd96tdT09PSS4nMiZbCGULoWwh3vbs+vp6X6f6sdraWpOHh4dNnpiYMHloaMjkqqoqk6+vr01+fX0tdYmxYLKFULaQWB/G39/fTf76pa6Li4vm2Nzc3I/ue3p62mT38tvBwUGTm5ubTf76UDs+Pm6OuZf+uv+Oo6Mjkz8+PgredxAEwfz8vMkvLy9BFjDZQihbCGULiXXPnp2dNfnu7i5/u6enp6T7Pjs7M9ndsz8/P01+e3sz+eLiIn97eXnZHDs5OTF5b2/P5OfnZ5MfHx9NrqmpMfn09DTIIiZbCGULoWwh4dfnwv8dzNC37G5vb5u8tbVl8uXlpcmHh4exnXtmZsZk9zUD9/cF93m3T3zLLoIgoGwplC3E21ucpRodHU3t3O7HUbru7+89raQ0TLYQyhZC2ULKZs/Oso2NjbSXEAmTLYSyhVC2EPbsb6yvr5s8NjZmsvvefJyvwyeJyRZC2UIoWwh7dgTu9W7t7e0ms2cjcyhbCGULYc+OwL1Oz/3bsHJRnqvGr1C2EMoWwp79C93d3SavrKyks5AfYrKFULYQyhbCnh2Br69PTBqTLYSyhfAw/o2dnR2T3cuSyhWTLYSyhVC2kLL5mA1Ew8dsIAgCypZC2UIoWwhlC6FsIZQthLKFULYQyhZC2UKKvjaOv4XJFkLZQihbCGULoWwhlC3kH+FisSgw3GDNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADrklEQVR4nO3dvyt9cRzH8XMkZSEkKSUDNiYDA4oku4lsKP+GSZlNBgOj2AwmE8LodylisClFSl3byfvz5Xy59/M5516v52M6r07O+eTVx8fh3HPiQqEQQUNV3gNAdihbCGULoWwhlC2EsoVUp+2M45jrsgpTKBTi7/Yxs4VQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQtpDU25JQnJGREZM3NjZMHhoaMvny8jL4mKKImS2FsoVQtpCga/bg4GCy3dTUZPZtbW2FPHWu+vr6TD46OsppJBYzWwhlC6FsIUHX7OHh4WS7s7PT7PtLa3ZVlZ0zHR0dJre3t5scx99+QicoZrYQyhZC2UKCrtkzMzPJ9v7+fshT5aq1tdXk2dlZk9fX102+uLgIPqavMLOFULYQyhYSdM12rz//qtXV1dT919fXGY0knUYbiKKIsqVQthCva3ZPT4/JLS0tPg9fturr61P37+7uZjSSdMxsIZQthLKFeF2zJyYmTK6trfV5+LLh/i7i/v/a9fDwEHI4P8bMFkLZQihbiNc1u7u7+9t9p6enPk+Vq+XlZZPdNfzq6srk5+fn4GP6CWa2EMoWQtlCMvt8drl83ukrdXV1Jo+Pj5s8PT1t8tjYWOrxFhcXTX56eip+cB4xs4VQtpDMfow3NjaW9PW9vb0mux+hGR0dNbmtrc3kmpqaZHtqasrsc2+fen19Nfnw8NDkt7c3k6ur7bfx5OQkKkfMbCGULYSyhcRpL0v/7Rv7VlZWTJ6fn0+23cuPu7u73xz6n1ue3DX7/f3d5JeXF5PPzs6SbXcNPj4+Nnlvb8/kx8dHk+/v701uaGgw+fPvB1njjX2IooiypVC2EK/X2QsLCybf3t4m2wMDAyUd213jt7e3TT4/Pzf54OCgpPN9Njc3Z3Jzc7PJNzc33s4VEjNbCGULoWwhQf82vrS0FPLwmXEfKe3a3NzMaCSlYWYLoWwhlC2E10Z4UCmP5mRmC6FsIZQthLKFULYQyhZC2UK4zi6Ce/9bV1eXyT7/l+4TM1sIZQuhbCGs2UVw77WvlEdtV8Yo4QVlC6FsIazZHvT395u8traWz0D+g5kthLKFULYQ1uwi5PVK5FIxs4VQthB+jP/Azs6OyZOTkzmNpDTMbCGULYSyhXh9NBbyx6OxEEURZUuhbCGULYSyhVC2EMoWQtlCKFsIZQuhbCGpfxvH38LMFkLZQihbCGULoWwhlC3kAw+zohFeAXUJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADv0lEQVR4nO2dPS80YRSGnxGRaAgKBZFsQnREoaBAQhCNigbR+QsShUbrBygUCgqFoFKoVMhqWV9RUegkEiKR7NttnCfMu2vnY9d9XdXceZg5XM6ezO6YCfL5vAMNatIuAJID2UIgWwhkC4FsIZAtRG3YYhAEnJdVGfl8Pvhpjc4WAtlCIFsIZAuBbCGQLQSyhUC2EMgWAtlCIFsIZAuBbCGQLQSyhUC2EMgWAtlChF6WFCVDQ0Mmt7S0mLy/v59UKbLQ2UIgWwhkC5HYzB4ZGTG5q6vL5L88s0dHR03e2dkxeXh4uLB9c3MTWx10thDIFgLZQsQ6s+/v7wvbp6enZm1hYSHOQ1cU/f39Jmez2VTqoLOFQLYQyBYi1pldU6Pxt9TW1mby6uqqyQMDAybPzs6aHOe59Vc0bIBzDtlSIFuISGd2T0+Pya2trVHuvmLZ3Nw0eWJiwuTt7W2Tr6+vY6/pO+hsIZAtBLKFiHRmT01NmVxfXx/l7iuWxsbG0PW7u7uEKgmHzhYC2UIgW4hIZ3Z3d/ePa5eXl1EeKlX89w8ymUzo1x8fH8dZTtHQ2UIgWwhkC5HYdeNpXXcVB+vr6yb/7zOAp6enOMspGjpbCGQLgWwhEpvZzc3NSR2qZBoaGkyenJw0eX5+3uTx8fHQ/d3e3pr8+vpaRnXRQWcLgWwhIn0Zf39/N/nrQ103NjbM2srKSui++vr6TO7t7TU5COxDbsbGxkxub283ua6urrA9Nzdn1vxLnv2f4/z83OSPjw+Ta2vtr3Ftbc3kl5cXVwnQ2UIgWwhkCxGEPSy93Cf2LS8vF7YHBwdL+t6Ojg6T/cuU/Zn9+flp8tvbm8lXV1eFbX8GX1xcmHxycmLy8/OzyY+PjyY3NTWF1prkpcM8sQ+cc8iWAtlCxDqzy+Hw8DB0/eDgwORcLmfy2dlZZLUsLS2Z7L9n8PDwYHJnZ2dkxy4VZjY455AtBbKFSOwjzlKZnp5Ou4QC/u0offb29hKqpDzobCGQLQSyhajYmV1NVMvts+lsIZAtBLKFQLYQyBYC2UIgWwjOs79hd3fXZP+W0YuLiyZH+dl5nNDZQiBbCGQLwcwuAv86vWp9HEZ1Vg2/AtlCIFsIZvYv8B/dtLW1lU4hJUJnC4FsIZAtBDO7CPz/Ba9W6GwhkC0EL+PfcHR0ZPLMzExKlUQLnS0EsoVAthAVe5sN+B3cZgOcc8iWAtlCIFsIZAuBbCGQLQSyhUC2EMgWAtlChL43Dn8LOlsIZAuBbCGQLQSyhUC2EP8AsvW0PyMKbgoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEA0lEQVR4nO2dvUorURSFd+QqBMGgIikiWImdYpFCCxXUCPbaqIli4TtY5AkEexUU1MJC1C4glqKiYOVvIIXxAYIWadTc5jK4zzVz482cmcS1vuoszsjs8LnnMJnMTKhUKgnBoCHoAoh/UDYQlA0EZQNB2UBQNhC/3CZDoRDPy+qMUqkUKjfHzgaCsoGgbCAoGwjKBoKygaBsICgbCMoGgrKBoGwgKBsIygaCsoGgbCBcr2fbZGhoSOX29naVDw4O/CwHAnY2EJQNRGCH8ZGREZW7u7tV5mHce9jZQFA2EJQNRGBrdjKZVPns7CygSnBgZwNB2UBQNhBW1+ze3l5nfHh4qOZOT09Vnpubs1lKTTE6Oqry7u6uMx4eHlZzDw8Pnu2XnQ0EZQNB2UBYXbMnJyedcTQatbmruiIej6t8eXnpy37Z2UBQNhCUDYRv342Hw2G/dhU4sVhM5XQ6rfLAwIDK09PTztjL82oTdjYQlA0EZQNhdc3u6ekpO3dzc2Nz14GysbGh8sTEhMo7Ozsq39/fW69JhJ0NBWUDQdlAeLpmb25uqpxKpZzx+Pi4mjs5OfFy13VFNpsNZL/sbCAoGwjKBsLTNbtYLJada2tr83JXNYV5rT4Sibhuf3x8bLOcsrCzgaBsICgbiJDbG/uqfW3E+/u7My4UCmru6enJ9W/7+/tV7uvrM2tTeWxsTOWVlZVKy5SWlhaVX15eXLff3t5WeWZmxnX7rq4ulfP5fMW1fRe+NoKICGVDYfUS5/LysjMeHBz81t9eX1+r/PlWIpG/D+Nvb28qd3Z2qtzU1OSMzcNuQ4P+n19cXFR5dnZW5UQiUa5sERF5fHxU+fX11XV7v2BnA0HZQFA2EFZPvarh6OhIZfOWX5O7uzuVM5mMyre3t8744uJCzV1dXam8urqqsrm9+XTG5uZmlT9f2hXRt+TahqdeREQoGwrKBqJm1+wgaWxsVPn5+dl1+9bWVpXN7wT8+qmwCNds8gfKBoKygQjscZa1zMLCgsodHR0q53I5levlESLsbCAoGwjKBoJr9heYj5s02d/f96kSb2FnA0HZQFA2EFyz/4N6fQ0VOxsIygaCsoHgmi0ie3t7Kk9NTQVUiV3Y2UBQNhCUDQTX7C8w7yMzfwd+fn7uZzmewc4GgrKBoGwguGZ/gflbevP+7XrlZ3wKUhGUDQRlA8E1uwLMVzNtbW0FU0iVsLOBoGwgIA/j8/PzKn9+Y56IyNramo/V+Ac7GwjKBoKygeBjNkTk4+ND5fX1dZWXlpb8LKcq+JgNIiKUDQVlAwF5nm3yUy5h/guMT0lEhLKhoGwgKBsIygaCsoGgbCBcvxsnPwt2NhCUDQRlA0HZQFA2EJQNxG/QzMNkybJhaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADrklEQVR4nO3dvyt9cRzH8XMkZSEkKSUDNiYDA4oku4lsKP+GSZlNBgOj2AwmE8LodylisClFSl3byfvz5Xy59/M5516v52M6r07O+eTVx8fh3HPiQqEQQUNV3gNAdihbCGULoWwhlC2EsoVUp+2M45jrsgpTKBTi7/Yxs4VQthDKFkLZQihbCGULoWwhlC2EsoVQthDKFkLZQihbCGULoWwhlC2EsoVQtpDU25JQnJGREZM3NjZMHhoaMvny8jL4mKKImS2FsoVQtpCga/bg4GCy3dTUZPZtbW2FPHWu+vr6TD46OsppJBYzWwhlC6FsIUHX7OHh4WS7s7PT7PtLa3ZVlZ0zHR0dJre3t5scx99+QicoZrYQyhZC2UKCrtkzMzPJ9v7+fshT5aq1tdXk2dlZk9fX102+uLgIPqavMLOFULYQyhYSdM12rz//qtXV1dT919fXGY0knUYbiKKIsqVQthCva3ZPT4/JLS0tPg9fturr61P37+7uZjSSdMxsIZQthLKFeF2zJyYmTK6trfV5+LLh/i7i/v/a9fDwEHI4P8bMFkLZQihbiNc1u7u7+9t9p6enPk+Vq+XlZZPdNfzq6srk5+fn4GP6CWa2EMoWQtlCMvt8drl83ukrdXV1Jo+Pj5s8PT1t8tjYWOrxFhcXTX56eip+cB4xs4VQtpDMfow3NjaW9PW9vb0mux+hGR0dNbmtrc3kmpqaZHtqasrsc2+fen19Nfnw8NDkt7c3k6ur7bfx5OQkKkfMbCGULYSyhcRpL0v/7Rv7VlZWTJ6fn0+23cuPu7u73xz6n1ue3DX7/f3d5JeXF5PPzs6SbXcNPj4+Nnlvb8/kx8dHk+/v701uaGgw+fPvB1njjX2IooiypVC2EK/X2QsLCybf3t4m2wMDAyUd213jt7e3TT4/Pzf54OCgpPN9Njc3Z3Jzc7PJNzc33s4VEjNbCGULoWwhQf82vrS0FPLwmXEfKe3a3NzMaCSlYWYLoWwhlC2E10Z4UCmP5mRmC6FsIZQthLKFULYQyhZC2UK4zi6Ce/9bV1eXyT7/l+4TM1sIZQuhbCGs2UVw77WvlEdtV8Yo4QVlC6FsIazZHvT395u8traWz0D+g5kthLKFULYQ1uwi5PVK5FIxs4VQthB+jP/Azs6OyZOTkzmNpDTMbCGULYSyhXh9NBbyx6OxEEURZUuhbCGULYSyhVC2EMoWQtlCKFsIZQuhbCGpfxvH38LMFkLZQihbCGULoWwhlC3kAw+zohFeAXUJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    out_aug = image_augmentation(img_tensor)\n",
    "    plot_image(out_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6005c",
   "metadata": {},
   "source": [
    "### 4.2 We can add the `image_augmentation` pipeline easily to the `preprocess` and rebuild `train_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b49e0712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m:\n",
       "\n",
       "   \u001b[32m   │└─────────────────────────────┐\n",
       "      │                              │\n",
       "      ▼ img                          ▼ img\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mconvert_to_tensor            \u001b[32m/\u001b[0m convert_to_tensor\n",
       "   \u001b[32m   │                              │\n",
       "      ▼ args                         ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mreshape(-1, 28, 28)          \u001b[32m/\u001b[0m padl.Identity()  \n",
       "   \u001b[32m   │                              │\n",
       "      ▼ args                         ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0m\u001b[32m[\u001b[0mimage_augmentation: ..\u001b[32m>>\u001b[0m..\u001b[32m]\u001b[0m \u001b[32m/\u001b[0m padl.Identity()  \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mBatchify(dim=0)             \n",
       "   \u001b[32m   │└─────────────────────────────┐\n",
       "      │                              │\n",
       "      ▼ x                            ▼ args\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mSimpleNet()                  \u001b[32m/\u001b[0m type(torch.int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_with_augmentation = (\n",
    "    convert_to_tensor / convert_to_tensor\n",
    "    >> padl.this.reshape(-1, 28, 28) / padl.Identity()\n",
    "    >> image_augmentation / padl.Identity()\n",
    ")\n",
    "\n",
    "train_model = (\n",
    "    preprocess_with_augmentation\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.this.type(torch.long)\n",
    ")\n",
    "\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd544eca",
   "metadata": {},
   "source": [
    "### 4.3 Retraining the model with `image_augmentation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cffaae3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0; Step: 0; loss: 0.32127583026885986\n",
      "Epoch:0; Step: 10; loss: 0.28310921788215637\n",
      "Epoch:0; Step: 20; loss: 0.37270084023475647\n",
      "Epoch:0; Step: 30; loss: 0.341715544462204\n",
      "Epoch:0; Step: 40; loss: 0.23670774698257446\n",
      "Epoch:0; Step: 50; loss: 0.29833006858825684\n",
      "Epoch:0; Step: 60; loss: 0.23841330409049988\n",
      "Epoch:0; Step: 70; loss: 0.20184899866580963\n",
      "Epoch:0; Step: 80; loss: 0.28233492374420166\n",
      "Epoch:0; Step: 90; loss: 0.21004031598567963\n",
      "Epoch:0; Step: 100; loss: 0.21558339893817902\n",
      "Epoch:0; Step: 110; loss: 0.2617678940296173\n",
      "Epoch:0; Step: 120; loss: 0.2567138373851776\n",
      "Epoch:0; Step: 130; loss: 0.24003081023693085\n",
      "Epoch:0; Step: 140; loss: 0.19946454465389252\n",
      "Epoch:0; Step: 150; loss: 0.21926629543304443\n",
      "Epoch:0; Step: 160; loss: 0.18367506563663483\n",
      "Epoch:0; Step: 170; loss: 0.19670942425727844\n",
      "Epoch:0; Step: 180; loss: 0.25974568724632263\n",
      "Epoch:0; Step: 190; loss: 0.1497238427400589\n",
      "Epoch:0; Step: 200; loss: 0.17082056403160095\n",
      "Epoch:0; Step: 210; loss: 0.21646824479103088\n",
      "Epoch:0; Step: 220; loss: 0.19863227009773254\n",
      "Epoch:0; Step: 230; loss: 0.04893667250871658\n",
      "Epoch:1; Step: 0; loss: 0.19727320969104767\n",
      "Epoch:1; Step: 10; loss: 0.17249901592731476\n",
      "Epoch:1; Step: 20; loss: 0.24625997245311737\n",
      "Epoch:1; Step: 30; loss: 0.2123267501592636\n",
      "Epoch:1; Step: 40; loss: 0.1660865992307663\n",
      "Epoch:1; Step: 50; loss: 0.25713714957237244\n",
      "Epoch:1; Step: 60; loss: 0.19159336388111115\n",
      "Epoch:1; Step: 70; loss: 0.1330374926328659\n",
      "Epoch:1; Step: 80; loss: 0.20974503457546234\n",
      "Epoch:1; Step: 90; loss: 0.18196849524974823\n",
      "Epoch:1; Step: 100; loss: 0.19427219033241272\n",
      "Epoch:1; Step: 110; loss: 0.23398856818675995\n",
      "Epoch:1; Step: 120; loss: 0.24615713953971863\n",
      "Epoch:1; Step: 130; loss: 0.18039870262145996\n",
      "Epoch:1; Step: 140; loss: 0.1898404061794281\n",
      "Epoch:1; Step: 150; loss: 0.1909160614013672\n",
      "Epoch:1; Step: 160; loss: 0.18827158212661743\n",
      "Epoch:1; Step: 170; loss: 0.13561831414699554\n",
      "Epoch:1; Step: 180; loss: 0.22158066928386688\n",
      "Epoch:1; Step: 190; loss: 0.11466304957866669\n",
      "Epoch:1; Step: 200; loss: 0.1729547530412674\n",
      "Epoch:1; Step: 210; loss: 0.19796967506408691\n",
      "Epoch:1; Step: 220; loss: 0.174199178814888\n",
      "Epoch:1; Step: 230; loss: 0.05051735043525696\n",
      "Epoch:2; Step: 0; loss: 0.19098712503910065\n",
      "Epoch:2; Step: 10; loss: 0.1572117656469345\n",
      "Epoch:2; Step: 20; loss: 0.2613510489463806\n",
      "Epoch:2; Step: 30; loss: 0.24245916306972504\n",
      "Epoch:2; Step: 40; loss: 0.13173560798168182\n",
      "Epoch:2; Step: 50; loss: 0.23269647359848022\n",
      "Epoch:2; Step: 60; loss: 0.1654384881258011\n",
      "Epoch:2; Step: 70; loss: 0.1463681310415268\n",
      "Epoch:2; Step: 80; loss: 0.2320900410413742\n",
      "Epoch:2; Step: 90; loss: 0.1484825760126114\n",
      "Epoch:2; Step: 100; loss: 0.1637241393327713\n",
      "Epoch:2; Step: 110; loss: 0.21291570365428925\n",
      "Epoch:2; Step: 120; loss: 0.23380424082279205\n",
      "Epoch:2; Step: 130; loss: 0.20331627130508423\n",
      "Epoch:2; Step: 140; loss: 0.19305892288684845\n",
      "Epoch:2; Step: 150; loss: 0.19027788937091827\n",
      "Epoch:2; Step: 160; loss: 0.15357324481010437\n",
      "Epoch:2; Step: 170; loss: 0.15402454137802124\n",
      "Epoch:2; Step: 180; loss: 0.23473630845546722\n",
      "Epoch:2; Step: 190; loss: 0.1396263986825943\n",
      "Epoch:2; Step: 200; loss: 0.16771972179412842\n",
      "Epoch:2; Step: 210; loss: 0.20064403116703033\n",
      "Epoch:2; Step: 220; loss: 0.1745293140411377\n",
      "Epoch:2; Step: 230; loss: 0.04223841056227684\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "nepoch = 3\n",
    "num_workers = 0\n",
    "\n",
    "optimizer = optim.SGD(train_model.pd_parameters(), lr=learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    step_counter = 0\n",
    "    for batch_output, batch_targets in train_model.train_apply(mnist_train_dataset, num_workers=num_workers, batch_size=256):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.nll_loss(batch_output, batch_targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        if step_counter % log_interval == 0:\n",
    "            print(f'Epoch:{epoch}; Step: {step_counter}; loss: {loss}')\n",
    "        step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3aeb9",
   "metadata": {},
   "source": [
    "### 4.4 Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cadda40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 0.9698\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = 0\n",
    "for batch_output, batch_targets in validation_model.eval_apply(mnist_test_dataset, num_workers=0, batch_size=256):\n",
    "    accuracy += (batch_targets == batch_output).sum()\n",
    "\n",
    "accuracy = accuracy.item()/ len(mnist_test_dataset)\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenen_env",
   "language": "python",
   "name": "tenen_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
