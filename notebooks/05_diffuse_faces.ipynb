{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notebook for portrait warping using masked classifier free guided diffusion.\n",
        "\n",
        "\n",
        "This notebook uses:\n",
        "\n",
        "- [an implementation of classifier free guided diffusion](https://github.com/crowsonkb/v-diffusion-pytorch) by Katherine Crowson and Chainbreakers AI\n",
        "\n",
        "- [the CLIP model by OpenAI](https://github.com/openai/CLIP)\n",
        "\n",
        "- [PADL](https://github.com/lf1-io/padl)\n",
        "\n",
        "- [Opencv face detection](https://opencv.org)\n",
        "\n",
        "We extended the sampling algorithm from https://github.com/crowsonkb/v-diffusion-pytorch with an in-painting technique for denoising diffusion models based on the paper [\"Inpainting using Denoising Diffusion Probabilistic Models\" by Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, Luc Van Gool](https://arxiv.org/pdf/2201.09865.pdf).\n",
        "\n",
        "**Execute all cells up to the Upload field to prepare, follow instructions from\n",
        "there on.**\n",
        "\n",
        "**The notebook requires GPU - make sure to enable GPU runtimes in Colab.**"
      ],
      "metadata": {
        "id": "2N4JxC-x2C06"
      },
      "id": "2N4JxC-x2C06"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "79df026c",
      "metadata": {
        "id": "79df026c",
        "outputId": "1ca2c0a5-161b-4a81-8fc9-6a31cc354a1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: padl in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from padl) (1.6.3)\n",
            "Requirement already satisfied: asttokens in /usr/local/lib/python3.7/dist-packages (from padl) (2.0.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from padl) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from padl) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens->padl) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->padl) (0.37.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->padl) (3.10.0.2)\n",
            "fatal: destination path 'v-diffusion-pytorch' already exists and is not an empty directory.\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from -r v-diffusion-pytorch/requirements.txt (line 1)) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r v-diffusion-pytorch/requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r v-diffusion-pytorch/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r v-diffusion-pytorch/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r v-diffusion-pytorch/requirements.txt (line 5)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r v-diffusion-pytorch/requirements.txt (line 6)) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r v-diffusion-pytorch/requirements.txt (line 7)) (4.63.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->-r v-diffusion-pytorch/requirements.txt (line 1)) (0.2.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r v-diffusion-pytorch/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r v-diffusion-pytorch/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r v-diffusion-pytorch/requirements.txt (line 3)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r v-diffusion-pytorch/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r v-diffusion-pytorch/requirements.txt (line 5)) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->-r v-diffusion-pytorch/requirements.txt (line 6)) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install padl\n",
        "!git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch.git\n",
        "!pip install -r v-diffusion-pytorch/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ddcd545",
      "metadata": {
        "id": "4ddcd545",
        "outputId": "9ce6aa13-87c6-41b1-8565-043288930815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            " 15 2300M   15  356M    0     0  20.1M      0  0:01:54  0:00:17  0:01:37 22.1M"
          ]
        }
      ],
      "source": [
        "# download the diffusion model checkpoint model\n",
        "!curl https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth --output cc12m_1_cfg.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777c39e4",
      "metadata": {
        "id": "777c39e4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('v-diffusion-pytorch')\n",
        "sys.path.append('v-diffusion-pytorch/CLIP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d6ceada",
      "metadata": {
        "id": "2d6ceada"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "\n",
        "import math\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import io\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from clip import clip, tokenize\n",
        "\n",
        "from diffusion import get_model, get_models, sampling, utils\n",
        "from diffusion.sampling import plms_sample\n",
        "from diffusion.models.cc12m_1 import CC12M1Model\n",
        "\n",
        "import padl\n",
        "\n",
        "transforms = padl.transform(transforms)\n",
        "F = padl.transform(F)\n",
        "CC12M1Model = padl.transform(CC12M1Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e957534b",
      "metadata": {
        "id": "e957534b"
      },
      "outputs": [],
      "source": [
        "IMAGE_FOLDER = 'generated_images'\n",
        "DEVICE = 'cuda:0'\n",
        "BATCH_SIZE = 4\n",
        "HEIGHT = 256\n",
        "WIDTH = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d64e7537",
      "metadata": {
        "id": "d64e7537"
      },
      "outputs": [],
      "source": [
        "# load models\n",
        "diffusion_model = CC12M1Model()\n",
        "diffusion_model.load_state_dict(torch.load('cc12m_1_cfg.pth'))\n",
        "clip_model = padl.transform(clip.load('ViT-B/16', jit=False)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8d15d6",
      "metadata": {
        "id": "be8d15d6"
      },
      "outputs": [],
      "source": [
        "# utils\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "@padl.transform\n",
        "class Resize:\n",
        "    def __init__(self, width, height):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "    \n",
        "    def __call__(self, image):\n",
        "        fac = max(self.width / image.size[0], self.height / image.size[1])\n",
        "        return image.resize((int(fac * image.size[0]), int(fac * image.size[1])), Image.LANCZOS)\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "def load_image(path):\n",
        "    return Image.open(path).convert('RGB')\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "def unpack(x):\n",
        "    if isinstance(x, tuple):\n",
        "        if len(x) == 0:\n",
        "            return x\n",
        "        return unpack(x[0]) + unpack(x[1:])\n",
        "    else:\n",
        "        return x,\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "def pil_image_to_tensor(x):\n",
        "    \"\"\"Converts from a PIL image to a tensor.\"\"\"\n",
        "    x = TF.to_tensor(x)\n",
        "    if x.ndim == 2:\n",
        "        x = x[..., None]\n",
        "    return x * 2 - 1\n",
        "\n",
        "def Preprocess(width, height, pad):\n",
        "    return (\n",
        "        transforms.Pad(pad)\n",
        "        >> Resize(width, height)\n",
        "        >> transforms.CenterCrop((width, height))\n",
        "        >> pil_image_to_tensor\n",
        "        >> padl.same.to(DEVICE)\n",
        "    )\n",
        "\n",
        "def ImageLoader(width, height, pad):\n",
        "    return (\n",
        "        load_image\n",
        "        >> Preprocess(width, height, pad)\n",
        "    )\n",
        "\n",
        "@padl.transform\n",
        "def display_images(images):\n",
        "    fig = plt.figure(0, figsize=(20,20))\n",
        "    for ix, im in enumerate(images):\n",
        "        ax = fig.add_subplot(2, len(images) // 2, ix+1)\n",
        "        ax.imshow(im)\n",
        "        ax.axis('off')\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988e00f4",
      "metadata": {
        "id": "988e00f4"
      },
      "outputs": [],
      "source": [
        "# utils for generating the face-mask\n",
        "\n",
        "@padl.transform\n",
        "def ellipse(im, box_coords):\n",
        "    x, y, w, h = tuple(box_coords)\n",
        "    center = ((x + w // 2), (y + h // 2))\n",
        "    return cv2.ellipse(im.copy(), center, (int(h // 2 * 1.3), w // 2), 90, 0, 360, 1, -1)\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "def grey(im_array):\n",
        "    return cv2.cvtColor(im_array, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "def torch_im_to_numpy(im):\n",
        "    return im.cpu().permute(0,1,2).numpy()\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "def torch_im_to_pil(im):\n",
        "    return utils.to_pil_image(im)\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "class FaceDetect:\n",
        "    def __init__(self):\n",
        "        self.cc = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    \n",
        "    def __call__(self, grey_image):\n",
        "        return self.cc.detectMultiScale(grey_image)[:1]\n",
        "\n",
        "    \n",
        "@padl.transform\n",
        "def add_batch_dim(im):\n",
        "    return im.repeat(BATCH_SIZE, 3, 1, 1)\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "def smoothen(im):\n",
        "    kernel = np.ones((30,30), np.float32) / 25\n",
        "    res = cv2.filter2D(im, -1, kernel)\n",
        "    res /= res.max()\n",
        "    res -= res.min()\n",
        "    return res\n",
        "    \n",
        "    \n",
        "facedetect_torch = (\n",
        "    torch_im_to_pil\n",
        "    >> padl.transform(np.array)\n",
        "    >> grey\n",
        "    >> FaceDetect()\n",
        ")\n",
        "\n",
        "facedetect_mask = (\n",
        "    facedetect_torch\n",
        "    >> ~ padl.transform(lambda x: ellipse(np.zeros((WIDTH, HEIGHT)), x))\n",
        "    >> padl.transform(lambda x: sum(x))\n",
        "    >> smoothen\n",
        "    >> padl.transform(lambda x: torch.tensor(x, dtype=torch.float32).to(DEVICE))\n",
        "    >> add_batch_dim\n",
        ")\n",
        "\n",
        "\n",
        "def t_to_alpha_sigma(t, wave_length=2):\n",
        "    \"\"\"Returns the scaling factors for the clean image and for the noise, given\n",
        "    a timestep.\"\"\"\n",
        "    return torch.cos(t * math.pi/wave_length), torch.sin(t * math.pi/wave_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4a7310",
      "metadata": {
        "id": "0a4a7310"
      },
      "outputs": [],
      "source": [
        "# clip-embedding pipelines\n",
        "\n",
        "embed_text = (\n",
        "    padl.transform(tokenize)\n",
        "    >> padl.same.to(DEVICE)\n",
        "    >> padl.transform(clip_model.encode_text)\n",
        ")\n",
        "\n",
        "embed_prompts = (\n",
        "    ~ embed_text\n",
        ")\n",
        "\n",
        "embed_image = (\n",
        "    Preprocess(WIDTH, HEIGHT, 0)\n",
        "    >> normalize\n",
        "    >> padl.batch\n",
        "    >> padl.transform(clip_model.encode_image)\n",
        "    >> padl.same.float()\n",
        "    >> padl.transform(lambda x: F.normalize(x, dim=-1))\n",
        ")\n",
        "\n",
        "embed_images = (\n",
        "    ~ embed_image\n",
        ")\n",
        "\n",
        "embed_images = embed_images.pd_to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09e2cc4",
      "metadata": {
        "id": "f09e2cc4"
      },
      "outputs": [],
      "source": [
        "# sampling algorithms\n",
        "\n",
        "@padl.transform\n",
        "class CFGModel(torch.nn.Module):\n",
        "    def __init__(self, model, target_embeddings, weights):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.target_embeddings = target_embeddings\n",
        "        self.weights = weights\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        n = x.shape[0]\n",
        "        n_conds = len(self.target_embeddings)\n",
        "        x_in = x.repeat([n_conds, 1, 1, 1])\n",
        "        t_in = t.repeat([n_conds])\n",
        "        clip_embed_in = torch.cat([*self.target_embeddings]).repeat_interleave(n, 0)\n",
        "        vs = self.model(x_in, t_in, clip_embed_in).view([n_conds, n, *x.shape[1:]])\n",
        "        v = vs.mul(self.weights[:, None, None, None, None]).sum(0)\n",
        "        return v\n",
        "\n",
        "    \n",
        "def make_model(text_prompts, image_prompts):\n",
        "    if text_prompts is None:\n",
        "        text_prompts = {}\n",
        "\n",
        "    if image_prompts is None:\n",
        "        image_prompts = {}\n",
        "\n",
        "    target_embeddings = (\n",
        "        torch.zeros([1, clip_model.visual.output_dim], device=DEVICE),\n",
        "        *list(embed_prompts(text_prompts) + embed_images.infer_apply([x[0] for x in image_prompts]))\n",
        "    )\n",
        "    weights =  list(text_prompts.values()) + list([x[1] for x in image_prompts])\n",
        "    weights = 1 - sum(weights), *weights\n",
        "    weights = torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
        "    return CFGModel(diffusion_model, target_embeddings, weights).pd_to(DEVICE)\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "class Sampler:\n",
        "    \"\"\"Draws samples from a model given starting noise.\n",
        "\n",
        "        :param model: Diffusion model\n",
        "        :param x: input\n",
        "        :param steps: List of stepsizes\n",
        "        :param eta: Amount of noise added every step\n",
        "        :param extra_args: args for model\n",
        "        :param target_img: Target Image to match to\n",
        "        :param mask: Mask to mask the target image\n",
        "        :param resample_step: Number of re-runs of resampling\n",
        "        :param resample_frequecy: How often to run resampling\n",
        "        :param stop_mask_step_size: Step when to stop masking\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 eta=0, \n",
        "                 sample_steps=50,\n",
        "                 n_resample_steps=10,\n",
        "                 resample_frequency=4,\n",
        "                 resample_fresh_noise=0.5,\n",
        "                 resample_stop_timestep=0,\n",
        "                 unmask_timestep=0,\n",
        "                 starting_timestep=1.,\n",
        "                 schedule_wave_length=2.,  # TODO: ?\n",
        "                 shape=(256,256)):\n",
        "        \n",
        "        self.eta = eta\n",
        "        self.sample_steps = sample_steps\n",
        "        \n",
        "        self.n_resample_steps = n_resample_steps\n",
        "        self.resample_frequency = resample_frequency\n",
        "        self.resample_fresh_noise = resample_fresh_noise\n",
        "        self.resample_stop_timestep = resample_stop_timestep\n",
        "        \n",
        "        self.starting_timestep = starting_timestep\n",
        "        \n",
        "        self.unmask_timestep = unmask_timestep\n",
        "        \n",
        "        self.t = torch.linspace(1, 0, sample_steps + 1, device=DEVICE)[:-1]\n",
        "        steps = utils.get_spliced_ddpm_cosine_schedule(self.t)\n",
        "        self.steps = steps[steps < starting_timestep]\n",
        "        \n",
        "        self.schedule_wave_length = schedule_wave_length\n",
        "        self.alphas, self.sigmas = t_to_alpha_sigma(self.steps, wave_length=schedule_wave_length)\n",
        "        \n",
        "        self.shape = shape\n",
        "\n",
        "    def __call__(self, target_image, mask, text_prompts=None, image_prompts=None):\n",
        "        \"\"\"Call (forward) of Sampler\"\"\"\n",
        "        \n",
        "        alphas, sigmas = self.alphas, self.sigmas\n",
        "        \n",
        "        cfg_model = make_model(text_prompts, image_prompts)\n",
        "        \n",
        "        x = torch.randn([BATCH_SIZE, 3, self.shape[0], self.shape[1]], device=DEVICE)\n",
        "        ts = x.new_ones([x.shape[0]])\n",
        "        \n",
        "        init = target_image[None].repeat([BATCH_SIZE, 1, 1, 1]).to(DEVICE)\n",
        "        x = init * alphas[0] + x * sigmas[0]\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        pbar = trange(len(self.steps), disable=None)\n",
        "\n",
        "        for step_ind in pbar:\n",
        "            \n",
        "            if self.steps[step_ind] < self.unmask_timestep:\n",
        "                mask = None\n",
        "\n",
        "            if mask is None:\n",
        "                n_resample_steps = 1\n",
        "            elif step_ind % self.resample_frequency != 1:\n",
        "                n_resample_steps = 1\n",
        "            elif step_ind == len(self.steps) - 1:\n",
        "                n_resample_steps = 1\n",
        "            elif self.steps[step_ind] < self.resample_stop_timestep:\n",
        "                n_resample_steps = 1\n",
        "            else:\n",
        "                n_resample_steps = self.n_resample_steps\n",
        "\n",
        "            for resample_step in range(n_resample_steps):\n",
        "                mask_none = mask is None\n",
        "                pbar.set_description(f'step: {step_ind}; resampling: {resample_step}/{n_resample_steps}; mask none: {mask_none}')\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    v = cfg_model.infer_apply((x, ts * self.steps[step_ind])).float()\n",
        "\n",
        "                # Predict the noise and the denoised image\n",
        "                pred = x * alphas[step_ind] - v * sigmas[step_ind]\n",
        "                eps = x * sigmas[step_ind] + v * alphas[step_ind]\n",
        "\n",
        "                preds.append((resample_step, pred))\n",
        "\n",
        "                # If we are in last step, stop here\n",
        "                if step_ind == len(self.steps) - 1:\n",
        "                    break\n",
        "\n",
        "                if resample_step < n_resample_steps - 1:\n",
        "                    # If resampling is continuing, readd noise and go again\n",
        "                    \n",
        "                    noise = (\n",
        "                        math.sqrt(self.resample_fresh_noise) * torch.randn_like(x) \n",
        "                        + math.sqrt(1 - self.resample_fresh_noise) * eps\n",
        "                    )\n",
        "\n",
        "                    x_guess = pred * alphas[step_ind] + noise * sigmas[step_ind]\n",
        "                    x_real = target_image * alphas[step_ind] + noise * sigmas[step_ind]\n",
        "                    x = x_real * mask + x_guess * abs(1 - mask)\n",
        "                    continue\n",
        "\n",
        "                    \n",
        "                ddim_sigma = self.eta * (sigmas[step_ind + 1]**2 / sigmas[step_ind]**2).sqrt() * \\\n",
        "                    (1 - alphas[step_ind]**2 / alphas[step_ind + 1]**2).sqrt()\n",
        "                adjusted_sigma = (sigmas[step_ind + 1]**2 - ddim_sigma**2).sqrt()\n",
        "                \n",
        "                x_guess = pred * alphas[step_ind + 1] + eps * adjusted_sigma\n",
        "                \n",
        "                if self.eta:\n",
        "                    x_guess += torch.randn_like(x) * ddim_sigma\n",
        "\n",
        "                if mask is None:\n",
        "                    x = x_guess\n",
        "                    continue\n",
        "\n",
        "                x_real = target_image * alphas[step_ind + 1] + eps * sigmas[step_ind + 1]\n",
        "                x = x_real * mask + x_guess * abs(1 - mask)\n",
        "                \n",
        "        return pred, preds\n",
        "\n",
        "\n",
        "@padl.transform\n",
        "class PLMSSampler:\n",
        "    def __init__(self,\n",
        "                 sample_steps=50,\n",
        "                 starting_timestep=None,\n",
        "                 shape=(256,256),\n",
        "                 batch_size=1\n",
        "                ):\n",
        "        \n",
        "        self.t = torch.linspace(1, 0, sample_steps + 1, device=DEVICE)[:-1]\n",
        "        self.steps = utils.get_spliced_ddpm_cosine_schedule(self.t)\n",
        "        if starting_timestep is not None:\n",
        "            self.steps = self.steps[self.steps < starting_timestep]\n",
        "        \n",
        "        self.alphas, self.sigmas = t_to_alpha_sigma(self.steps)\n",
        "        \n",
        "        self.shape = shape\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def __call__(self, target_images=None, text_prompts=None, image_prompts=None):\n",
        "        \"\"\"Call (forward) of Sampler\"\"\"\n",
        "        \n",
        "        alphas, sigmas = self.alphas, self.sigmas\n",
        "        \n",
        "        cfg_model = make_model(text_prompts, image_prompts)\n",
        "        \n",
        "        x = torch.randn([self.batch_size, 3, self.shape[0], self.shape[1]], device=DEVICE)\n",
        "        \n",
        "        if target_images is not None:\n",
        "            x = target_images.to(DEVICE) * alphas[0] + x * sigmas[0]\n",
        "        \n",
        "        return plms_sample(cfg_model, x, self.steps, {})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce06b0a3",
      "metadata": {
        "id": "ce06b0a3"
      },
      "source": [
        "Build the sampler - play with the parameters to change the results.\n",
        "\n",
        "Description of parameters:\n",
        "    \n",
        "| Parameter Name            | Permitted Values | Description                                                    |\n",
        "| ------------------------- | ---------------- | -------------------------------------------------------------- |\n",
        "| `eta`                     | 0 - 1            | higher values make the picture more random                     |\n",
        "| `sample_steps`            | int              | number of sample steps, higher values add finer details        |\n",
        "| `n_resample_steps`        | int              | number of resampling steps, more increases fit                 |\n",
        "| `resample_frequency`      | int              | number of steps between resamplings                            |\n",
        "| `resample_fresh_noise`    | 0 - 1            | level of new noise to add when resampling                      |\n",
        "| `resample_stop_time_step` | 0 - 1            | time step at which to stop resampling                          |\n",
        "| `unmask_timestep`         | 0 - 1            | time step at which to remove the mask                          |\n",
        "| `starting_timestep`       | 0 - 1            | time step at which to start sampling                           |\n",
        "| `shape`                   | tuple[w, h]      | shape of image                                                 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456347ad",
      "metadata": {
        "id": "456347ad"
      },
      "outputs": [],
      "source": [
        "sampler = Sampler(eta=0,\n",
        "                  sample_steps=30,\n",
        "                  n_resample_steps=10, \n",
        "                  resample_frequency=3,\n",
        "                  resample_fresh_noise=0.5,\n",
        "                  resample_stop_timestep=0.7,\n",
        "                  unmask_timestep=0.85,\n",
        "                  starting_timestep=0.95,\n",
        "                  shape=(WIDTH, HEIGHT))\n",
        "\n",
        "\n",
        "plms_sampler = PLMSSampler(sample_steps=40,\n",
        "                           starting_timestep=0.6,\n",
        "                           shape=(512, 512),\n",
        "                           batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac65e5d",
      "metadata": {
        "id": "cac65e5d"
      },
      "outputs": [],
      "source": [
        "# pipeline plumbing\n",
        "\n",
        "load_and_mask = (\n",
        "    Preprocess(WIDTH, HEIGHT, 0)\n",
        "    >> padl.identity + facedetect_mask\n",
        ")\n",
        "\n",
        "sample_pipeline = (\n",
        "    load_and_mask / padl.identity\n",
        "    >> unpack\n",
        "    >> sampler\n",
        "    >> padl.same[0]\n",
        "    >> ~ torch_im_to_pil\n",
        ")\n",
        "\n",
        "upscale = (\n",
        "    (Resize(512, 512) >> pil_image_to_tensor) / padl.identity\n",
        "    >> plms_sampler\n",
        "    >> torch_im_to_pil\n",
        ")\n",
        "\n",
        "sample_and_display = (\n",
        "    sample_pipeline\n",
        "    >> display_images + padl.identity\n",
        "    >> padl.same[1]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following cell and upload a picture using the \"Upload\" button that appears. The picture should have a face on it."
      ],
      "metadata": {
        "id": "mwm9H8pz8YAx"
      },
      "id": "mwm9H8pz8YAx"
    },
    {
      "cell_type": "code",
      "source": [
        "uploader = widgets.FileUpload()\n",
        "uploader"
      ],
      "metadata": {
        "id": "WrBN9Os9wDu-"
      },
      "id": "WrBN9Os9wDu-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, file_info in uploader.value.items():\n",
        "    img = Image.open(io.BytesIO(file_info['content']))"
      ],
      "metadata": {
        "id": "ZBIGkwIW0dp8"
      },
      "id": "ZBIGkwIW0dp8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to generate pictures.\n",
        "Experiment with the prompt.\n",
        "\n",
        "These might work:\n",
        "\n",
        "- \"portrait of the queen, oil on canvas\"\n",
        "- \"communist sci-fi movie poster\"\n",
        "- \"a clown, by van gogh\"\n",
        "\n",
        "How well this works depends on the combination of picture and prompt - some prompts may work well for a given picture others less so."
      ],
      "metadata": {
        "id": "_f4sBHzc8rAX"
      },
      "id": "_f4sBHzc8rAX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d5cc3a",
      "metadata": {
        "id": "09d5cc3a"
      },
      "outputs": [],
      "source": [
        "prompt = {\"portrait painting of a woman with flowers, by frida kahlo, trending on artstation\": 5}\n",
        "r = sample_pipeline.infer_apply((img, prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fc3b93",
      "metadata": {
        "scrolled": false,
        "id": "f8fc3b93"
      },
      "outputs": [],
      "source": [
        "display_images(r);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell scales the picture up and adds some more detail."
      ],
      "metadata": {
        "id": "b1Vd914Z9Y5p"
      },
      "id": "b1Vd914Z9Y5p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592f43eb",
      "metadata": {
        "id": "592f43eb"
      },
      "outputs": [],
      "source": [
        "upscale.infer_apply((r[2], prompt))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "05_diffuse_faces.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}