{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!curl https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8?resourcekey=0-5BR16BdXnb8hVj6CNHKzLg&usp=sharing -o data/celeba.zip\n",
    "!unzip data/celeba.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d71f34",
   "metadata": {},
   "source": [
    "This tutorial shows you how to use PADL to build pipelines in PyTorch on an iconic deep learning task - generating celebrity faces! We follow the PyTorch [example](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) closely, which will allow you to compare and contrast how to build\n",
    "equivalent functionality with PADL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"data/celeba/img_align_celeba/\"\n",
    "workers = 2\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "nc = 3\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "num_epochs = 5\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c531a712",
   "metadata": {},
   "source": [
    "Let's import some key functions - from PADL and torchvision (since we'll be generating images). We can use all of the functionality in torchvision, and any third party packages, by wrapping a module in the package with `transform`. This gives us native pipeline blocks known as \"transforms\", which may be combined to create pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl import transform\n",
    "from torchvision import transforms as vision\n",
    "\n",
    "vision = transform(vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e02b8a",
   "metadata": {},
   "source": [
    "Now we can compose any functions or callables with a nice piping syntax, combining transforms into a single pipeline. The pipeline has a handy print functionality, to really see what is going on in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prep = (\n",
    "    vision.Resize(image_size)\n",
    "    >> vision.CenterCrop(image_size)\n",
    "    >> vision.ToTensor()\n",
    "    >> vision.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    ")\n",
    "image_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4eabcd",
   "metadata": {},
   "source": [
    "These pipelines are callable - we can test this on an image from the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "image_prep(PIL.Image.open('data/celeba/img_align_celeba/000001.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1e947",
   "metadata": {},
   "source": [
    "To check the intermediate steps of the transform, we can use a handy subsetting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a468ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prep[:2](PIL.Image.open('data/celeba/img_align_celeba/000001.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204ec86",
   "metadata": {},
   "source": [
    "We can define custom transforms by decorating functions or callable classes with `@transform`. We can also wrap single functions as we do here with `PIL.Image.open`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import random\n",
    "\n",
    "images = [f'{dataroot}/{x}' for x in os.listdir(dataroot)]\n",
    "\n",
    "@transform\n",
    "def random_image(*args, **kwargs):\n",
    "    image = random.choice(images)\n",
    "    return image\n",
    "\n",
    "load_image = transform(PIL.Image.open)\n",
    "    \n",
    "# test the composition\n",
    "(random_image >> load_image)(())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f387c",
   "metadata": {},
   "source": [
    "Now we can combine these steps to get an image tensor sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = (random_image >> load_image >> image_prep)\n",
    "image_loader(())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8451445",
   "metadata": {},
   "source": [
    "Pytorch layers are first class citizens in PADL, and can be converted to PADL just as before with `@transform`. PADL tracks all torch functionality by composing the class with a PADL object. In the wrapped class, PADL functionality is isolated under methods beginning `.pd_...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9801d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "@transform\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super().__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = torch.nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            torch.nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            torch.nn.BatchNorm2d(ngf * 8),\n",
    "            torch.nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            torch.nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(ngf * 4),\n",
    "            torch.nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            torch.nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(ngf * 2),\n",
    "            torch.nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            torch.nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(ngf),\n",
    "            torch.nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            torch.nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            torch.nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "\n",
    "@transform\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super().__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = torch.nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            torch.nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            torch.nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(ndf * 2),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            torch.nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(ndf * 4),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            torch.nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            torch.nn.BatchNorm2d(ndf * 8),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            torch.nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "\n",
    "netD = Discriminator(ngpu)\n",
    "netG = Generator(ngpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae603c7",
   "metadata": {},
   "source": [
    "We do something similar for the generator model.\n",
    "\n",
    "Here we use the keyword `same` which allows for a sort of neat inline lambda function. Standard `lambda` functions are also supported.\n",
    "\n",
    "You'll also see the `batch` and `unbatch` keywords. These define where the preprocessing ends and forward pass begins, and forward pass ends and postprocessing begins.\n",
    "\n",
    "When used in batch-mode (see below), everything prior to the `batch` is wrapped into a `torch.utils.data.DataLoader`. Every after `unbatch` is mapped over the individual batch elements of the forward pass. When used in single data-point mode, a single element batch is constructed.\n",
    "\n",
    "This leads to far less boilerplate, and far fewer errors with batch dimensions, etc.. \n",
    "\n",
    "The *main* advantage of this, however, is that it allows the program to isolate all bits of code to run the generation pipeline, and to export these into a single portable saved artifact. This artifact may be then shared, compressed, imported into a serving environment etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl import batch, unbatch, same\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "\n",
    "@transform\n",
    "def generate_noise(dummy):\n",
    "    return torch.randn(nz, 1, 1)\n",
    "\n",
    "\n",
    "@transform\n",
    "def denormalize(x):\n",
    "    rescaled = 255 * (x * 0.5 + 0.5)\n",
    "    converted = rescaled.numpy()\n",
    "    return converted.astype(numpy.uint8)\n",
    "    \n",
    "    \n",
    "generator = (\n",
    "    generate_noise\n",
    "    >> batch\n",
    "    >> netG\n",
    "    >> unbatch\n",
    "    >> denormalize\n",
    "    >> same.transpose(1, 2, 0)\n",
    "    >> transform(lambda x: PIL.Image.fromarray(x))\n",
    ")\n",
    "\n",
    "generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d98b2e",
   "metadata": {},
   "source": [
    "Let's check the PADL-saved output. The saved artifact consists of a small python module, which includes only the bits of code which went into defining the generator. The saver tracks down all global variables, imports, functions, weights and data artifacts necessary for redefining and restoring the pipeline in its entirety. This is all packaged together into a compact, exportable directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl import save\n",
    "save(generator, 'test.padl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8713c82",
   "metadata": {},
   "source": [
    "When the keywords `batch` or `unbatch` are used, it's no longer to use the `__call__` methods directly anymore. Instead, the pipeline must be \"applied\" in one of three modes \"train\", \"eval\", and \"infer\". That's because the pipeline needs to be told how to construct the batch, and whether to include gradients, and functionality only needed in training.\n",
    "\n",
    "The modes are accessed with three key methods: `train_apply`, `eval_apply`, and `infer_apply`. With `infer_apply`, \n",
    "a single data-point batch is created at the `batch` point of the transform, and then these batch dimensions are removed again by the `unbatch` statement.\n",
    "\n",
    "In `train_apply` and `eval_apply`, a data loader is constructed on the fly and the batches out of this data loader are passed throught the forward pass. The batch is then split into single rows after the `unbatch` statement, and the postprocessing is mapped over these rows. In `train_apply` gradients are activated; in the other modes there are no gradients.\n",
    "\n",
    "Let's apply the generator. Since it is a sampler, we can just pass an empty tuple or list of empty tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.infer_apply(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c590744",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for b in generator.eval_apply([() for _ in range(20)], batch_size=5, num_workers=0):\n",
    "    output.append(b)\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef360bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in generator[:3].train_apply([() for _ in range(20)], batch_size=5, flatten=False):\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ff67c",
   "metadata": {},
   "source": [
    "We can dissect the generating pipeline into preprocessing, forward pass, postprocessing. Let's have a look and \n",
    "validate that `generator.pd_preprocess >> generator.pd_forward >> generator.pd_postproces` is equivalent to `generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.pd_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.pd_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.pd_postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73953f76",
   "metadata": {},
   "source": [
    "These decompositions now also become handy to check the output of the real data pipeline. We can postprocess the tensors coming out of the image-loader with the postprocess part of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image_loader >> generator.pd_postprocess[1:])(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fa81d",
   "metadata": {},
   "source": [
    "There are ways to create branches in the workflow using the operators `/`, `+` and `~`. See [here](link_to_the other_notebook) for details.\n",
    "In the following part, we use `+` to add a label to the discriminator pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl import identity\n",
    "\n",
    "@transform\n",
    "def real_label(x):\n",
    "    return torch.ones_like(x)\n",
    "\n",
    "\n",
    "criterion = transform(torch.nn.BCELoss())\n",
    "\n",
    "\n",
    "errD_real = image_loader >> batch >> netD >> (identity + real_label) >> criterion\n",
    "errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def fake_label(x):\n",
    "    return torch.zeros_like(x)\n",
    "\n",
    "\n",
    "make_fake_tensor = generator.pd_preprocess >> generator.pd_forward\n",
    "\n",
    "\n",
    "errD_fake = (\n",
    "    same.detach()\n",
    "    >> netD\n",
    "    >> identity + fake_label\n",
    "    >> criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79727c0",
   "metadata": {},
   "source": [
    "A test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "errD_fake.infer_apply(torch.randn(1, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfd690",
   "metadata": {},
   "source": [
    "The generator pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "errG = (\n",
    "    netD\n",
    "    >> identity + real_label \n",
    "    >> criterion\n",
    ")\n",
    "errG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81cb5d",
   "metadata": {},
   "source": [
    "We can now create the optimizers and the iterators so that we can do some learning steps. Beware that\n",
    "PyTorch requires specifying how the seed is set in each worker using `init_worker_fn` -- otherwise it's\n",
    "possible to identical lines in the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2772855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbdc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def random_seed_init(i):\n",
    "    torch.manual_seed(int(i))\n",
    "    random.seed(int(i))\n",
    "    numpy.random.seed(int(i))\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "make_fake_tensor.pd_to('cuda')\n",
    "errD_real.pd_to('cuda')\n",
    "generator.pd_to('cuda')\n",
    "errD_fake.pd_to('cuda')\n",
    "errG.pd_to('cuda')\n",
    "\n",
    "fake_generator = iter(make_fake_tensor.train_apply(range(1000000), batch_size=batch_size, num_workers=10,\n",
    "                                                   worker_init_fn=random_seed_init))\n",
    "errD_real_generator = iter(errD_real.train_apply(range(1000000), batch_size=batch_size, num_workers=10,\n",
    "                                                 worker_init_fn=random_seed_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3188c5",
   "metadata": {},
   "source": [
    "The training loop based on these pipelines is now super simple and (hopefully) sheds light\n",
    "on the important structure of how the DC-gan algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-reviewer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "it = 0\n",
    "while True:\n",
    "\n",
    "    fake_tensor = next(fake_generator)\n",
    "\n",
    "    netD.zero_grad()\n",
    "    ed_r = next(errD_real_generator)\n",
    "    ed_r.backward()\n",
    "\n",
    "    ed_f = errD_fake(fake_tensor)\n",
    "    ed_f.backward()\n",
    "    \n",
    "    optimizerD.step()\n",
    "    \n",
    "    netG.zero_grad()\n",
    "    eg = errG(fake_tensor)\n",
    "    eg.backward()\n",
    "\n",
    "    optimizerG.step()\n",
    "    \n",
    "    if it % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        for j in range(5):\n",
    "            display(generator.infer_apply())\n",
    "        print(f'Iteration: {it}; ErrD/real: {ed_r:.3f}; ErrD/fake: {ed_f:.3f}; ErrG: {eg:.3f};')\n",
    "    \n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b2b50",
   "metadata": {},
   "source": [
    "Now let's save the trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a053680",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(generator, 'finished.padl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2859f",
   "metadata": {},
   "source": [
    "A really useful feature, and making the finished pipeline super portable, is the ability to reload the full saved pipeline, without any importing or extra definitions. The following cell works, even after restarting the kernel/ or in a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl import load\n",
    "\n",
    "reloader = load('finished.padl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65caa519",
   "metadata": {},
   "source": [
    "We can now try a few sample generations from the trained pipeline, to check we get what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloader.infer_apply()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
