{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857b925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from padl import transform, batch, unbatch, identity, load, same, value, save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55328284",
   "metadata": {},
   "source": [
    "Get data for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b833d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip\n",
    "!unzip -f training.1600000.processed.noemoticon.csv.zip\n",
    "!rm training.1600000.processed.noemoticon.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d62ec7",
   "metadata": {},
   "source": [
    "Preprocessing for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdba9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'training.1600000.processed.noemoticon.csv',\n",
    "    header=None,\n",
    "    encoding='latin-1',\n",
    ")\n",
    "    \n",
    "    \n",
    "X = df.iloc[:, 5].tolist()\n",
    "Y = df.iloc[:, 0].tolist()\n",
    "Y = [{0: 'BAD', 4: 'GOOD'}[y] for y in Y]\n",
    "    \n",
    "perm = [int(i) for i in numpy.random.permutation(len(X))]\n",
    "X_train = [X[i] for i in perm[:-500]]\n",
    "X_valid = [X[i] for i in perm[-500:]]\n",
    "Y_train = [Y[i] for i in perm[:-500]]\n",
    "Y_valid = [Y[i] for i in perm[-500:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4243b",
   "metadata": {},
   "source": [
    "You’ve spent a few days with sweat and tears putting together a PyTorch model in a Jupyter notebook, tinkering with parameters, trying various preprocessing methods, post-processing methods, validating the model results on examples, and checking on validation data sets that the model performs well. The results are good, you’re happy, you’re boss and colleagues are happy. Now you’d like to share the results, so that other people can play with the model; so what do you do? Here are the options:\n",
    "\n",
    "1. EITHER: Share the notebook\n",
    "2. OR: Share the model weights/ JIT compiled model\n",
    "3. OR: Reprogram everything in proper scripts so that the model can be reloaded by re-importing functions you created out of the individual notebook cells.\n",
    "\n",
    "None of these are ideal. Let’s see why:\n",
    "\n",
    "1. Sharing the notebook only, means that you’d be saving yourself any extra work. But it also means that whoever gets the notebook needs to run it again (reloading the data, retraining the model) in order to get the same results. This may be expensive, or not possible, since you may not be allowed to share the training data.\n",
    "2. The weights or JIT compiled model are no good by themselves; whoever receives these would need to dissect the notebook in order to make sure that they are putting the tensors into the network(s), and post-processing the outputs in the same way.\n",
    "3. Reprogramming is a lengthy and error prone process especially for complex workflows/ trainings. At the end you’d hope to have resurrected the exact algorithms/ routines you have in your Jupyter cells, but with tidy signatures, one of which allows the weights which you’d trained to be reloaded, reproducing exactly the results of the notebook. Unfortunately, when you’re finished, most likely the changes are so **sweeping that you’re not sure the results are properly replicable based on the refactoring. So you’d then want to run the training one or more times, to verify that the results are the same. In addition, you’ll lose the user friendly/ interactive aspect which the notebook has.\n",
    "\n",
    "*It doesn’t have to be this way...*\n",
    "\n",
    "[PADL](https://padl.lf1.io/) allows you to work interactively in notebooks, using global variables, inline functions, preprocessing and post-processing, which utilizes the full gamut of the scientific python ecosystem and beyond. When the notebook is done, and you’re happy with the results, you can simply save the pipeline with the PADL serializer. The saved pipeline will exactly replicate the result obtained in the notebook, including the preprocessing and post-processing, including any additional artifacts necessary, data blobs, third party models (such as scikit-learn) and more. This artifact may then be shared, forwarded, experimented with, served and tested in complete isolation from the original notebook. Creating the workflow with PADL, also has a multitude of user friendly additional benefits - transparency, easy debugging, less boilerplate, code which is close to a common graphical mental model for deep learning.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "PADL tracks down the code and data necessary to save a full model pipeline using two handy abstractions: the “transform” and the “pipeline”. \n",
    "\n",
    "The “transform” is a computational block subsuming preprocessing or postprocessing step, forward pass step or layer into one single class of object. These transforms may be written in a variety of ways. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d281036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform                                               # transform tracks code dependencies\n",
    "def clean(text):\n",
    "    return re.sub('[^A-Za-z\\ ]', ' ', text)\n",
    "\n",
    "\n",
    "split_strip = transform(lambda x: x.strip().split())     # transform can wrap a function like this too\n",
    "\n",
    "\n",
    "lower_case = same.lower()                                # same allows easy referring to input - like a simple inline lambda\n",
    "\n",
    "\n",
    "@transform                                               # callable classes work too!\n",
    "class Dictionary:\n",
    "    def __init__(self, d, default='<unk>'):\n",
    "        self.d = d\n",
    "        self.default = default\n",
    "        \n",
    "    def __call__(self, token):\n",
    "        if token in self.d:\n",
    "            return self.d[token]\n",
    "        else:\n",
    "            return self.d[self.default]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154a3cb",
   "metadata": {},
   "source": [
    "Once you’ve defined a collection of transforms using `transform`, they may be linked into pipelines using a few primitive operators, leading to a DAG structure for the pipeline. The operators are:\n",
    "\n",
    "`Compose` which has the overloading short-hand `>>`. This is similar to composing in, for example, `torchvision`. Transforms or other pipelines’ outputs are passed positionally onto the subsequent objects in the composition. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd0665c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"text_process\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ text\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mclean                      \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mlower()                    \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mlambda x: x.strip().split()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process = (\n",
    "    clean\n",
    "    >> lower_case\n",
    "    >> split_strip\n",
    ")\n",
    "\n",
    "text_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a570f",
   "metadata": {},
   "source": [
    "We can use this text processor, for instance, to create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39404370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_dictionary(x, fn):\n",
    "    with open(fn, 'w') as f:\n",
    "        json.dump(x, f)\n",
    "        \n",
    "        \n",
    "def load_dictionary(x):\n",
    "    with open(fn) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8de805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1308911/1599500 [00:06<00:01, 191763.37it/s]"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in tqdm.tqdm(X_train):\n",
    "    words.extend(text_process(sentence))\n",
    "counts = dict(collections.Counter(words))\n",
    "allowed = sorted(list(counts.keys()), key=lambda x: -counts[x])[:20000]\n",
    "allowed = value(allowed, (save_dictionary, load_dictionary, '.json'))\n",
    "allowed.append('<unk>')\n",
    "dictionary = Dictionary(dict(zip(allowed, range(len(allowed)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3f974",
   "metadata": {},
   "source": [
    "Let's add this to the text-processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_process = text_process >> ~ dictionary\n",
    "text_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a68ba6",
   "metadata": {},
   "source": [
    "`Parallel` which has the overloading short-hand `/`. This refers to the situation where multiple transforms are applied “in parallel” to a tuple/ list of outputs from a previous step. This allows you to create complex branching pipelines, providing great flexibility and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel sends each part of a tuple to the ith transform in \"parallel\"\n",
    "(clean / lower_case)(('Test another', 'Test thingy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431d835",
   "metadata": {},
   "source": [
    "`Rollout` is related to `Parallel` and has the short hand `+`; several transforms are applied over the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5077dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(clean + lower_case)('Testing transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfb35a",
   "metadata": {},
   "source": [
    "Map is the classical well known functional primitive and has short hand ~. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a967a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~ clean)(['Testing transform' for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c43a7c",
   "metadata": {},
   "source": [
    "PyTorch layers are first class objects in PADL, that means we can decorate the layers directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ceb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "class TextModel(torch.nn.Module):\n",
    "    def __init__(self, n_tokens, hidden_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(emb_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.embed = torch.nn.Embedding(n_tokens, emb_dim)\n",
    "        self.output = torch.nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, lens):\n",
    "        hidden = self.rnn(self.embed(x))[0]\n",
    "        last = torch.stack([hidden[i, lens[i] - 1, :] for i in range(hidden.shape[0])])\n",
    "        return self.output(last)\n",
    "    \n",
    "    \n",
    "layer = TextModel(len(dictionary.d), 1024, 64)\n",
    "\n",
    "print(layer)\n",
    "\n",
    "print(isinstance(layer, torch.nn.Module))\n",
    "print(isinstance(layer, padl.transforms.Transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb380f1",
   "metadata": {},
   "source": [
    "Let’s now create the entire pipeline for classication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = dictionary('<unk>')\n",
    "MIN_LEN = 100\n",
    "\n",
    "@transform\n",
    "def pad(x):\n",
    "    return list(x) + [UNK for _ in range(MIN_LEN - len(x))]\n",
    "\n",
    "\n",
    "@transform\n",
    "def truncate(x):\n",
    "    return x[:MIN_LEN]\n",
    "\n",
    "model = (\n",
    "    text_process\n",
    "    >> truncate\n",
    "    >> pad + transform(lambda x: len(x))\n",
    "    >> transform(lambda x: torch.tensor(x)) / transform(lambda x: torch.tensor(x))\n",
    "    >> batch\n",
    "    >> layer\n",
    "    >> transform(torch.nn.Sigmoid())\n",
    "    >> same[:, 0]\n",
    "    >> unbatch\n",
    "    >> transform(lambda x: {False: 'BAD', True: 'GOOD'}[(x > 0.5).item()])     # Reviews are 1-5\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39118915",
   "metadata": {},
   "source": [
    "What does all this mean? \n",
    "\n",
    "You can see we've added a few more steps to the processing - padding, converting to tensors. These are necessary so that data loading goes through.\n",
    "\n",
    "The `batch` function is used to automatically construct a data loader. The processing up to `batch` is mapped over the input using multiprocessing.\n",
    "\n",
    "Between `batch` and `unbatch` is carried out on the GPU, and the part after `unbatch` is performed in serial on the CPU.\n",
    "\n",
    "This means we save on boilerplate code for data loading, and the whole workflow from raw data to human readable/ useable outputs are together in the pipeline. This can have major practical advantages, such as portability, \n",
    "easy debugging, easy model interrogation, collaboration and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da0696",
   "metadata": {},
   "source": [
    "There are 3 ways to pass data through a pipeline with batch - these ways are “infer”, “train”, “eval”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fff72",
   "metadata": {},
   "source": [
    "“Infer”: in this mode, single data points are passed through the model a single batch (batch with one data point) is created and passed to the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17c5cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.infer_apply('This film was terrible.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de48e3",
   "metadata": {},
   "source": [
    "“Eval”: in this mode, data is loaded using multiprocessing and gradients are switched off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in model.eval_apply(['This film was terrible.', 'This film was great.'] * 10, batch_size=2):\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b8bf7",
   "metadata": {},
   "source": [
    "“Train”: in this mode, data is loaded using multiprocessing and gradients are switched on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12159287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_outputs = model.pd_preprocess >> model.pd_forward\n",
    "for output in tensor_outputs.train_apply(['This film was terrible.', 'This film was great.'] * 10, batch_size=2):\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533c44",
   "metadata": {},
   "source": [
    "Lets get a loss output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b944ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = (\n",
    "    transform(lambda x: {'BAD': 0, 'GOOD': 1}[x])\n",
    "    >> batch\n",
    "    >> transform(lambda x: x.type(torch.float))\n",
    ")\n",
    "loss = tensor_outputs / targets >> transform(torch.nn.BCELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fea88",
   "metadata": {},
   "source": [
    "Equipped with these tools, you’re now ready to train the pipeline, which allows for everything you’d expect in a PyTorch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42205188",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "o = torch.optim.Adam(model.pd_parameters(), lr=0.0005)\n",
    "loss.pd_to('cuda')\n",
    "model.pd_to('cuda')\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(100):\n",
    "    for it, l in enumerate(loss.train_apply(list(zip(X_train, Y_train)), batch_size=200, shuffle=True)):\n",
    "        o.zero_grad()\n",
    "        l.backward()\n",
    "        o.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            print(f'TRAIN; Epoch: {epoch}; Iteration: {iteration}; Loss: {l}')\n",
    "            \n",
    "        if iteration % 100 == 0:\n",
    "            predictions = list(model.eval_apply(X_valid, batch_size=200))\n",
    "            accuracy = sum([a == b for a, b in zip(predictions, Y_valid)]) / len(Y_valid)\n",
    "            print(f'VALID; Iteration: {iteration}; Epoch: {epoch}; Accuracy: {accuracy}')\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a3091",
   "metadata": {},
   "source": [
    "So your model is trained and the results are good! What should you do? Well save it, dear Liza!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, 'mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = load('mymodel.padl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
