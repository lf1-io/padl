{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857b925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "import padl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55328284",
   "metadata": {},
   "source": [
    "Get data for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b833d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-18 17:05:16--  https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip\n",
      "Resolving nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)... 162.243.189.2\n",
      "Connecting to nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)|162.243.189.2|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 85088192 (81M) [application/zip]\n",
      "Saving to: ‘training.1600000.processed.noemoticon.csv.zip’\n",
      "\n",
      "training.1600000.pr 100%[===================>]  81,15M  14,3MB/s    in 6,1s    \n",
      "\n",
      "2022-02-18 17:05:22 (13,2 MB/s) - ‘training.1600000.processed.noemoticon.csv.zip’ saved [85088192/85088192]\n",
      "\n",
      "Archive:  training.1600000.processed.noemoticon.csv.zip\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip\n",
    "!unzip -f training.1600000.processed.noemoticon.csv.zip\n",
    "!rm training.1600000.processed.noemoticon.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d62ec7",
   "metadata": {},
   "source": [
    "Preprocessing for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdba9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'training.1600000.processed.noemoticon.csv',\n",
    "    header=None,\n",
    "    encoding='latin-1',\n",
    ")\n",
    "    \n",
    "    \n",
    "X = df.iloc[:, 5].tolist()\n",
    "Y = df.iloc[:, 0].tolist()\n",
    "Y = [{0: 'BAD', 4: 'GOOD'}[y] for y in Y]\n",
    "    \n",
    "perm = [int(i) for i in numpy.random.permutation(len(X))]\n",
    "X_train = [X[i] for i in perm[:-500]]\n",
    "X_valid = [X[i] for i in perm[-500:]]\n",
    "Y_train = [Y[i] for i in perm[:-500]]\n",
    "Y_valid = [Y[i] for i in perm[-500:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4243b",
   "metadata": {},
   "source": [
    "## Sharing your prototyped results in PyTorch can be tricky\n",
    "\n",
    "You’ve spent a few days with sweat and tears putting together a PyTorch model in a Jupyter notebook, tinkering with parameters, trying various preprocessing methods, post-processing methods, validating the model results on examples, and checking on validation data sets that the model performs well. The results are good, you’re happy, you’re boss and colleagues are happy. Now you’d like to share the results, so that other people can play with the model; so what do you do? Here are the options:\n",
    "\n",
    "1. EITHER: Share the notebook\n",
    "2. AND/ OR: Share the model weights/ JIT compiled model\n",
    "3. AND/ OR: Reprogram everything in proper scripts so that the model can be reloaded by re-importing functions you created out of the individual notebook cells.\n",
    "\n",
    "None of these are ideal. Let’s see why:\n",
    "\n",
    "1. Sharing the notebook only, means that you’d be saving yourself any extra work. But it also means that whoever gets the notebook needs to run it again (reloading the data, retraining the model) in order to get the same results. This may be expensive, or impossible, since you may not be allowed to share the training data.\n",
    "2. The weights or JIT compiled model are no good by themselves; whoever receives these would need to dissect the notebook in order to make sure that they are putting the tensors into the network(s), and post-processing the outputs in the same way.\n",
    "3. Reprogramming is a lengthy and error prone process especially for complex workflows/ trainings. At the end you’d hope to have resurrected the exact algorithms/ routines you have in your Jupyter cells, but with tidy signatures, one of which allows the weights which you’d trained to be reloaded, reproducing exactly the results of the notebook. Unfortunately, when you’re finished, most likely the changes are so sweeping that you’re not sure the results are properly replicable based on the refactoring. So you’d then want to run the training one or more times, to verify that the results are the same. In addition, you’ll lose the user friendly/ interactive aspect which the notebook has.\n",
    "\n",
    "*It doesn’t have to be this way...*\n",
    "\n",
    "## PADL is a tool which boosts collaboration\n",
    "\n",
    "[PADL](https://padl.lf1.io/) allows you to work interactively in notebooks, using global variables, inline functions, preprocessing and post-processing, which utilizes the full gamut of the scientific python ecosystem and beyond. When the notebook is done, and you’re happy with the results, you can simply save the pipeline with the PADL serializer. The saved pipeline will exactly replicate the result obtained in the notebook, including the preprocessing and post-processing, including any additional artifacts necessary, data blobs, third party models (such as scikit-learn) and more. This artifact may then be shared, forwarded, experimented with, served and tested in complete isolation from the original notebook. Creating the workflow with PADL, also has a multitude of user friendly additional benefits - transparency, easy debugging, less boilerplate, code which is close to a common graphical mental model for deep learning.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "PADL tracks down the code and data necessary to save a full model pipeline using two handy abstractions: the “transform” and the “pipeline”. \n",
    "\n",
    "## The \"transform\" is a basic block of computation\n",
    "\n",
    "The “transform” is a computational block subsuming preprocessing or postprocessing step, forward pass step or layer into one single class of object. These transforms may be written in a variety of ways. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d281036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform tracks code dependencies\n",
    "@padl.transform                                               \n",
    "def clean(text):\n",
    "    return re.sub('[^A-Za-z\\ ]', ' ', text)\n",
    "\n",
    "# transform can wrap a function like this too\n",
    "split_strip = padl.transform(lambda x: x.strip().split())     \n",
    "\n",
    "# same allows easy referring to input - like a simple inline lambda\n",
    "lower_case = padl.same.lower()                                \n",
    "\n",
    "# callable classes work too!\n",
    "@padl.transform                                               \n",
    "class Dictionary:\n",
    "    def __init__(self, d, default='<unk>'):\n",
    "        self.d = d\n",
    "        self.default = default\n",
    "        \n",
    "    def __call__(self, token):\n",
    "        if token in self.d:\n",
    "            return self.d[token]\n",
    "        else:\n",
    "            return self.d[self.default]\n",
    "        \n",
    "\n",
    "def save_dictionary(val, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(val, f)\n",
    "        \n",
    "        \n",
    "def load_dictionary(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b0f07",
   "metadata": {},
   "source": [
    "## Transforms may be linked together into \"pipelines\"\n",
    "\n",
    "Once you’ve defined a collection of transforms using `transform`, they may be linked into pipelines using a few primitive operators, leading to a DAG structure for the pipeline. The operators are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204eaa4",
   "metadata": {},
   "source": [
    "`Map` is the classical well known functional primitive and has short hand `~`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac176c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Testing transform',\n",
       " 'Testing transform',\n",
       " 'Testing transform',\n",
       " 'Testing transform',\n",
       " 'Testing transform')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~ clean)(['Testing transform' for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9020934",
   "metadata": {},
   "source": [
    "`Compose` which has the overloading short-hand `>>`. This is similar to composing in, for example, `torchvision`. Transforms or other pipelines’ outputs are passed positionally onto the subsequent objects in the composition. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd0665c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"text_process\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ text\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mclean                      \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mlower()                    \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mlambda x: x.strip().split()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process = (\n",
    "    clean\n",
    "    >> lower_case\n",
    "    >> split_strip\n",
    ")\n",
    "\n",
    "text_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a570f",
   "metadata": {},
   "source": [
    "## Data artifacts/ blobs can be included in the pipeline\n",
    "\n",
    "We can use this text processor, for instance, to create the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8de805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599500/1599500 [00:08<00:00, 193479.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1mDictionary(d={'<unk>': 20000, 'a': 3, 'aa': 4780, 'aaa': 7944, ...})\u001b[0m - \"dictionary\":\n",
       "\n",
       "   class Dictionary:\n",
       "       def __init__(self, d, default='<unk>'):\n",
       "           self.d = d\n",
       "           self.default = default\n",
       "\n",
       "       def __call__(self, token):\n",
       "           if token in self.d:\n",
       "               return self.d[token]\n",
       "           else:\n",
       "               return self.d[self.default]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in tqdm.tqdm(X_train):\n",
    "    words.extend(text_process(sentence))\n",
    "counts = dict(collections.Counter(words))\n",
    "allowed = sorted(list(counts.keys()), key=lambda x: -counts[x])[:20000]\n",
    "allowed.append('<unk>')\n",
    "allowed = padl.value(allowed)\n",
    "dictionary = Dictionary({k: i for i, k in enumerate(allowed)})\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707796b9",
   "metadata": {},
   "source": [
    "You'll notice the use of `padl.value` here - this keyword allows PADL to track data artifacts,\n",
    "which should also be saved with the pipeline as data blobs (not the code which created them),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3f974",
   "metadata": {},
   "source": [
    "Let's add this to the text-processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5a0fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"text_process\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ text\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mclean                                                                 \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mlower()                                                               \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mlambda x: x.strip().split()                                           \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0m~ Dictionary(d={'<unk>': 20000, 'a': 3, 'aa': 4780, 'aaa': 7944, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process = text_process >> ~ dictionary\n",
    "text_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a68ba6",
   "metadata": {},
   "source": [
    "`Parallel` which has the overloading short-hand `/`. This refers to the situation where multiple transforms are applied “in parallel” to a tuple/ list of outputs from a previous step. This allows you to create complex branching pipelines, providing great flexibility and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc9621e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namedtuple(clean='Test another   ', lower_case='test thingy')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallel sends each part of a tuple to the ith transform in \"parallel\"\n",
    "(clean / lower_case)(('Test another&*$', 'Test thingy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431d835",
   "metadata": {},
   "source": [
    "`Rollout` is related to `Parallel` and has the short hand `+`; several transforms are applied over the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5077dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namedtuple(clean='Testing transform', lower_case='testing transform')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clean + lower_case)('Testing transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c43a7c",
   "metadata": {},
   "source": [
    "## PyTorch layers may be included organically in your pipeline\n",
    "\n",
    "PyTorch layers are first class objects in PADL, that means we can decorate the layers directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240ceb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextModel(n_tokens=20001, hidden_size=1024, emb_dim=64)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "@padl.transform\n",
    "class TextModel(torch.nn.Module):\n",
    "    def __init__(self, n_tokens, hidden_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(emb_dim, hidden_size=hidden_size,\n",
    "                                batch_first=True)\n",
    "        self.embed = torch.nn.Embedding(n_tokens, emb_dim)\n",
    "        self.output = torch.nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, lens):\n",
    "        hidden = self.rnn(self.embed(x))[0]\n",
    "        last = torch.stack([hidden[i, lens[i] - 1, :]\n",
    "                            for i in range(hidden.shape[0])])\n",
    "        return self.output(last)\n",
    "    \n",
    "    \n",
    "layer = TextModel(len(dictionary.d), 1024, 64)\n",
    "\n",
    "print(layer)\n",
    "\n",
    "print(isinstance(layer, torch.nn.Module))\n",
    "print(isinstance(layer, padl.transforms.Transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb380f1",
   "metadata": {},
   "source": [
    "Let’s now create the entire pipeline for classication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62a1a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"model\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ text\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mclean                                                   \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mlower()                                                 \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mlambda x: x.strip().split()                             \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0m~ Dictionary(d={'<unk>': 20000, 'a': 3, 'aa': 4780, 'aa \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mtruncate                                                \n",
       "   \u001b[32m   └──────────────────────────────────────────────────────────┐\n",
       "      │                                                          │\n",
       "      ▼ x                                                        ▼ x\u001b[0m\n",
       "   \u001b[1m5: \u001b[0mpad                                                      \u001b[32m+\u001b[0m lambda x: len(x)         \n",
       "   \u001b[32m   │                                                          │\n",
       "      ▼ x                                                        ▼ x\u001b[0m\n",
       "   \u001b[1m6: \u001b[0mlambda x: torch.tensor(x)                                \u001b[32m/\u001b[0m lambda x: torch.tensor(x)\n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m7: \u001b[0mBatchify(dim=0)                                         \n",
       "   \u001b[32m   │\n",
       "      ▼ (x, lens)\u001b[0m\n",
       "   \u001b[1m8: \u001b[0mTextModel(n_tokens=20001, hidden_size=1024, ...)        \n",
       "   \u001b[32m   │\n",
       "      ▼ input\u001b[0m\n",
       "   \u001b[1m9: \u001b[0mSigmoid(torch.nn.Sigmoid())                             \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m10: \u001b[0m__getitem__((slice(None, None, None), 0))               \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m11: \u001b[0mUnbatchify(dim=0, cpu=True)                             \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m12: \u001b[0mlambda x: {False: 'BAD', True: 'GOOD'}[(x > 0.5).item()]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNK = dictionary('<unk>')\n",
    "MIN_LEN = 100\n",
    "\n",
    "@padl.transform\n",
    "def pad(x):\n",
    "    return list(x) + [UNK for _ in range(MIN_LEN - len(x))]\n",
    "\n",
    "\n",
    "@padl.transform\n",
    "def truncate(x):\n",
    "    return x[:MIN_LEN]\n",
    "\n",
    "to_tensor = padl.transform(lambda x: torch.tensor(x))\n",
    "\n",
    "model = (\n",
    "    text_process\n",
    "    >> truncate\n",
    "    >> pad + padl.transform(lambda x: len(x))\n",
    "    >> to_tensor / to_tensor\n",
    "    >> padl.batch\n",
    "    >> layer\n",
    "    >> padl.transform(torch.nn.Sigmoid())\n",
    "    >> padl.same[:, 0]\n",
    "    >> padl.unbatch\n",
    "    >> padl.transform(\n",
    "        lambda x: {False: 'BAD', True: 'GOOD'}[(x > 0.5).item()]\n",
    "    )\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39118915",
   "metadata": {},
   "source": [
    "What does all this mean? \n",
    "\n",
    "You can see we've added a few more steps to the processing - padding, converting to tensors. These are necessary so that data loading goes through.\n",
    "\n",
    "The `batch` function is used to automatically construct a data loader. The processing up to `batch` is mapped over the input using multiprocessing.\n",
    "\n",
    "Between `batch` and `unbatch` is carried out on the GPU, and the part after `unbatch` is performed in serial on the CPU.\n",
    "\n",
    "This means we save on boilerplate code for data loading, and the whole workflow from raw data to human readable/ useable outputs are together in the pipeline. This can have major practical advantages, such as portability, \n",
    "easy debugging, easy model interrogation, collaboration and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da0696",
   "metadata": {},
   "source": [
    "## PADL allows for iterating through data in several ways\n",
    "\n",
    "There are 3 ways to pass data through a pipeline with batch - these ways are “infer”, “train”, “eval”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fff72",
   "metadata": {},
   "source": [
    "“Infer”: in this mode, single data points are passed through the model a single batch (batch with one data point) is created and passed to the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d17c5cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAD'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_apply('This film was terrible.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de48e3",
   "metadata": {},
   "source": [
    "“Eval”: in this mode, data is loaded using multiprocessing and gradients are switched off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b4d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n",
      "BAD\n"
     ]
    }
   ],
   "source": [
    "for output in model.eval_apply(['This film was terrible.',\n",
    "                                'This film was great.'] * 10, batch_size=2):\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b8bf7",
   "metadata": {},
   "source": [
    "“Train”: in this mode, data is loaded using multiprocessing and gradients are switched on. Here we use the keywords `model.pd_preprocess` (extracts pipeline up to `batch`) and `model.pd_forward` (forward pass part of pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12159287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "tensor_outputs = model.pd_preprocess >> model.pd_forward\n",
    "for output in tensor_outputs.train_apply(['This film was terrible.', 'This film was great.'] * 10, batch_size=2):\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533c44",
   "metadata": {},
   "source": [
    "## PADL training is completely flexible\n",
    "\n",
    "\n",
    "Lets get a transform which outputs a loss scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b944ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = (\n",
    "    padl.transform(lambda x: {'BAD': 0, 'GOOD': 1}[x])\n",
    "    >> padl.batch\n",
    "    >> padl.transform(lambda x: x.type(torch.float))\n",
    ")\n",
    "loss = tensor_outputs / targets >> padl.transform(torch.nn.BCELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f46a3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"loss\":\n",
       "\n",
       "   \u001b[32m   │└────────────────────────────┐\n",
       "      │                             │\n",
       "      ▼ args                        ▼ args\u001b[0m\n",
       "   \u001b[1m0: \u001b[0m\u001b[32m[\u001b[0mtensor_outputs: ..\u001b[32m>>\u001b[0m..\u001b[32m]\u001b[0m \u001b[32m/\u001b[0m \u001b[32m[\u001b[0mtargets: ..\u001b[32m>>\u001b[0m..\u001b[32m]\u001b[0m\n",
       "   \u001b[32m   │\n",
       "      ▼ (input, target)\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mBCELoss(torch.nn.BCELoss())"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fea88",
   "metadata": {},
   "source": [
    "Equipped with these tools, you’re now ready to train the pipeline, which allows for everything you’d expect in a PyTorch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42205188",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN; Epoch: 0; Iteration: 0; Loss: 0.5170862674713135\n",
      "VALID; Iteration: 0; Epoch: 0; Accuracy: 0.758\n",
      "TRAIN; Epoch: 0; Iteration: 10; Loss: 0.45407652854919434\n",
      "TRAIN; Epoch: 0; Iteration: 20; Loss: 0.4555834233760834\n",
      "TRAIN; Epoch: 0; Iteration: 30; Loss: 0.4157324433326721\n",
      "quitting...\n"
     ]
    }
   ],
   "source": [
    "o = torch.optim.Adam(model.pd_parameters(), lr=0.0005)\n",
    "loss.pd_to('cuda')\n",
    "model.pd_to('cuda')\n",
    "\n",
    "iteration = 0\n",
    "try:\n",
    "    for epoch in range(100):\n",
    "        for it, l in enumerate(loss.train_apply(list(zip(X_train, Y_train)), batch_size=200, shuffle=True)):\n",
    "            o.zero_grad()\n",
    "            l.backward()\n",
    "            o.step()\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                print(f'TRAIN; Epoch: {epoch}; Iteration: {iteration}; Loss: {l}')\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                predictions = list(model.eval_apply(X_valid, batch_size=200))\n",
    "                accuracy = sum([a == b for a, b in zip(predictions, Y_valid)]) / len(Y_valid)\n",
    "                print(f'VALID; Iteration: {iteration}; Epoch: {epoch}; Accuracy: {accuracy}')\n",
    "            iteration += 1\n",
    "except KeyboardInterrupt:\n",
    "    print('quitting...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a3091",
   "metadata": {},
   "source": [
    "## Saving and loading in PADL includes everything\n",
    "\n",
    "So your model is trained and the results are good! What should you do? Well save it, dear Liza!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2278510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving torch module to mymodel.padl/13.pt\n",
      "saving torch module to mymodel.padl/14.pt\n"
     ]
    }
   ],
   "source": [
    "padl.save(model, 'mymodel', force_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbc7e2",
   "metadata": {},
   "source": [
    "The following cell works in a completely new session/ after restarting the kernel -- no imports, data processing etc.. required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d051354f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading torch module from mymodel.padl/13.pt\n",
      "loading torch module from mymodel.padl/14.pt\n",
      "BAD\n",
      "GOOD\n"
     ]
    }
   ],
   "source": [
    "reloaded = padl.load('mymodel.padl')\n",
    "print(reloaded.infer_apply('I am really not very happy right now'))\n",
    "print(reloaded.infer_apply('I am really stoked to try out this great padl thing!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c712226",
   "metadata": {},
   "source": [
    "## Apply PADL to all your workflows\n",
    "\n",
    "In this tutorial we implemented an NLP workflow using PyTorch and PADL. However all of this generalizes to the full range of Deep Learning tasks. If you can implement your preprocessing/ forward pass and postprocess with PyTorch and the Python ecosystem, then you can PADL-lize it!\n",
    "\n",
    "Once your pipeline is in PADL, then you can share the exported pipelines, interact with the steps of the pipelines,\n",
    "import the pipeline easily, into another session, notebook, or server.\n",
    "\n",
    "So no more excuses - your model is trained, you are happy, your boss is happy - now your collaborators will be happy too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd693164",
   "metadata": {},
   "source": [
    "**Happy PADL-ling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
